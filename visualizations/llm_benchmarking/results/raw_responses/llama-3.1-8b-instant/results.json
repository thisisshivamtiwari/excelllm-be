[
  {
    "question_id": "Complex_13",
    "question_text": "How does the average rework count for 'Widget-A' differ between Line-1 and Line-2?",
    "category": "Complex",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Identify Relevant Tables and Filters\nTo answer the question, we need to analyze the quality control data for 'Widget-A' on both Line-1 and Line-2. We will use the **quality_control** table and apply filters for the product 'Widget-A' and the relevant lines.\n\n```sql\nSELECT *\nFROM quality_control\nWHERE Product = 'Widget-A'\n```\n\n### Step 2: Filter Data by Line\nWe need to separate the data for Line-1 and Line-2. We will use the **quality_control** table and apply filters for the relevant lines.\n\n```sql\nSELECT *\nFROM quality_control\nWHERE Product = 'Widget-A'\n  AND Line IN ('Line-1', 'Line-2')\n```\n\n### Step 3: Calculate Average Rework Count by Line\nWe will calculate the average rework count for 'Widget-A' on both Line-1 and Line-2. We will use the **quality_control** table and apply the filters from Step 2. We will then group the data by Line and calculate the average Rework_Count.\n\n```sql\nSELECT Line, AVG(Rework_Count) AS Average_Rework_Count\nFROM quality_control\nWHERE Product = 'Widget-A'\n  AND Line IN ('Line-1', 'Line-2')\nGROUP BY Line\n```\n\n### Step 4: Combine Data for Final Answer\nWe will combine the results from Step 3 into a single table to get the final answer.\n\n```sql\nSELECT Line, Average_Rework_Count\nFROM (\n  SELECT Line, AVG(Rework_Count) AS Average_Rework_Count\n  FROM quality_control\n  WHERE Product = 'Widget-A'\n    AND Line IN ('Line-1', 'Line-2')\n  GROUP BY Line\n) AS Subquery\n```\n\n### Step 5: Interpret Results\nWe will interpret the results from Step 4 to answer the question. The average rework count for 'Widget-A' on Line-1 and Line-2 will be displayed in a table format.\n\nThe final answer will be a table with two columns: Line and Average_Rework_Count.",
    "sql_response": "```sql\nSELECT \n    L1.Line, \n    AVG(L1.Rework_Count) AS Avg_Rework_Count_Line1, \n    AVG(L2.Rework_Count) AS Avg_Rework_Count_Line2\nFROM \n    (SELECT \n         Line, \n         Product, \n         Rework_Count\n     FROM \n         quality_control\n     WHERE \n         Product = 'Widget-A' AND Line = 'Line-1') L1\nFULL OUTER JOIN \n    (SELECT \n         Line, \n         Product, \n         Rework_Count\n     FROM \n         quality_control\n     WHERE \n         Product = 'Widget-A' AND Line = 'Line-2') L2\nON \n    L1.Product = L2.Product\nGROUP BY \n    L1.Line, L2.Line;\n```\n\nThis SQL query will return the average rework count for 'Widget-A' on both Line-1 and Line-2. If there are no records for 'Widget-A' on either line, the query will still return the result with NULL values for the respective lines.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns that contain information about the rework count for 'Widget-A' on Line-1 and Line-2.\n\nBased on the available tables and columns, we can see that the `quality_control` table contains information about the rework count. Specifically, the `Rework_Count` column in the `quality_control` table contains the rework count for each product.\n\nHowever, we also need to identify the Line-1 and Line-2 data. We can use the `Line` column in the `quality_control` table to filter the data for Line-1 and Line-2.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Product\", \"Line\", \"Rework_Count\"]\n}\n```\n\nThis JSON object indicates that we need to use the `quality_control` table and the `Product`, `Line`, and `Rework_Count` columns to answer the question.",
    "methodology_score": 55.55555555555556,
    "sql_score": 82.96153846153847,
    "table_column_score": 100.0,
    "response_quality_score": 95.97241544723511,
    "overall_score": 80.30044667292864,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 10,
      "expected_keyword_count": 18,
      "response_keyword_count": 62
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 75.0,
      "aggregate_match": 100.0,
      "clause_match": 80.0,
      "fuzzy_score": 48.07692307692307,
      "overall_score": 82.96153846153847,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "line",
        "product",
        "rework_count"
      ],
      "generated_columns": [
        "line",
        "product",
        "rework_count",
        "l1"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "line",
        "rework_count"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "line",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "line",
        "rework_count"
      ]
    },
    "total_latency_ms": 4027.5845527648926,
    "total_tokens": 2016,
    "errors": [],
    "timestamp": "2025-11-30T12:54:56.351148"
  },
  {
    "question_id": "Easy_15",
    "question_text": "Which inspector had the highest number of reworked items?",
    "category": "Easy",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Answering the Question: Which inspector had the highest number of reworked items?\n\n### Step-by-Step Methodology:\n\n```steps\n1. **Identify the relevant table**: The quality_control table contains information about inspections, including reworked items. We will use this table to answer the question.\n\n2. **Filter the data**: We need to filter the quality_control table to only include rows where the Rework_Count is greater than 0, as we are interested in inspectors who have reworked items.\n\n   ```sql\nSELECT *\nFROM quality_control\nWHERE Rework_Count > 0;\n```\n\n3. **Group the data by Inspector_Name**: We need to group the filtered data by the Inspector_Name column to calculate the total number of reworked items per inspector.\n\n   ```sql\nSELECT Inspector_Name, SUM(Rework_Count) AS Total_Reworked_Items\nFROM quality_control\nWHERE Rework_Count > 0\nGROUP BY Inspector_Name;\n```\n\n4. **Sort the data in descending order**: We need to sort the grouped data in descending order by the Total_Reworked_Items column to identify the inspector with the highest number of reworked items.\n\n   ```sql\nSELECT Inspector_Name, SUM(Rework_Count) AS Total_Reworked_Items\nFROM quality_control\nWHERE Rework_Count > 0\nGROUP BY Inspector_Name\nORDER BY Total_Reworked_Items DESC;\n```\n\n5. **Identify the inspector with the highest number of reworked items**: The top row in the sorted data will be the inspector with the highest number of reworked items.\n\nBy following these steps, we can identify the inspector who had the highest number of reworked items.",
    "sql_response": "```sql\nSELECT \n    Inspector_Name,\n    MAX(Rework_Count) AS Max_Rework_Count\nFROM \n    quality_control\nGROUP BY \n    Inspector_Name\nORDER BY \n    Max_Rework_Count DESC\nLIMIT 1;\n```\n\nHowever, the above query will return the inspector with the highest rework count and the maximum rework count. If you want to get the inspector with the highest rework count without specifying the maximum rework count, you can use the following query:\n\n```sql\nSELECT \n    Inspector_Name,\n    MAX(Rework_Count) AS Max_Rework_Count\nFROM \n    quality_control\nGROUP BY \n    Inspector_Name\nORDER BY \n    Max_Rework_Count DESC\nLIMIT 1;\n```\n\nIf you want to get the inspector with the highest rework count without specifying the maximum rework count and also want to get all inspectors with the same highest rework count, you can use the following query:\n\n```sql\nSELECT \n    Inspector_Name,\n    MAX(Rework_Count) AS Max_Rework_Count\nFROM \n    quality_control\nGROUP BY \n    Inspector_Name\nHAVING \n    MAX(Rework_Count) = (SELECT MAX(Rework_Count) FROM quality_control)\nORDER BY \n    Max_Rework_Count DESC;\n```",
    "table_selection_response": "To answer the question, we need to identify which tables and columns are needed to find the inspector with the highest number of reworked items.\n\nThe relevant table is `quality_control` because it contains information about the quality control process, including reworked items.\n\nThe relevant columns are:\n\n- `Inspector_Name`: This column contains the name of the inspector who performed the quality control.\n- `Rework_Count`: This column contains the number of items reworked by the inspector.\n\nHowever, we also need to consider the `Inspector_Name` column from the `quality_control` table to identify the inspector with the highest number of reworked items.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Inspector_Name\", \"Rework_Count\"]\n}\n```",
    "methodology_score": 72.72727272727273,
    "sql_score": 69.68459495351925,
    "table_column_score": 100.0,
    "response_quality_score": 97.8916609287262,
    "overall_score": 80.99695614478617,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 8,
      "expected_keyword_count": 11,
      "response_keyword_count": 75
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 50.0,
      "aggregate_match": 33.33333333333333,
      "clause_match": 100.0,
      "fuzzy_score": 70.1195219123506,
      "overall_score": 69.68459495351925,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "sum",
        "rework_count",
        "inspector_name"
      ],
      "generated_columns": [
        "max_rework_count",
        "rework_count",
        "inspector_name"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "rework_count",
        "inspector_name"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "rework_count",
        "inspector_name"
      ],
      "generated_columns": [
        "rework_count",
        "inspector_name"
      ]
    },
    "total_latency_ms": 2108.3390712738037,
    "total_tokens": 1873,
    "errors": [],
    "timestamp": "2025-11-30T12:55:05.148646"
  },
  {
    "question_id": "Easy_4",
    "question_text": "What is the total number of components reworked in Line-2?",
    "category": "Easy",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Answering the Question: Total Number of Components Reworked in Line-2\n\n### Step 1: Identify the relevant table and filter\nUse the **quality_control** table, which contains information about inspections, reworks, and defects. Filter the data to include only rows where the Line is 'Line-2'.\n\n```sql\nSELECT * FROM quality_control WHERE Line = 'Line-2'\n```\n\n### Step 2: Extract the relevant column\nFrom the filtered data, extract the Rework_Count column, which represents the number of components reworked.\n\n```sql\nSELECT Rework_Count FROM quality_control WHERE Line = 'Line-2'\n```\n\n### Step 3: Calculate the total number of components reworked\nUse the SUM aggregation function to calculate the total number of components reworked in Line-2.\n\n```sql\nSELECT SUM(Rework_Count) AS Total_Reworked FROM quality_control WHERE Line = 'Line-2'\n```\n\n### Step 4: Execute the query\nRun the query to obtain the total number of components reworked in Line-2.\n\n### Step 5: Verify the result\nVerify that the result is a single value representing the total number of components reworked in Line-2.\n\n### Final Query:\n```sql\nSELECT SUM(Rework_Count) AS Total_Reworked FROM quality_control WHERE Line = 'Line-2'\n```\n\nThis query will return the total number of components reworked in Line-2.",
    "sql_response": "```sql\nSELECT \n    SUM(qc.Rework_Count) AS Total_Reworked_Components\nFROM \n    quality_control qc\nWHERE \n    qc.Line = 'Line-2';\n```\n\nThis SQL query will return the total number of components reworked in Line-2. The `SUM` function is used to calculate the total number of reworked components, and the `WHERE` clause is used to filter the results to only include rows where the `Line` is 'Line-2'.",
    "table_selection_response": "To answer the question, we need to identify the total number of components reworked in Line-2. This involves understanding the concept of rework, which is typically associated with quality control processes.\n\nBased on the available tables and columns, we can infer that the quality control table contains information about inspections, reworks, and defects. The Line column in the quality control table seems to be related to the production line, which could be used to identify Line-2.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Rework_Count\"]\n}\n```\n\nHowever, we need to filter the data by Line to get the total number of components reworked in Line-2. We can use the WHERE clause to achieve this. But since the Line column is not explicitly mentioned in the question, we can assume that it is the same as the Line_Machine column in the production_logs table.\n\nTo get the total number of components reworked in Line-2, we would need to join the quality_control table with the production_logs table on the Product column and then filter the data by Line_Machine = 'Line-2'. But since the question only asks for the total number of components reworked, we can directly use the quality_control table with the Line column.\n\nHere's the updated JSON object:\n\n```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Line\", \"Rework_Count\"]\n}\n```\n\nTo get the total number of components reworked in Line-2, we would need to use a SQL query like this:\n\n```sql\nSELECT SUM(Rework_Count) \nFROM quality_control \nWHERE Line = 'Line-2';\n```\n\nThis query will return the total number of components reworked in Line-2.",
    "methodology_score": 100,
    "sql_score": 77.01676528599606,
    "table_column_score": 80.0,
    "response_quality_score": 98.04512286186218,
    "overall_score": 86.76038013628485,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 7,
      "expected_keyword_count": 7,
      "response_keyword_count": 64
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 50.0,
      "aggregate_match": 66.66666666666666,
      "clause_match": 100.0,
      "fuzzy_score": 74.55621301775149,
      "overall_score": 77.01676528599606,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "line",
        "rework_count"
      ],
      "generated_columns": [
        "rework_count"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 80.0,
      "table_score": 100.0,
      "column_score": 66.66666666666666,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 50.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "rework_count"
      ],
      "columns_missing": [
        "line"
      ],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "line",
        "rework_count"
      ],
      "generated_columns": [
        "rework_count"
      ]
    },
    "total_latency_ms": 1954.8771381378174,
    "total_tokens": 1887,
    "errors": [],
    "timestamp": "2025-11-30T12:55:14.875556"
  },
  {
    "question_id": "Complex_26",
    "question_text": "What is the impact of preventive maintenance on the subsequent breakdown frequency for Machine-M1?",
    "category": "Complex",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Impact of Preventive Maintenance on Breakdown Frequency for Machine-M1\n\nTo answer this question, we'll follow these steps:\n\n### Step 1: Identify Preventive Maintenance Records for Machine-M1\nUse the **maintenance_logs** table and filter for records where the Machine is 'Machine-M1' and the Maintenance_Type is 'Preventive'.\n\n```sql\nSELECT *\nFROM maintenance_logs\nWHERE Machine = 'Machine-M1'\n  AND Maintenance_Type = 'Preventive';\n```\n\n### Step 2: Extract Relevant Dates for Preventive Maintenance\nFrom the filtered records, extract the Maintenance_Date for each preventive maintenance record.\n\n```sql\nSELECT Maintenance_Date\nFROM maintenance_logs\nWHERE Machine = 'Machine-M1'\n  AND Maintenance_Type = 'Preventive';\n```\n\n### Step 3: Identify Breakdown Records for Machine-M1\nUse the **maintenance_logs** table and filter for records where the Machine is 'Machine-M1' and the Maintenance_Type is 'Breakdown'.\n\n```sql\nSELECT *\nFROM maintenance_logs\nWHERE Machine = 'Machine-M1'\n  AND Maintenance_Type = 'Breakdown';\n```\n\n### Step 4: Extract Relevant Dates for Breakdowns\nFrom the filtered records, extract the Breakdown_Date for each breakdown record.\n\n```sql\nSELECT Breakdown_Date\nFROM maintenance_logs\nWHERE Machine = 'Machine-M1'\n  AND Maintenance_Type = 'Breakdown';\n```\n\n### Step 5: Calculate Time Between Preventive Maintenance and Breakdowns\nUse the extracted dates from Steps 2 and 4 to calculate the time difference between each preventive maintenance record and the subsequent breakdown record (if any).\n\n```sql\nWITH preventive_maintenance AS (\n  SELECT Maintenance_Date\n  FROM maintenance_logs\n  WHERE Machine = 'Machine-M1'\n    AND Maintenance_Type = 'Preventive'\n),\nbreakdowns AS (\n  SELECT Breakdown_Date\n  FROM maintenance_logs\n  WHERE Machine = 'Machine-M1'\n    AND Maintenance_Type = 'Breakdown'\n),\ntime_diff AS (\n  SELECT \n    pm.Maintenance_Date,\n    b.Breakdown_Date,\n    DATEDIFF(b.Breakdown_Date, pm.Maintenance_Date) AS time_between\n  FROM preventive_maintenance pm\n  LEFT JOIN breakdowns b ON pm.Maintenance_Date < b.Breakdown_Date\n)\nSELECT *\nFROM time_diff;\n```\n\n### Step 6: Calculate Breakdown Frequency\nCalculate the average breakdown frequency for Machine-M1 by dividing the total number of breakdowns by the total time between preventive maintenance records.\n\n```sql\nWITH breakdown_frequency AS (\n  SELECT \n    COUNT(b.Breakdown_Date) AS total_breakdowns,\n    SUM(DATEDIFF(b.Breakdown_Date, pm.Maintenance_Date)) AS total_time\n  FROM preventive_maintenance pm\n  LEFT JOIN breakdowns b ON pm.Maintenance_Date < b.Breakdown_Date\n)\nSELECT \n  total_breakdowns / NULLIF(total_time, 0) AS breakdown_frequency\nFROM breakdown_frequency;\n```\n\n### Step 7: Compare Breakdown Frequency with and without Preventive Maintenance\nTo assess the impact of preventive maintenance, compare the breakdown frequency calculated in Step 6 with the overall breakdown frequency for Machine-M1 (without considering preventive maintenance).\n\n```sql\nWITH overall_breakdown_frequency AS (\n  SELECT \n    COUNT(Breakdown_Date) AS total_breakdowns,\n    SUM(DATEDIFF(Breakdown_Date, Maintenance_Date)) AS total_time\n  FROM maintenance_logs\n  WHERE Machine = 'Machine-M1'\n    AND Maintenance_Type = 'Breakdown'\n),\npreventive_breakdown_frequency AS (\n  SELECT \n    COUNT(b.Breakdown_Date) AS total_breakdowns,\n    SUM(DATEDIFF(b.Breakdown_Date, pm.Maintenance_Date)) AS total_time\n  FROM preventive_maintenance pm\n  LEFT JOIN breakdowns b ON pm.Maintenance_Date < b.Breakdown_Date\n)\nSELECT \n  (overall_breakdown_frequency.total_breakdowns / NULLIF(overall_breakdown_frequency.total_time, 0)) AS overall_breakdown_frequency,\n  (preventive_breakdown_frequency.total_breakdowns / NULLIF(preventive_breakdown_frequency.total_time, 0)) AS preventive_breakdown_frequency,\n  (overall_breakdown_frequency.total_breakdowns / NULLIF(overall_breakdown_frequency.total_time, 0)) - (preventive_breakdown_frequency.total_breakdowns / NULLIF(preventive_breakdown_frequency.total_time, 0)) AS impact_of_preventive_maintenance\nFROM overall_breakdown_frequency\nCROSS JOIN preventive_breakdown_frequency;\n```\n\n### Step 8: Interpret Results\nAnalyze the results from Step 7 to understand the impact of preventive maintenance on the breakdown frequency for Machine-M1. A positive value indicates a reduction in breakdown frequency, while a negative value indicates an increase.",
    "sql_response": "```sql\nWITH \n-- Calculate the total downtime hours for Machine-M1 due to breakdowns\nbreakdown_downtime AS (\n  SELECT \n    Machine,\n    SUM(Downtime_Hours) AS total_breakdown_downtime\n  FROM \n    maintenance_logs\n  WHERE \n    Maintenance_Type = 'Breakdown' AND Machine = 'Machine-M1'\n  GROUP BY \n    Machine\n),\n-- Calculate the total downtime hours for Machine-M1 due to preventive maintenance\npreventive_downtime AS (\n  SELECT \n    Machine,\n    SUM(Downtime_Hours) AS total_preventive_downtime\n  FROM \n    maintenance_logs\n  WHERE \n    Maintenance_Type = 'Preventive' AND Machine = 'Machine-M1'\n  GROUP BY \n    Machine\n),\n-- Calculate the total downtime hours for Machine-M1\ntotal_downtime AS (\n  SELECT \n    Machine,\n    SUM(Downtime_Hours) AS total_downtime\n  FROM \n    maintenance_logs\n  WHERE \n    Machine = 'Machine-M1'\n  GROUP BY \n    Machine\n)\n-- Calculate the impact of preventive maintenance on breakdown frequency\nSELECT \n  pd.total_preventive_downtime AS preventive_maintenance_downtime,\n  bd.total_breakdown_downtime AS breakdown_downtime,\n  (bd.total_breakdown_downtime - bd.total_breakdown_downtime) / bd.total_breakdown_downtime AS impact\nFROM \n  breakdown_downtime bd\n  JOIN preventive_downtime pd ON bd.Machine = pd.Machine\n  JOIN total_downtime td ON bd.Machine = td.Machine\nWHERE \n  bd.Machine = 'Machine-M1';\n```\n\nThis SQL query uses Common Table Expressions (CTEs) to calculate the total downtime hours for Machine-M1 due to breakdowns, preventive maintenance, and total downtime. It then calculates the impact of preventive maintenance on breakdown frequency by subtracting the preventive maintenance downtime from the breakdown downtime and dividing by the breakdown downtime. The result is a measure of the impact of preventive maintenance on the subsequent breakdown frequency for Machine-M1.",
    "table_selection_response": "To answer the question about the impact of preventive maintenance on the subsequent breakdown frequency for Machine-M1, we need to analyze the relationship between maintenance activities and machine downtime. Here's the required information:\n\n### Tables:\n- `maintenance_logs`: This table contains information about maintenance activities, including preventive maintenance.\n- `production_logs`: This table contains information about production, including downtime minutes, which can be used to calculate breakdown frequency.\n\n### Columns:\n- `Maintenance_Date` from `maintenance_logs`: This column will help us identify the dates when preventive maintenance was performed.\n- `Breakdown_Date` from `maintenance_logs`: This column will help us identify the dates when the machine broke down after preventive maintenance.\n- `Downtime_Minutes` from `production_logs`: This column will help us calculate the breakdown frequency after preventive maintenance.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\", \"production_logs\"],\n  \"columns\": [\"Maintenance_Date\", \"Breakdown_Date\", \"Downtime_Minutes\"]\n}\n```",
    "methodology_score": 55.00000000000001,
    "sql_score": 47.86961028192371,
    "table_column_score": 60.952380952380956,
    "response_quality_score": 90.9852647781372,
    "overall_score": 57.590985314582255,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 11,
      "expected_keyword_count": 20,
      "response_keyword_count": 118
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 12.5,
      "aggregate_match": 0.0,
      "clause_match": 80.0,
      "fuzzy_score": 51.63073521282476,
      "overall_score": 47.86961028192371,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "breakdown_date",
        "maintenance_date",
        "daystobreakdown",
        "machine"
      ],
      "generated_columns": [
        "total_preventive_downtime",
        "maintenance_type",
        "machine",
        "downtime_hours",
        "total_breakdown_downtime"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 60.952380952380956,
      "table_score": 66.66666666666666,
      "column_score": 57.14285714285715,
      "table_precision": 50.0,
      "table_recall": 100.0,
      "column_precision": 66.66666666666666,
      "column_recall": 50.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [
        "production_logs"
      ],
      "columns_correct": [
        "breakdown_date",
        "maintenance_date"
      ],
      "columns_missing": [
        "maintenance_type",
        "machine"
      ],
      "columns_extra": [
        "downtime_minutes"
      ],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs",
        "production_logs"
      ],
      "expected_columns": [
        "breakdown_date",
        "maintenance_date",
        "maintenance_type",
        "machine"
      ],
      "generated_columns": [
        "breakdown_date",
        "maintenance_date",
        "downtime_minutes"
      ]
    },
    "total_latency_ms": 9014.735221862793,
    "total_tokens": 2768,
    "errors": [],
    "timestamp": "2025-11-30T12:55:30.220679"
  },
  {
    "question_id": "Medium_2",
    "question_text": "What is the total cost of maintenance for Machine-M2 in 2026?",
    "category": "Medium",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Filter Maintenance Logs for Machine-M2 in 2026\nUse the **maintenance_logs** table and filter the data for Machine-M2 and the year 2026.\n```sql\nSELECT *\nFROM maintenance_logs\nWHERE Machine = 'Machine-M2'\n  AND Maintenance_Date >= '2026-01-01'\n  AND Maintenance_Date < '2027-01-01';\n```\n\n### Step 2: Calculate Total Cost of Maintenance for Each Record\nFor each record, calculate the total cost of maintenance by multiplying the cost of parts replaced by the unit cost of the parts.\n```sql\nSELECT \n  Maintenance_Date,\n  Machine,\n  Maintenance_Type,\n  Breakdown_Date,\n  Downtime_Hours,\n  Issue_Description,\n  Technician,\n  Parts_Replaced,\n  Cost_Rupees,\n  Parts_Replaced * Unit_Cost_Rupees AS Total_Cost\nFROM \n  maintenance_logs\nWHERE \n  Machine = 'Machine-M2'\n  AND Maintenance_Date >= '2026-01-01'\n  AND Maintenance_Date < '2027-01-01';\n```\nNote: Since the unit cost is not directly available in the maintenance_logs table, we will join it with the inventory_logs table in the next step.\n\n### Step 3: Join Inventory Logs with Maintenance Logs\nJoin the **inventory_logs** table with the filtered **maintenance_logs** table on the Material_Code column to get the unit cost of the parts replaced.\n```sql\nSELECT \n  ml.Maintenance_Date,\n  ml.Machine,\n  ml.Maintenance_Type,\n  ml.Breakdown_Date,\n  ml.Downtime_Hours,\n  ml.Issue_Description,\n  ml.Technician,\n  ml.Parts_Replaced,\n  il.Material_Name,\n  il.Unit_Cost_Rupees,\n  ml.Parts_Replaced * il.Unit_Cost_Rupees AS Total_Cost\nFROM \n  maintenance_logs ml\n  INNER JOIN inventory_logs il\n  ON ml.Machine = il.Material_Code\nWHERE \n  ml.Machine = 'Machine-M2'\n  AND ml.Maintenance_Date >= '2026-01-01'\n  AND ml.Maintenance_Date < '2027-01-01';\n```\n\n### Step 4: Group Records by Machine and Calculate Total Cost\nGroup the records by Machine and calculate the total cost of maintenance for each machine.\n```sql\nSELECT \n  Machine,\n  SUM(Total_Cost) AS Total_Maintenance_Cost\nFROM \n  (\n    SELECT \n      ml.Maintenance_Date,\n      ml.Machine,\n      ml.Maintenance_Type,\n      ml.Breakdown_Date,\n      ml.Downtime_Hours,\n      ml.Issue_Description,\n      ml.Technician,\n      ml.Parts_Replaced,\n      il.Material_Name,\n      il.Unit_Cost_Rupees,\n      ml.Parts_Replaced * il.Unit_Cost_Rupees AS Total_Cost\n    FROM \n      maintenance_logs ml\n      INNER JOIN inventory_logs il\n      ON ml.Machine = il.Material_Code\n    WHERE \n      ml.Machine = 'Machine-M2'\n      AND ml.Maintenance_Date >= '2026-01-01'\n      AND ml.Maintenance_Date < '2027-01-01'\n  ) AS subquery\nGROUP BY \n  Machine;\n```\n\n### Step 5: Filter Results for Machine-M2\nFilter the results to get the total cost of maintenance for Machine-M2 in 2026.\n```sql\nSELECT \n  SUM(Total_Maintenance_Cost) AS Total_Maintenance_Cost\nFROM \n  (\n    SELECT \n      Machine,\n      SUM(Total_Cost) AS Total_Maintenance_Cost\n    FROM \n      (\n        SELECT \n          ml.Maintenance_Date,\n          ml.Machine,\n          ml.Maintenance_Type,\n          ml.Breakdown_Date,\n          ml.Downtime_Hours,\n          ml.Issue_Description,\n          ml.Technician,\n          ml.Parts_Replaced,\n          il.Material_Name,\n          il.Unit_Cost_Rupees,\n          ml.Parts_Replaced * il.Unit_Cost_Rupees AS Total_Cost\n        FROM \n          maintenance_logs ml\n          INNER JOIN inventory_logs il\n          ON ml.Machine = il.Material_Code\n        WHERE \n          ml.Machine = 'Machine-M2'\n          AND ml.Maintenance_Date >= '2026-01-01'\n          AND ml.Maintenance_Date < '2027-01-01'\n      ) AS subquery\n    GROUP BY \n      Machine\n  ) AS subquery;\n```\n\n### Step 6: Execute the Query\nExecute the final query to get the total cost of maintenance for Machine-M2 in 2026.\n```sql\nSELECT \n  SUM(Total_Maintenance_Cost) AS Total_Maintenance_Cost\nFROM \n  (\n    SELECT \n      Machine,\n      SUM(Total_Cost) AS Total_Maintenance_C",
    "sql_response": "```sql\nSELECT \n    SUM(Cost_Rupees) AS Total_Maintenance_Cost\nFROM \n    maintenance_logs\nWHERE \n    Machine = 'Machine-M2' AND \n    Maintenance_Date >= '2026-01-01' AND \n    Maintenance_Date < '2027-01-01';\n```\n\nThis SQL query will return the total cost of maintenance for Machine-M2 in the year 2026. The `SUM` function is used to calculate the total cost, and the `WHERE` clause is used to filter the results to only include maintenance logs for Machine-M2 in 2026.",
    "table_selection_response": "To answer the question, we need to identify the total cost of maintenance for Machine-M2 in 2026. This requires data from the `maintenance_logs` table, specifically the `Cost_Rupees` column.\n\nHowever, we also need to filter the data by the machine name and the year. Since the machine name is not explicitly mentioned in the question, we will assume it is Machine-M2. We also need to filter by the year 2026, but since the year is not explicitly mentioned in the `maintenance_logs` table, we will assume it is stored in the `Maintenance_Date` column.\n\nHere is the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Maintenance_Date\", \"Machine\", \"Cost_Rupees\"]\n}\n```\n\nNote that we also need to convert the `Maintenance_Date` column to a date format that can be easily filtered by year. This can be done using date functions in SQL, such as `YEAR()` or `EXTRACT(YEAR FROM ...)`.",
    "methodology_score": 55.55555555555556,
    "sql_score": 85.63829787234043,
    "table_column_score": 100.0,
    "response_quality_score": 91.18814706802368,
    "overall_score": 80.75888562878819,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 5,
      "expected_keyword_count": 9,
      "response_keyword_count": 98
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 70.92198581560284,
      "overall_score": 85.63829787234043,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "machine"
      ],
      "generated_columns": [
        "cost_rupees",
        "machine"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "cost_rupees",
        "maintenance_date",
        "machine"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "maintenance_date",
        "machine"
      ],
      "generated_columns": [
        "cost_rupees",
        "maintenance_date",
        "machine"
      ]
    },
    "total_latency_ms": 8811.852931976318,
    "total_tokens": 2489,
    "errors": [],
    "timestamp": "2025-11-30T12:55:45.354958"
  },
  {
    "question_id": "Easy_32",
    "question_text": "What is the percentage of failed quantity for each product?",
    "category": "Easy",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Identify the relevant table(s) and filter the data\nUse the **quality_control** table, which contains information about the inspected quantity, passed quantity, and failed quantity for each product.\n\n### Step 2: Filter the data to only include failed quantity\nApply a filter to the **quality_control** table to only include rows where the **Failed_Qty** is greater than 0.\n\n### Step 3: Calculate the total failed quantity for each product\nPerform a groupby operation on the filtered **quality_control** table to calculate the total failed quantity for each product.\n\n### Step 4: Calculate the total inspected quantity for each product\nPerform a groupby operation on the original **quality_control** table to calculate the total inspected quantity for each product.\n\n### Step 5: Calculate the percentage of failed quantity for each product\nDivide the total failed quantity for each product (calculated in Step 3) by the total inspected quantity for each product (calculated in Step 4), and multiply by 100 to get the percentage.\n\n### Step 6: Round the percentages to a reasonable number of decimal places (e.g., 2 decimal places)\nRound the calculated percentages to 2 decimal places to make them easier to read and understand.\n\n### Step 7: Sort the results by product name (optional)\nSort the results by product name in ascending or descending order, depending on the desired output.\n\n### Step 8: Display the final results\nDisplay the final results, which should show the percentage of failed quantity for each product.\n\n## SQL Code to Implement the Steps\n```sql\nWITH failed_quantity AS (\n  SELECT Product, SUM(Failed_Qty) AS total_failed_qty\n  FROM quality_control\n  WHERE Failed_Qty > 0\n  GROUP BY Product\n),\ninspected_quantity AS (\n  SELECT Product, SUM(Inspected_Qty) AS total_inspected_qty\n  FROM quality_control\n  GROUP BY Product\n)\nSELECT \n  f.Product,\n  ROUND((f.total_failed_qty / i.total_inspected_qty) * 100, 2) AS failed_percentage\nFROM failed_quantity f\nJOIN inspected_quantity i ON f.Product = i.Product\nORDER BY f.Product;\n```\nNote: The above SQL code assumes that the **quality_control** table has the columns **Product**, **Failed_Qty**, and **Inspected_Qty**. If the column names are different, adjust the code accordingly.",
    "sql_response": "```sql\nSELECT \n    q.Product, \n    ROUND((q.Failed_Qty * 100.0 / q.Inspected_Qty), 2) AS Failed_Percentage\nFROM \n    quality_control q\nGROUP BY \n    q.Product\nORDER BY \n    Failed_Percentage DESC;\n```\n\nThis SQL query will return the product names and their corresponding failed quantity percentages. The `ROUND` function is used to round the failed percentage to two decimal places. The results are sorted in descending order by failed percentage.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns that provide the necessary information. \n\nFor the percentage of failed quantity for each product, we need the total quantity inspected and the quantity that failed. \n\nBased on the available tables and columns, we can use the `quality_control` table to get the total quantity inspected and the quantity that failed.\n\nThe required tables and columns are:\n\n```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Product\", \"Failed_Qty\"]\n}\n```\n\nHowever, since we need to calculate the percentage of failed quantity, we also need to divide the Failed_Qty by the Inspected_Qty. But we can't directly get the Inspected_Qty from the quality_control table. We can get the Inspected_Qty from the same table but we need to sum it up for each product.\n\nSo, the final tables and columns are:\n\n```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Product\", \"Failed_Qty\", \"Inspected_Qty\"]\n}\n```\n\nWe can calculate the Inspected_Qty by summing up the Inspected_Qty for each product.",
    "methodology_score": 76.92307692307693,
    "sql_score": 49.8566398390342,
    "table_column_score": 88.0,
    "response_quality_score": 92.10983061790466,
    "overall_score": 71.73773008237552,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 10,
      "expected_keyword_count": 13,
      "response_keyword_count": 118
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 14.285714285714285,
      "aggregate_match": 0.0,
      "clause_match": 75.0,
      "fuzzy_score": 66.90140845070422,
      "overall_score": 49.8566398390342,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "0 / inspected_qty",
        "failed_qty"
      ],
      "generated_columns": [
        "inspected_qty",
        "product",
        "q",
        "2",
        "failed_percentage"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 88.0,
      "table_score": 100.0,
      "column_score": 80.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 66.66666666666666,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "failed_qty"
      ],
      "columns_missing": [
        "inspected_qty"
      ],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "inspected_qty",
        "failed_qty"
      ],
      "generated_columns": [
        "product",
        "failed_qty"
      ]
    },
    "total_latency_ms": 7890.169382095337,
    "total_tokens": 1964,
    "errors": [],
    "timestamp": "2025-11-30T12:55:59.503858"
  },
  {
    "question_id": "Easy_29",
    "question_text": "What is the maximum downtime hours recorded for any breakdown?",
    "category": "Easy",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Identify the relevant table\nThe question asks for maximum downtime hours recorded for any breakdown, which is related to machine maintenance. Therefore, we will use the **maintenance_logs** table.\n\n### Step 2: Filter the table for relevant data\nWe need to filter the **maintenance_logs** table to only include rows where the maintenance type is related to a breakdown. This can be done by applying a filter on the **Maintenance_Type** column.\n\n```sql\nSELECT * FROM maintenance_logs\nWHERE Maintenance_Type = 'Breakdown';\n```\n\n### Step 3: Extract downtime hours from the filtered table\nWe need to extract the **Downtime_Hours** column from the filtered table, as it contains the downtime hours recorded for each breakdown.\n\n```sql\nSELECT Downtime_Hours FROM maintenance_logs\nWHERE Maintenance_Type = 'Breakdown';\n```\n\n### Step 4: Find the maximum downtime hours\nWe need to find the maximum downtime hours recorded for any breakdown. This can be done by applying the MAX aggregation function on the **Downtime_Hours** column.\n\n```sql\nSELECT MAX(Downtime_Hours) AS Max_Downtime_Hours\nFROM maintenance_logs\nWHERE Maintenance_Type = 'Breakdown';\n```\n\n### Step 5: Combine data (if necessary)\nIn this case, we don't need to combine data from multiple tables, as the **maintenance_logs** table contains all the necessary information.\n\n### Step 6: Execute the query\nFinally, we execute the query to get the maximum downtime hours recorded for any breakdown.\n\n```sql\nSELECT MAX(Downtime_Hours) AS Max_Downtime_Hours\nFROM maintenance_logs\nWHERE Maintenance_Type = 'Breakdown';\n```\n\nThis will give us the maximum downtime hours recorded for any breakdown, which is the answer to the question.",
    "sql_response": "```sql\nSELECT MAX(Downtime_Hours) AS Max_Downtime_Hours\nFROM maintenance_logs\nWHERE Maintenance_Type = 'Breakdown';\n```\n\nThis SQL query will return the maximum downtime hours recorded for any breakdown. The `MAX` function is used to find the maximum value in the `Downtime_Hours` column, and the `WHERE` clause filters the results to only include rows where the `Maintenance_Type` is 'Breakdown'.",
    "table_selection_response": "To answer the question, we need to identify the table that contains downtime information and the column that stores downtime hours. \n\nBased on the available tables and columns, we can see that the `maintenance_logs` table contains downtime information, and the `Downtime_Hours` column stores downtime hours.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Downtime_Hours\"]\n}\n```\n\nThis will allow us to extract the maximum downtime hours recorded for any breakdown.",
    "methodology_score": 77.77777777777779,
    "sql_score": 98.17460317460318,
    "table_column_score": 80.0,
    "response_quality_score": 92.47680163383484,
    "overall_score": 86.94212460782792,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 7,
      "expected_keyword_count": 9,
      "response_keyword_count": 75
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 87.83068783068784,
      "overall_score": 98.17460317460318,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "downtime_hours",
        "maintenance_type"
      ],
      "generated_columns": [
        "downtime_hours",
        "maintenance_type"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 80.0,
      "table_score": 100.0,
      "column_score": 66.66666666666666,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 50.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "downtime_hours"
      ],
      "columns_missing": [
        "maintenance_type"
      ],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "downtime_hours",
        "maintenance_type"
      ],
      "generated_columns": [
        "downtime_hours"
      ]
    },
    "total_latency_ms": 7523.198366165161,
    "total_tokens": 1701,
    "errors": [],
    "timestamp": "2025-11-30T12:56:13.340391"
  },
  {
    "question_id": "Easy_18",
    "question_text": "How many preventive maintenance activities were performed on Machine-M2 this year?",
    "category": "Easy",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Identify the relevant table\nWe need to use the **maintenance_logs** table to find information about maintenance activities.\n\n### Step 2: Filter the data for the specified machine and year\nWe will filter the data to include only rows where the machine is 'Machine-M2' and the maintenance date is within this year (2024). We will assume the current year is 2024.\n\n```sql\nSELECT *\nFROM maintenance_logs\nWHERE Machine = 'Machine-M2'\n  AND Maintenance_Date >= '2024-01-01'\n  AND Maintenance_Date < '2025-01-01';\n```\n\n### Step 3: Identify the type of maintenance activities\nWe need to filter the data to include only rows where the maintenance type is 'Preventive'.\n\n```sql\nSELECT *\nFROM (\n  SELECT *\n  FROM maintenance_logs\n  WHERE Machine = 'Machine-M2'\n    AND Maintenance_Date >= '2024-01-01'\n    AND Maintenance_Date < '2025-01-01'\n) AS filtered_data\nWHERE Maintenance_Type = 'Preventive';\n```\n\n### Step 4: Count the number of preventive maintenance activities\nWe will use the COUNT() function to count the number of rows in the filtered data.\n\n```sql\nSELECT COUNT(*)\nFROM (\n  SELECT *\n  FROM maintenance_logs\n  WHERE Machine = 'Machine-M2'\n    AND Maintenance_Date >= '2024-01-01'\n    AND Maintenance_Date < '2025-01-01'\n) AS filtered_data\nWHERE Maintenance_Type = 'Preventive';\n```\n\n### Step 5: Combine the code into a single query\nWe can combine the above steps into a single query.\n\n```sql\nSELECT COUNT(*)\nFROM maintenance_logs\nWHERE Machine = 'Machine-M2'\n  AND Maintenance_Date >= '2024-01-01'\n  AND Maintenance_Date < '2025-01-01'\n  AND Maintenance_Type = 'Preventive';\n```\n\nThis query will return the number of preventive maintenance activities performed on Machine-M2 this year.",
    "sql_response": "```sql\nSELECT COUNT(*) \nFROM maintenance_logs \nWHERE Maintenance_Date >= DATE('2023-01-01') \n  AND Maintenance_Date < DATE('2024-01-01') \n  AND Machine = 'Machine-M2' \n  AND Maintenance_Type = 'Preventive';\n```\n\nThis SQL query will return the count of preventive maintenance activities performed on Machine-M2 in the current year (2023). The `DATE('2023-01-01')` and `DATE('2024-01-01')` are used to represent the start and end of the year, respectively.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns that contain information about maintenance activities performed on Machine-M2 this year.\n\nBased on the available tables and columns, we can see that the `maintenance_logs` table contains information about maintenance activities, including the `Machine` column which can be used to filter for Machine-M2.\n\nHowever, we also need to filter for maintenance activities performed this year. The `maintenance_logs` table contains a `Maintenance_Date` column which can be used to filter for the current year.\n\nTherefore, the tables and columns needed to answer this question are:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Maintenance_Date\", \"Maintenance_Type\"]\n}\n```\n\nNote that we also include the `Maintenance_Type` column, as the question asks for preventive maintenance activities, and we can filter for maintenance activities with a specific type (e.g. \"Preventive\") in the `maintenance_logs` table.",
    "methodology_score": 83.33333333333334,
    "sql_score": 71.34453781512605,
    "table_column_score": 88.0,
    "response_quality_score": 92.39713788032532,
    "overall_score": 81.21030202332665,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 10,
      "expected_keyword_count": 12,
      "response_keyword_count": 68
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 0.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 75.63025210084034,
      "overall_score": 71.34453781512605,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "machine"
      ],
      "generated_columns": [
        "maintenance_date"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 88.0,
      "table_score": 100.0,
      "column_score": 80.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 66.66666666666666,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "maintenance_date",
        "maintenance_type"
      ],
      "columns_missing": [
        "machine"
      ],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "maintenance_date",
        "maintenance_type",
        "machine"
      ],
      "generated_columns": [
        "maintenance_date",
        "maintenance_type"
      ]
    },
    "total_latency_ms": 7602.862119674683,
    "total_tokens": 1872,
    "errors": [],
    "timestamp": "2025-11-30T12:56:27.233379"
  },
  {
    "question_id": "Complex_32",
    "question_text": "Which machine has the highest average downtime due to breakdowns in the last quarter, and what is the total cost associated with those breakdowns?",
    "category": "Complex",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Answering the Question: Highest Average Downtime and Total Cost of Breakdowns\n\n### Step 1: Filter the maintenance_logs table for the last quarter\nUse the **maintenance_logs** table and filter the data for the last quarter (January to March, assuming the current date is March 31st). Apply the following filter:\n```sql\nWHERE Maintenance_Date >= DATEADD(quarter, DATEDIFF(quarter, 0, GETDATE()) - 1, 0)\n  AND Maintenance_Date < DATEADD(quarter, DATEDIFF(quarter, 0, GETDATE()), 0)\n```\n### Step 2: Filter the maintenance_logs table for breakdowns\nFrom the filtered data in Step 1, select only the rows where the **Maintenance_Type** is 'Breakdown'. This will give us the maintenance logs related to breakdowns.\n```sql\nWHERE Maintenance_Type = 'Breakdown'\n```\n### Step 3: Calculate the total downtime hours for each machine\nGroup the filtered data by **Machine** and calculate the total downtime hours for each machine.\n```sql\nGROUP BY Machine\nSELECT Machine, SUM(Downtime_Hours) AS Total_Downtime_Hours\n```\n### Step 4: Calculate the average downtime hours for each machine\nFrom the grouped data in Step 3, calculate the average downtime hours for each machine.\n```sql\nSELECT Machine, AVG(Total_Downtime_Hours) AS Average_Downtime_Hours\n```\n### Step 5: Join the maintenance_logs table with the inventory_logs table to get the total cost of breakdowns\nJoin the **maintenance_logs** table with the **inventory_logs** table on the **Machine** column. Filter the data to include only the rows where the **Maintenance_Date** is in the last quarter (as in Step 1). Calculate the total cost of breakdowns for each machine.\n```sql\nJOIN inventory_logs ON maintenance_logs.Machine = inventory_logs.Material_Code\nWHERE maintenance_logs.Maintenance_Date >= DATEADD(quarter, DATEDIFF(quarter, 0, GETDATE()) - 1, 0)\n  AND maintenance_logs.Maintenance_Date < DATEADD(quarter, DATEDIFF(quarter, 0, GETDATE()), 0)\nGROUP BY Machine\nSELECT Machine, SUM(maintenance_logs.Downtime_Hours * inventory_logs.Unit_Cost_Rupees) AS Total_Cost\n```\n### Step 6: Combine the results from Steps 4 and 5 to get the machine with the highest average downtime and total cost of breakdowns\nJoin the results from Steps 4 and 5 on the **Machine** column. Sort the data by **Average_Downtime_Hours** in descending order and select the top row.\n```sql\nJOIN (\n  SELECT Machine, AVG(Total_Downtime_Hours) AS Average_Downtime_Hours\n  FROM (\n    SELECT Machine, SUM(Downtime_Hours) AS Total_Downtime_Hours\n    FROM maintenance_logs\n    WHERE Maintenance_Type = 'Breakdown'\n    GROUP BY Machine\n  ) AS subquery\n) AS avg_downtime ON maintenance_logs.Machine = avg_downtime.Machine\nJOIN (\n  SELECT Machine, SUM(maintenance_logs.Downtime_Hours * inventory_logs.Unit_Cost_Rupees) AS Total_Cost\n  FROM maintenance_logs\n  JOIN inventory_logs ON maintenance_logs.Machine = inventory_logs.Material_Code\n  WHERE maintenance_logs.Maintenance_Date >= DATEADD(quarter, DATEDIFF(quarter, 0, GETDATE()) - 1, 0)\n    AND maintenance_logs.Maintenance_Date < DATEADD(quarter, DATEDIFF(quarter, 0, GETDATE()), 0)\n  GROUP BY Machine\n) AS total_cost ON maintenance_logs.Machine = total_cost.Machine\nORDER BY avg_downtime.Average_Downtime_Hours DESC\nSELECT TOP 1 Machine, Average_Downtime_Hours, Total_Cost\n```\n### Step 7: Execute the query and get the final result\nExecute the query in Step 6 to get the machine with the highest average downtime and total cost of breakdowns in the last quarter.",
    "sql_response": "```sql\nWITH breakdowns AS (\n  SELECT \n    Machine,\n    SUM(Downtime_Hours) AS total_downtime_hours,\n    COUNT(*) AS breakdown_count\n  FROM \n    maintenance_logs\n  WHERE \n    Maintenance_Type = 'Breakdown' AND \n    Maintenance_Date >= DATE_TRUNC('quarter', CURRENT_DATE) - INTERVAL '1 quarter'\n  GROUP BY \n    Machine\n),\naverage_downtime AS (\n  SELECT \n    Machine,\n    total_downtime_hours / breakdown_count AS average_downtime_hours\n  FROM \n    breakdowns\n)\nSELECT \n  Machine,\n  AVG(average_downtime_hours) AS average_downtime_hours,\n  SUM(CASE WHEN Maintenance_Type = 'Breakdown' THEN Cost_Rupees ELSE 0 END) AS total_breakdown_cost\nFROM \n  maintenance_logs\nWHERE \n  Maintenance_Date >= DATE_TRUNC('quarter', CURRENT_DATE) - INTERVAL '1 quarter'\nGROUP BY \n  Machine\nORDER BY \n  AVG(average_downtime_hours) DESC\nLIMIT 1;\n```\n\nThis query first calculates the total downtime hours and breakdown count for each machine in the last quarter. Then, it calculates the average downtime hours for each machine. Finally, it selects the machine with the highest average downtime hours and the total cost associated with its breakdowns in the last quarter.",
    "table_selection_response": "To answer this question, we need to identify the machine with the highest average downtime due to breakdowns in the last quarter and the total cost associated with those breakdowns. \n\nWe can achieve this by analyzing the following tables and columns:\n\n- **maintenance_logs**: This table contains information about machine breakdowns, including the downtime hours and cost associated with each breakdown.\n- **production_logs**: This table contains information about production, including downtime minutes. However, we are more interested in the maintenance_logs table for breakdowns.\n\nHowever, to get the total cost associated with those breakdowns, we need to join the **maintenance_logs** table with the **production_logs** table to get the total downtime minutes and then calculate the average downtime per machine.\n\nBut, since we are looking for the total cost associated with those breakdowns, we can directly use the **maintenance_logs** table.\n\nTo get the machine with the highest average downtime due to breakdowns, we need to group the data by machine and calculate the average downtime hours.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Machine\", \"Downtime_Hours\", \"Cost_Rupees\"]\n}\n```\n\nHowever, to get the total cost associated with those breakdowns, we need to sum up the cost for each machine. \n\nHere's the updated JSON object:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Machine\", \"Downtime_Hours\", \"Cost_Rupees\"]\n}\n```\n\nBut, to get the machine with the highest average downtime due to breakdowns in the last quarter, we need to filter the data for the last quarter. We can do this by filtering the data for the last 3 months.\n\nHere's the updated JSON object:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Machine\", \"Downtime_Hours\", \"Cost_Rupees\", \"Maintenance_Date\"]\n}\n```\n\nWe can then filter the data for the last quarter using the Maintenance_Date column.\n\nTo get the total cost associated with those breakdowns, we can sum up the Cost_Rupees for each machine.\n\nHere's the updated JSON object:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Machine\", \"Downtime_Hours\", \"Cost_Rupees\", \"Maintenance_Date\"]\n}\n```\n\nWe can then filter the data for the last quarter using the Maintenance_Date column and sum up the Cost_Rupees for each machine.\n\nTo get the machine with the highest average downtime due to breakdowns in the last quarter, we need to group the data by machine, calculate the average downtime hours, and then get the machine with the highest average downtime hours.\n\nHere's the updated JSON object:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Machine\", \"Downtime_Hours\", \"Cost_Rupees\", \"Maintenance_Date\"]\n}\n```\n\nWe can then filter the data for the last quarter using the Maintenance_Date column, group the data by machine, calculate the average downtime hours, and then get the machine with the highest average downtime hours.\n\nTo get the total cost associated with those breakdowns, we can sum up the Cost_Rupees for each machine.\n\nHere's the final JSON object:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Machine\", \"Downtime_Hours\", \"Cost_Rupees\", \"Maintenance_Date\"]\n}\n```",
    "methodology_score": 68.96551724137932,
    "sql_score": 70.38559754851889,
    "table_column_score": 84.99999999999999,
    "response_quality_score": 89.49200391769409,
    "overall_score": 75.52381470616481,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 20,
      "expected_keyword_count": 29,
      "response_keyword_count": 99
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 27.27272727272727,
      "aggregate_match": 75.0,
      "clause_match": 100.0,
      "fuzzy_score": 57.11610486891385,
      "overall_score": 70.38559754851889,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "totalcost",
        "avgdowntime",
        "maintenance_type",
        "machine",
        "cost_rupees",
        "downtime_hours"
      ],
      "generated_columns": [
        "maintenance_date",
        "total_downtime_hours / breakdown_count",
        "case when maintenance_type = breakdown then cost_rupees else 0 end",
        "maintenance_type",
        "machine",
        "downtime_hours",
        "avg",
        "average_downtime_hours"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 84.99999999999999,
      "table_score": 100.0,
      "column_score": 74.99999999999999,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 60.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "cost_rupees",
        "downtime_hours",
        "machine"
      ],
      "columns_missing": [
        "maintenance_date",
        "maintenance_type"
      ],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "maintenance_date",
        "maintenance_type",
        "machine",
        "cost_rupees",
        "downtime_hours"
      ],
      "generated_columns": [
        "cost_rupees",
        "downtime_hours",
        "machine"
      ]
    },
    "total_latency_ms": 10507.996082305908,
    "total_tokens": 3021,
    "errors": [],
    "timestamp": "2025-11-30T12:56:44.093389"
  },
  {
    "question_id": "Easy_14",
    "question_text": "What is the total wastage in kilograms for 'Steel Sheet 101' this year?",
    "category": "Easy",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Identify the Relevant Table\nThe question asks for the total wastage in kilograms for 'Steel Sheet 101' this year. The relevant table for this information is **inventory_logs**, as it contains data on material consumption and wastage.\n\n### Step 2: Filter the Data by Material Name and Year\nTo get the data for 'Steel Sheet 101' this year, we need to filter the **inventory_logs** table by the following conditions:\n- Material_Name = 'Steel Sheet 101'\n- Date is within the current year (we can use the current year as a filter or assume it's 2024 for simplicity)\n\n```sql\nSELECT *\nFROM inventory_logs\nWHERE Material_Name = 'Steel Sheet 101'\n  AND Date >= '2024-01-01'\n  AND Date < '2025-01-01';\n```\n\n### Step 3: Calculate the Total Wastage\nOnce we have the filtered data, we can calculate the total wastage in kilograms by summing up the Wastage_Kg column.\n\n```sql\nSELECT SUM(Wastage_Kg) AS Total_Wastage\nFROM (\n  SELECT *\n  FROM inventory_logs\n  WHERE Material_Name = 'Steel Sheet 101'\n    AND Date >= '2024-01-01'\n    AND Date < '2025-01-01'\n) AS filtered_data;\n```\n\n### Step 4: Execute the Query\nWe can execute the query to get the total wastage in kilograms for 'Steel Sheet 101' this year.\n\n### Step 5: Verify the Results\nVerify the results by checking if the total wastage is accurate and makes sense in the context of the manufacturing operations.\n\n### Note:\nIf the data is not up-to-date or if there are any issues with the data quality, the results may not be accurate. In such cases, we may need to revisit the data collection process or perform additional data cleaning and validation steps.",
    "sql_response": "```sql\nSELECT SUM(Wastage_Kg) AS Total_Wastage\nFROM inventory_logs\nWHERE Material_Name = 'Steel Sheet 101'\n  AND Date >= DATE('2022-01-01')\n  AND Date <= DATE('2022-12-31');\n```\n\nThis SQL query will return the total wastage in kilograms for 'Steel Sheet 101' in the year 2022. The `DATE('2022-01-01')` and `DATE('2022-12-31')` are used to specify the start and end dates of the year 2022. The `SUM(Wastage_Kg)` function is used to calculate the total wastage.",
    "table_selection_response": "To answer the question, we need to identify the total wastage in kilograms for 'Steel Sheet 101' this year. This involves analyzing the inventory logs to find the wastage of 'Steel Sheet 101' in the current year.\n\nBased on the available tables and columns, we can use the `inventory_logs` table to find the wastage of 'Steel Sheet 101'. We need to filter the data by 'Material_Name' and 'Date' to get the wastage for the current year.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Material_Name\", \"Wastage_Kg\"]\n}\n```\n\nHowever, we need to note that 'Wastage_Kg' is not a column in the `inventory_logs` table. Instead, we can use the 'Material_Name' column to filter the data and then calculate the total wastage for 'Steel Sheet 101' in the current year.\n\nTo calculate the total wastage, we can use a SQL query like this:\n\n```sql\nSELECT SUM(Wastage_Kg) AS Total_Wastage\nFROM inventory_logs\nWHERE Material_Name = 'Steel Sheet 101'\n  AND Date >= DATE_TRUNC('year', CURRENT_DATE);\n```\n\nThis query will give us the total wastage in kilograms for 'Steel Sheet 101' this year.",
    "methodology_score": 69.23076923076923,
    "sql_score": 72.53246753246754,
    "table_column_score": 88.0,
    "response_quality_score": 91.70873165130615,
    "overall_score": 77.32646757072501,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 9,
      "expected_keyword_count": 13,
      "response_keyword_count": 91
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 50.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 66.88311688311688,
      "overall_score": 72.53246753246754,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg"
      ],
      "generated_columns": [
        "wastage_kg",
        "material_name"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 88.0,
      "table_score": 100.0,
      "column_score": 80.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 66.66666666666666,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "wastage_kg",
        "material_name"
      ],
      "columns_missing": [
        "date"
      ],
      "columns_extra": [],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "material_name",
        "date"
      ],
      "generated_columns": [
        "wastage_kg",
        "material_name"
      ]
    },
    "total_latency_ms": 8291.268348693848,
    "total_tokens": 1971,
    "errors": [],
    "timestamp": "2025-11-30T12:56:58.696961"
  },
  {
    "question_id": "Complex_18",
    "question_text": "What is the average unit cost of materials purchased from each supplier, and which supplier has the lowest average cost?",
    "category": "Complex",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Identify the Relevant Table and Filter\nUse the **inventory_logs** table, which contains information about material purchases. Filter the data to include only rows where the **Received_Kg** is greater than 0, indicating a material purchase.\n\n```sql\nSELECT *\nFROM inventory_logs\nWHERE Received_Kg > 0;\n```\n\n### Step 2: Calculate the Total Cost of Materials Purchased from Each Supplier\nPerform a group by operation on the filtered data to calculate the total cost of materials purchased from each supplier. Use the **Supplier** column as the grouping variable and the **Unit_Cost_Rupees** column to calculate the total cost.\n\n```sql\nSELECT Supplier, SUM(Received_Kg * Unit_Cost_Rupees) AS Total_Cost\nFROM inventory_logs\nWHERE Received_Kg > 0\nGROUP BY Supplier;\n```\n\n### Step 3: Calculate the Total Quantity of Materials Purchased from Each Supplier\nPerform another group by operation on the filtered data to calculate the total quantity of materials purchased from each supplier. Use the **Supplier** column as the grouping variable and the **Received_Kg** column to calculate the total quantity.\n\n```sql\nSELECT Supplier, SUM(Received_Kg) AS Total_Quantity\nFROM inventory_logs\nWHERE Received_Kg > 0\nGROUP BY Supplier;\n```\n\n### Step 4: Calculate the Average Unit Cost of Materials Purchased from Each Supplier\nJoin the results of steps 2 and 3 on the **Supplier** column to calculate the average unit cost of materials purchased from each supplier. Divide the total cost by the total quantity to get the average unit cost.\n\n```sql\nSELECT \n  a.Supplier, \n  a.Total_Cost, \n  b.Total_Quantity, \n  a.Total_Cost / b.Total_Quantity AS Average_Unit_Cost\nFROM \n  (SELECT Supplier, SUM(Received_Kg * Unit_Cost_Rupees) AS Total_Cost\n   FROM inventory_logs\n   WHERE Received_Kg > 0\n   GROUP BY Supplier) a\n  INNER JOIN \n  (SELECT Supplier, SUM(Received_Kg) AS Total_Quantity\n   FROM inventory_logs\n   WHERE Received_Kg > 0\n   GROUP BY Supplier) b\n  ON a.Supplier = b.Supplier;\n```\n\n### Step 5: Identify the Supplier with the Lowest Average Unit Cost\nUse the result of step 4 to identify the supplier with the lowest average unit cost. Use the **Average_Unit_Cost** column to determine the supplier with the lowest average cost.\n\n```sql\nSELECT Supplier, Average_Unit_Cost\nFROM \n  (SELECT \n     a.Supplier, \n     a.Total_Cost, \n     b.Total_Quantity, \n     a.Total_Cost / b.Total_Quantity AS Average_Unit_Cost\n   FROM \n     (SELECT Supplier, SUM(Received_Kg * Unit_Cost_Rupees) AS Total_Cost\n      FROM inventory_logs\n      WHERE Received_Kg > 0\n      GROUP BY Supplier) a\n     INNER JOIN \n     (SELECT Supplier, SUM(Received_Kg) AS Total_Quantity\n      FROM inventory_logs\n      WHERE Received_Kg > 0\n      GROUP BY Supplier) b\n     ON a.Supplier = b.Supplier)\nWHERE Average_Unit_Cost = (SELECT MIN(Average_Unit_Cost) FROM (SELECT ...));\n```\n\n### Step 6: Finalize the Answer\nThe final answer is the supplier with the lowest average unit cost, which can be obtained from the result of step 5.",
    "sql_response": "```sql\nSELECT \n  Supplier, \n  AVG(Unit_Cost_Rupees) AS Average_Unit_Cost\nFROM \n  inventory_logs\nGROUP BY \n  Supplier\nORDER BY \n  Average_Unit_Cost ASC;\n```\n\nThis SQL query will return a list of suppliers along with their average unit cost. The `ORDER BY` clause is used to sort the results in ascending order, so the supplier with the lowest average cost will be at the top.\n\nIf you want to get the supplier with the lowest average cost, you can use the `MIN` function along with the `ORDER BY` clause:\n\n```sql\nSELECT \n  Supplier, \n  AVG(Unit_Cost_Rupees) AS Average_Unit_Cost\nFROM \n  inventory_logs\nGROUP BY \n  Supplier\nORDER BY \n  Average_Unit_Cost ASC\nLIMIT 1;\n```\n\nThis query will return the supplier with the lowest average cost.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns that provide information about the materials purchased and their unit costs. Based on the available tables and columns, we can use the `inventory_logs` table to get the unit costs and the suppliers.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Supplier\", \"Unit_Cost_Rupees\"]\n}\n```\n\nHowever, to calculate the average unit cost for each supplier, we need to group the data by supplier and calculate the average unit cost. This can be achieved using a SQL query with a GROUP BY clause. \n\nHere's an example SQL query:\n\n```sql\nSELECT \n  Supplier, \n  AVG(Unit_Cost_Rupees) AS Average_Unit_Cost\nFROM \n  inventory_logs\nGROUP BY \n  Supplier\nORDER BY \n  Average_Unit_Cost ASC;\n```\n\nThis query will return the supplier with the lowest average unit cost at the top of the result set.",
    "methodology_score": 75.0,
    "sql_score": 82.18774703557312,
    "table_column_score": 100.0,
    "response_quality_score": 90.15667939186096,
    "overall_score": 85.2813794016367,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 12,
      "expected_keyword_count": 16,
      "response_keyword_count": 91
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 50.0,
      "aggregate_match": 100.0,
      "clause_match": 80.0,
      "fuzzy_score": 84.58498023715416,
      "overall_score": 82.18774703557312,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "unit_cost_rupees",
        "supplier",
        "avgunitcost"
      ],
      "generated_columns": [
        "average_unit_cost",
        "unit_cost_rupees",
        "supplier"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "unit_cost_rupees",
        "supplier"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "unit_cost_rupees",
        "supplier"
      ],
      "generated_columns": [
        "unit_cost_rupees",
        "supplier"
      ]
    },
    "total_latency_ms": 9843.320608139038,
    "total_tokens": 2288,
    "errors": [],
    "timestamp": "2025-11-30T12:57:15.167493"
  },
  {
    "question_id": "Complex_1",
    "question_text": "What is the average downtime (in minutes) per shift for each machine in the last quarter?",
    "category": "Complex",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Filter the production_logs table to include only the last quarter's data\nUse the `production_logs` table and filter the data to include only the last quarter's data. Assuming the current date is 2023-11-30, the last quarter would be 2023-09-01 to 2023-11-30. Apply the following filter:\n```sql\nWHERE Date >= '2023-09-01' AND Date <= '2023-11-30'\n```\n\n### Step 2: Group the filtered data by Line_Machine and Shift\nGroup the filtered data by `Line_Machine` and `Shift` to calculate the total downtime for each machine per shift.\n```sql\nGROUP BY Line_Machine, Shift\n```\n\n### Step 3: Calculate the total downtime for each group\nCalculate the total downtime for each group by summing up the `Downtime_Minutes` column.\n```sql\nSUM(Downtime_Minutes) AS Total_Downtime_Minutes\n```\n\n### Step 4: Calculate the average downtime for each group\nCalculate the average downtime for each group by dividing the total downtime by the count of shifts.\n```sql\nAVG(Total_Downtime_Minutes) AS Average_Downtime_Minutes\n```\n\n### Step 5: Select the required columns and perform the aggregations\nSelect the required columns (`Line_Machine`, `Shift`, and `Average_Downtime_Minutes`) and perform the aggregations.\n```sql\nSELECT Line_Machine, Shift, Average_Downtime_Minutes\n```\n\n### Step 6: Order the results by Line_Machine and Shift\nOrder the results by `Line_Machine` and `Shift` for easier analysis.\n```sql\nORDER BY Line_Machine, Shift\n```\n\n### Step 7: Execute the query\nExecute the query to get the final results.\n\n## Final Query\n```sql\nSELECT \n  Line_Machine, \n  Shift, \n  AVG(Downtime_Minutes) AS Average_Downtime_Minutes\nFROM \n  production_logs\nWHERE \n  Date >= '2023-09-01' AND Date <= '2023-11-30'\nGROUP BY \n  Line_Machine, \n  Shift\nORDER BY \n  Line_Machine, \n  Shift\n```\n\nThis query will provide the average downtime (in minutes) per shift for each machine in the last quarter.",
    "sql_response": "```sql\nSELECT \n    Line_Machine,\n    Shift,\n    AVG(Downtime_Minutes) AS Average_Downtime_Minutes\nFROM \n    production_logs\nWHERE \n    Date >= DATE_TRUNC('quarter', CURRENT_DATE) - INTERVAL '1 quarter'\nGROUP BY \n    Line_Machine,\n    Shift\nORDER BY \n    Line_Machine,\n    Shift;\n```\n\nThis SQL query will return the average downtime (in minutes) per shift for each machine in the last quarter. The `DATE_TRUNC` function is used to get the start date of the last quarter, and the `INTERVAL` expression is used to subtract one quarter from it. The `GROUP BY` clause groups the results by machine and shift, and the `AVG` function calculates the average downtime for each group. The results are ordered by machine and shift for easier analysis.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns required to calculate the average downtime per shift for each machine in the last quarter.\n\n### Tables:\n- `production_logs`: This table contains information about production, including downtime minutes.\n\n### Columns:\n- `Date`: To filter the data for the last quarter.\n- `Shift`: To group the data by shift.\n- `Line_Machine`: To identify the machine.\n- `Downtime_Minutes`: To calculate the average downtime.\n\n### Additional Considerations:\n- We need to filter the data for the last quarter, so we'll need to use a date range to select the relevant data.\n- We'll need to group the data by machine and shift to calculate the average downtime per shift for each machine.\n\n### JSON Response:\n```json\n{\n  \"tables\": [\"production_logs\"],\n  \"columns\": [\"Date\", \"Shift\", \"Line_Machine\", \"Downtime_Minutes\"]\n}\n```\nNote that we don't need to include `Actual_Qty` in the columns array, as it's not required to calculate the average downtime.",
    "methodology_score": 76.92307692307693,
    "sql_score": 82.02493765586036,
    "table_column_score": 100.0,
    "response_quality_score": 91.01352000236511,
    "overall_score": 85.88700325671073,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 10,
      "expected_keyword_count": 13,
      "response_keyword_count": 82
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 60.0,
      "aggregate_match": 100.0,
      "clause_match": 80.0,
      "fuzzy_score": 66.83291770573567,
      "overall_score": 82.02493765586036,
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "pl",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ]
    },
    "total_latency_ms": 8986.479997634888,
    "total_tokens": 2047,
    "errors": [],
    "timestamp": "2025-11-30T12:57:30.502402"
  },
  {
    "question_id": "Easy_12",
    "question_text": "How many breakdown maintenance events occurred in the last quarter?",
    "category": "Easy",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Identify the Relevant Table\nUse the **maintenance_logs** table as it contains information about maintenance events, including breakdowns.\n\n### Step 2: Filter the Data for the Last Quarter\nApply a filter to the **maintenance_logs** table to include only records from the last quarter. Assuming the current date is December 31, 2023, and the quarter is from October 1 to December 31, the filter would be:\n```sql\nWHERE Maintenance_Date >= '2023-10-01' AND Maintenance_Date <= '2023-12-31'\n```\n### Step 3: Identify Breakdown Maintenance Events\nFilter the data to include only breakdown maintenance events. This can be done by applying a filter on the **Maintenance_Type** column:\n```sql\nWHERE Maintenance_Type = 'Breakdown'\n```\n### Step 4: Count the Number of Breakdown Events\nPerform a count aggregation on the filtered data to get the total number of breakdown maintenance events:\n```sql\nSELECT COUNT(*) AS Breakdown_Count\nFROM maintenance_logs\nWHERE Maintenance_Date >= '2023-10-01' AND Maintenance_Date <= '2023-12-31'\nAND Maintenance_Type = 'Breakdown'\n```\n### Step 5: No Additional Steps Required\nSince we have already filtered and aggregated the data, no further steps are required to answer the question.\n\nThe final query to answer the question would be:\n```sql\nSELECT COUNT(*) AS Breakdown_Count\nFROM maintenance_logs\nWHERE Maintenance_Date >= '2023-10-01' AND Maintenance_Date <= '2023-12-31'\nAND Maintenance_Type = 'Breakdown'\n```",
    "sql_response": "```sql\nSELECT \n    COUNT(*) AS Breakdown_Maintenance_Events\nFROM \n    maintenance_logs\nWHERE \n    Maintenance_Date >= DATE_TRUNC('quarter', CURRENT_DATE - INTERVAL '1 quarter')\n    AND Maintenance_Type = 'Breakdown';\n```\n\nThis SQL query will return the total number of breakdown maintenance events that occurred in the last quarter. The `DATE_TRUNC` function is used to truncate the date to the quarter level, and the `CURRENT_DATE - INTERVAL '1 quarter'` expression is used to get the start date of the last quarter. The `Maintenance_Type = 'Breakdown'` condition filters the results to only include breakdown maintenance events.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns that contain information about maintenance events, specifically breakdown maintenance events, and their occurrence in the last quarter.\n\nBased on the available tables and columns, we can see that the `maintenance_logs` table contains information about maintenance events, including breakdown maintenance events.\n\nTo filter the data for the last quarter, we can use the `Maintenance_Date` column in the `maintenance_logs` table.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Maintenance_Date\", \"Maintenance_Type\"]\n}\n```\n\nHowever, we also need to filter the data for the last quarter, which means we need to consider the date range. We can use the `Date` column in the `maintenance_logs` table to filter the data.\n\nHere's the updated JSON object:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Maintenance_Date\", \"Maintenance_Type\"]\n}\n```\n\nTo answer the question, we need to count the number of breakdown maintenance events in the last quarter. We can use the `Maintenance_Type` column to filter the data for breakdown maintenance events.\n\nHere's the updated JSON object:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Maintenance_Date\", \"Maintenance_Type\"]\n}\n```\n\nWe can use a SQL query to count the number of breakdown maintenance events in the last quarter:\n\n```sql\nSELECT COUNT(*) \nFROM maintenance_logs \nWHERE Maintenance_Date >= DATE_TRUNC('quarter', CURRENT_DATE - INTERVAL '1 quarter') \n  AND Maintenance_Type = 'Breakdown';\n```\n\nThis query will return the count of breakdown maintenance events in the last quarter.",
    "methodology_score": 70.0,
    "sql_score": 70.5351170568562,
    "table_column_score": 70.0,
    "response_quality_score": 91.02481079101562,
    "overall_score": 72.28977204900123,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 7,
      "expected_keyword_count": 10,
      "response_keyword_count": 74
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 0.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 70.23411371237458,
      "overall_score": 70.5351170568562,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "maintenance_type"
      ],
      "generated_columns": [
        "maintenance_date"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 70.0,
      "table_score": 100.0,
      "column_score": 50.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 50.0,
      "column_recall": 50.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "maintenance_type"
      ],
      "columns_missing": [
        "breakdown_date"
      ],
      "columns_extra": [
        "maintenance_date"
      ],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "breakdown_date",
        "maintenance_type"
      ],
      "generated_columns": [
        "maintenance_date",
        "maintenance_type"
      ]
    },
    "total_latency_ms": 8975.189208984375,
    "total_tokens": 1944,
    "errors": [],
    "timestamp": "2025-11-30T12:57:45.733871"
  },
  {
    "question_id": "Complex_7",
    "question_text": "What are the top 3 defect types that lead to rework for Widget-B, and what is their total rework count?",
    "category": "Complex",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Answering the Question: Top 3 Defect Types for Rework of Widget-B\n\n### Step 1: Filter Quality Control Data for Widget-B\nUse the **quality_control** table and filter the data for Widget-B by applying the following conditions:\n- Product = 'Widget-B'\n- Rework_Count > 0 (to focus on defect types that lead to rework)\n\n```sql\nSELECT *\nFROM quality_control\nWHERE Product = 'Widget-B' AND Rework_Count > 0;\n```\n\n### Step 2: Group Defect Types by Count of Rework\nGroup the filtered data by **Defect_Type** and calculate the total **Rework_Count** for each defect type.\n\n```sql\nSELECT Defect_Type, SUM(Rework_Count) AS Total_Rework_Count\nFROM (\n  SELECT *\n  FROM quality_control\n  WHERE Product = 'Widget-B' AND Rework_Count > 0\n) AS subquery\nGROUP BY Defect_Type;\n```\n\n### Step 3: Rank Defect Types by Total Rework Count\nRank the defect types by their total rework count in descending order.\n\n```sql\nSELECT Defect_Type, Total_Rework_Count,\n       DENSE_RANK() OVER (ORDER BY Total_Rework_Count DESC) AS Rank\nFROM (\n  SELECT Defect_Type, SUM(Rework_Count) AS Total_Rework_Count\n  FROM (\n    SELECT *\n    FROM quality_control\n    WHERE Product = 'Widget-B' AND Rework_Count > 0\n  ) AS subquery\n  GROUP BY Defect_Type\n) AS subquery;\n```\n\n### Step 4: Select Top 3 Defect Types\nSelect the top 3 defect types with the highest total rework count.\n\n```sql\nSELECT Defect_Type, Total_Rework_Count\nFROM (\n  SELECT Defect_Type, Total_Rework_Count,\n         DENSE_RANK() OVER (ORDER BY Total_Rework_Count DESC) AS Rank\n  FROM (\n    SELECT Defect_Type, SUM(Rework_Count) AS Total_Rework_Count\n    FROM (\n      SELECT *\n      FROM quality_control\n      WHERE Product = 'Widget-B' AND Rework_Count > 0\n    ) AS subquery\n    GROUP BY Defect_Type\n  ) AS subquery\n) AS subquery\nWHERE Rank <= 3;\n```\n\n### Step 5: Combine Results (Optional)\nIf needed, combine the results with other data, such as the total rework count for each defect type.\n\n```sql\nSELECT Defect_Type, Total_Rework_Count\nFROM (\n  SELECT Defect_Type, SUM(Rework_Count) AS Total_Rework_Count\n  FROM (\n    SELECT *\n    FROM quality_control\n    WHERE Product = 'Widget-B' AND Rework_Count > 0\n  ) AS subquery\n  GROUP BY Defect_Type\n) AS subquery\nWHERE Defect_Type IN (\n  SELECT Defect_Type\n  FROM (\n    SELECT Defect_Type, Total_Rework_Count,\n           DENSE_RANK() OVER (ORDER BY Total_Rework_Count DESC) AS Rank\n    FROM (\n      SELECT Defect_Type, SUM(Rework_Count) AS Total_Rework_Count\n      FROM (\n        SELECT *\n        FROM quality_control\n        WHERE Product = 'Widget-B' AND Rework_Count > 0\n      ) AS subquery\n      GROUP BY Defect_Type\n    ) AS subquery\n  ) AS subquery\n  WHERE Rank <= 3\n);\n```\n\nThe final result will provide the top 3 defect types that lead to rework for Widget-B, along with their total rework count.",
    "sql_response": "```sql\nSELECT \n    Defect_Type,\n    SUM(Rework_Count) AS Total_Rework_Count\nFROM \n    quality_control\nWHERE \n    Product = 'Widget-B'\nGROUP BY \n    Defect_Type\nORDER BY \n    Total_Rework_Count DESC\nLIMIT 3;\n```\n\nThis SQL query will return the top 3 defect types that lead to rework for Widget-B, along with their total rework count. The `GROUP BY` clause groups the results by defect type, and the `ORDER BY` clause sorts the results in descending order by total rework count. The `LIMIT 3` clause limits the results to the top 3 defect types.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns that contain information about defect types and rework counts for Widget-B.\n\nBased on the available tables and columns, we can see that the `quality_control` table contains information about defects and rework counts. Specifically, the `Defect_Type` column in this table contains the types of defects, and the `Rework_Count` column contains the count of rework for each defect type.\n\nHowever, we also need to consider the `Product` column in the `quality_control` table to filter the results for Widget-B.\n\nTherefore, the tables and columns needed to answer this question are:\n\n```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Defect_Type\", \"Rework_Count\", \"Product\"]\n}\n```\n\nNote that we don't need to join any other tables to answer this question, as the required information is already available in the `quality_control` table.",
    "methodology_score": 76.0,
    "sql_score": 99.95522388059702,
    "table_column_score": 100.0,
    "response_quality_score": 90.83675289154053,
    "overall_score": 91.86800364736301,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 19,
      "expected_keyword_count": 25,
      "response_keyword_count": 71
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 99.70149253731343,
      "overall_score": 99.95522388059702,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "defect_type",
        "total_rework_count",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "defect_type",
        "total_rework_count",
        "rework_count"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "defect_type",
        "rework_count"
      ]
    },
    "total_latency_ms": 9163.247108459473,
    "total_tokens": 2227,
    "errors": [],
    "timestamp": "2025-11-30T12:58:01.608921"
  },
  {
    "question_id": "Medium_21",
    "question_text": "What is the average downtime per shift for Machine-M1 in October 2026?",
    "category": "Medium",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Filter production_logs table for Machine-M1 in October 2026\nUse the **production_logs** table and apply filters for:\n- Date: October 2026 (inclusive)\n- Line_Machine: Machine-M1\n\n```sql\nSELECT *\nFROM production_logs\nWHERE Date >= '2026-10-01' AND Date <= '2026-10-31' AND Line_Machine = 'Machine-M1'\n```\n\n### Step 2: Calculate total downtime minutes for Machine-M1 in October 2026\nPerform a group by operation on the filtered data to calculate the total downtime minutes for each shift.\n\n```sql\nSELECT Shift, SUM(Downtime_Minutes) AS Total_Downtime_Minutes\nFROM (\n  SELECT *\n  FROM production_logs\n  WHERE Date >= '2026-10-01' AND Date <= '2026-10-31' AND Line_Machine = 'Machine-M1'\n) AS filtered_data\nGROUP BY Shift\n```\n\n### Step 3: Calculate average downtime per shift for Machine-M1 in October 2026\nPerform an average operation on the total downtime minutes for each shift.\n\n```sql\nSELECT AVG(Total_Downtime_Minutes) AS Average_Downtime_Per_Shift\nFROM (\n  SELECT Shift, SUM(Downtime_Minutes) AS Total_Downtime_Minutes\n  FROM (\n    SELECT *\n    FROM production_logs\n    WHERE Date >= '2026-10-01' AND Date <= '2026-10-31' AND Line_Machine = 'Machine-M1'\n  ) AS filtered_data\n  GROUP BY Shift\n) AS grouped_data\n```\n\n### Step 4: Round the average downtime per shift to the nearest whole number (optional)\nIf desired, round the average downtime per shift to the nearest whole number.\n\n```sql\nSELECT ROUND(Average_Downtime_Per_Shift) AS Average_Downtime_Per_Shift\nFROM (\n  SELECT AVG(Total_Downtime_Minutes) AS Average_Downtime_Per_Shift\n  FROM (\n    SELECT Shift, SUM(Downtime_Minutes) AS Total_Downtime_Minutes\n    FROM (\n      SELECT *\n      FROM production_logs\n      WHERE Date >= '2026-10-01' AND Date <= '2026-10-31' AND Line_Machine = 'Machine-M1'\n    ) AS filtered_data\n    GROUP BY Shift\n  ) AS grouped_data\n) AS averaged_data\n```\n\n### Final Query\nCombine all the steps into a single query.\n\n```sql\nSELECT ROUND(AVG(Total_Downtime_Minutes)) AS Average_Downtime_Per_Shift\nFROM (\n  SELECT Shift, SUM(Downtime_Minutes) AS Total_Downtime_Minutes\n  FROM (\n    SELECT *\n    FROM production_logs\n    WHERE Date >= '2026-10-01' AND Date <= '2026-10-31' AND Line_Machine = 'Machine-M1'\n  ) AS filtered_data\n  GROUP BY Shift\n) AS grouped_data\n```",
    "sql_response": "```sql\nSELECT \n    AVG(Downtime_Minutes / 60.0) AS Average_Downtime_Hours\nFROM \n    production_logs\nWHERE \n    Line_Machine = 'Line-1/Machine-M1'\n    AND Date >= '2026-10-01'\n    AND Date < '2026-11-01';\n```\n\nThis SQL query calculates the average downtime per shift for Machine-M1 in October 2026. It selects the average of the downtime in minutes, divided by 60 to convert it to hours, from the production_logs table where the Line_Machine is 'Line-1/Machine-M1' and the date is within October 2026.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns that contain the required information. \n\nFor the average downtime per shift for Machine-M1 in October 2026, we need the following:\n\n- The table that contains downtime information is `production_logs`.\n- The columns we need from this table are `Date` and `Downtime_Minutes`.\n- We also need to filter the data for Machine-M1, which can be done using the `Line_Machine` column.\n- Finally, we need to filter the data for October 2026, which can be done using the `Date` column.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"production_logs\"],\n  \"columns\": [\"Date\", \"Downtime_Minutes\", \"Line_Machine\"]\n}\n```\n\nHowever, we also need to filter the data for Machine-M1 and October 2026, which can be done using SQL queries or data manipulation techniques. The above JSON object only specifies the tables and columns needed to answer the question.",
    "methodology_score": 78.57142857142857,
    "sql_score": 65.31752411575563,
    "table_column_score": 91.42857142857143,
    "response_quality_score": 91.63017702102661,
    "overall_score": 78.45272257118856,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 11,
      "expected_keyword_count": 14,
      "response_keyword_count": 68
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 0.0,
      "aggregate_match": 100.0,
      "clause_match": 75.0,
      "fuzzy_score": 60.45016077170418,
      "overall_score": 65.31752411575563,
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "0"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 91.42857142857143,
      "table_score": 100.0,
      "column_score": 85.71428571428571,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 75.0,
      "tables_correct": [
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "line_machine",
        "date",
        "downtime_minutes"
      ],
      "columns_missing": [
        "shift"
      ],
      "columns_extra": [],
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "date",
        "downtime_minutes"
      ]
    },
    "total_latency_ms": 8369.822978973389,
    "total_tokens": 2131,
    "errors": [],
    "timestamp": "2025-11-30T12:58:16.269236"
  },
  {
    "question_id": "Easy_5",
    "question_text": "How many breakdown maintenances were performed on Machine-M2 in Q3 2026?",
    "category": "Easy",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Identify the relevant table and filter the data for Q3 2026\nUse the **maintenance_logs** table and filter the data for Q3 2026 by applying the following conditions:\n- Maintenance_Date is within the Q3 2026 period (July 1, 2026, to September 30, 2026)\n```python\nimport pandas as pd\n\n# Assuming maintenance_logs is a pandas DataFrame\nq3_2026_maintenance_logs = maintenance_logs[\n    (maintenance_logs['Maintenance_Date'].dt.month >= 7) & \n    (maintenance_logs['Maintenance_Date'].dt.month <= 9) & \n    (maintenance_logs['Maintenance_Date'].dt.year == 2026)\n]\n```\n\n### Step 2: Filter the data for Machine-M2\nApply a filter to the **q3_2026_maintenance_logs** DataFrame to include only rows where the Machine is 'Machine-M2'\n```python\nmachine_m2_maintenance_logs = q3_2026_maintenance_logs[q3_2026_maintenance_logs['Machine'] == 'Machine-M2']\n```\n\n### Step 3: Identify breakdown maintenance records\nFilter the **machine_m2_maintenance_logs** DataFrame to include only rows where the Maintenance_Type is 'Breakdown'\n```python\nbreakdown_maintenance_logs = machine_m2_maintenance_logs[machine_m2_maintenance_logs['Maintenance_Type'] == 'Breakdown']\n```\n\n### Step 4: Count the number of breakdown maintenance records\nPerform a count aggregation on the **breakdown_maintenance_logs** DataFrame to get the total number of breakdown maintenance records\n```python\nnum_breakdown_maintenances = breakdown_maintenance_logs.shape[0]\n```\n\n### Step 5: Return the result\nReturn the count of breakdown maintenance records as the final answer\n```python\nprint(num_breakdown_maintenances)\n```\n\nThis step-by-step methodology will provide the correct count of breakdown maintenance records performed on Machine-M2 in Q3 2026.",
    "sql_response": "```sql\nSELECT COUNT(*) \nFROM maintenance_logs \nWHERE Maintenance_Date >= '2026-07-01' \n  AND Maintenance_Date <= '2026-09-30' \n  AND Machine = 'Machine-M2' \n  AND Maintenance_Type = 'Breakdown';\n```\n\nThis SQL query will return the count of breakdown maintenances performed on Machine-M2 in Q3 2026. The date range for Q3 2026 is from July 1, 2026, to September 30, 2026.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns that contain information about machine breakdowns and their dates.\n\nBased on the available tables and columns, we can see that the `maintenance_logs` table contains information about machine maintenance, including breakdowns. We need to focus on the following columns:\n\n- `Maintenance_Date`: This column contains the date of the maintenance, which we can use to filter the data for Q3 2026.\n- `Machine`: This column contains the machine ID, which we can use to filter the data for Machine-M2.\n- `Maintenance_Type`: This column contains the type of maintenance, which we can use to filter the data for breakdown maintenances.\n\nTherefore, the tables and columns needed to answer the question are:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Maintenance_Date\", \"Machine\", \"Maintenance_Type\"]\n}\n```\n\nHowever, we also need to filter the data for Q3 2026, which means we need to extract the year and quarter from the `Maintenance_Date` column. Unfortunately, the `maintenance_logs` table does not contain a separate column for the year or quarter. \n\nTo solve this, we can use the `production_logs` table to get the date format and then use it to extract the year and quarter from the `maintenance_logs` table. \n\nBut since we are only interested in the count of breakdown maintenances, we can use the `maintenance_logs` table alone and filter the data for Q3 2026 by using the `Maintenance_Date` column.\n\nSo, the final answer remains the same:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Maintenance_Date\", \"Machine\", \"Maintenance_Type\"]\n}\n```",
    "methodology_score": 80.0,
    "sql_score": 72.58333333333333,
    "table_column_score": 100.0,
    "response_quality_score": 90.66362166404724,
    "overall_score": 83.4705288330714,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 12,
      "expected_keyword_count": 15,
      "response_keyword_count": 75
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 0.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 83.88888888888889,
      "overall_score": 72.58333333333333,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "machine"
      ],
      "generated_columns": [
        "maintenance_date"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "maintenance_date",
        "maintenance_type",
        "machine"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "maintenance_date",
        "maintenance_type",
        "machine"
      ],
      "generated_columns": [
        "maintenance_date",
        "maintenance_type",
        "machine"
      ]
    },
    "total_latency_ms": 9336.378335952759,
    "total_tokens": 2030,
    "errors": [],
    "timestamp": "2025-11-30T12:58:31.946765"
  },
  {
    "question_id": "Complex_33",
    "question_text": "What are the top 3 most frequently replaced parts across all machines, and what is the total cost associated with replacing these parts?",
    "category": "Complex",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Identify the relevant table and filter\nUse the **maintenance_logs** table, which contains information about the parts replaced during maintenance activities.\n\n### Step 2: Filter the data to include only relevant columns\nSelect the following columns from the **maintenance_logs** table:\n- Machine\n- Parts_Replaced\n- Cost_Rupees\n\n### Step 3: Group the data by parts replaced and calculate the frequency\nUse the `GROUP BY` clause to group the data by `Parts_Replaced`, and use the `COUNT` aggregation function to calculate the frequency of each part replacement.\n\n```sql\nSELECT Parts_Replaced, COUNT(*) as Replacement_Frequency\nFROM maintenance_logs\nGROUP BY Parts_Replaced\n```\n\n### Step 4: Rank the parts by replacement frequency\nUse the `RANK` function to rank the parts by replacement frequency in descending order.\n\n```sql\nSELECT Parts_Replaced, Replacement_Frequency, RANK() OVER (ORDER BY Replacement_Frequency DESC) as Rank\nFROM (\n  SELECT Parts_Replaced, COUNT(*) as Replacement_Frequency\n  FROM maintenance_logs\n  GROUP BY Parts_Replaced\n) as subquery\n```\n\n### Step 5: Filter the data to include only the top 3 parts\nUse the `WHERE` clause to filter the data and include only the top 3 parts with the highest replacement frequency.\n\n```sql\nSELECT Parts_Replaced, Replacement_Frequency\nFROM (\n  SELECT Parts_Replaced, Replacement_Frequency, RANK() OVER (ORDER BY Replacement_Frequency DESC) as Rank\n  FROM (\n    SELECT Parts_Replaced, COUNT(*) as Replacement_Frequency\n    FROM maintenance_logs\n    GROUP BY Parts_Replaced\n  ) as subquery\n) as subquery\nWHERE Rank <= 3\n```\n\n### Step 6: Calculate the total cost associated with replacing these parts\nUse the `SUM` aggregation function to calculate the total cost associated with replacing the top 3 parts.\n\n```sql\nSELECT SUM(Cost_Rupees) as Total_Cost\nFROM maintenance_logs\nWHERE Parts_Replaced IN (\n  SELECT Parts_Replaced\n  FROM (\n    SELECT Parts_Replaced, Replacement_Frequency, RANK() OVER (ORDER BY Replacement_Frequency DESC) as Rank\n    FROM (\n      SELECT Parts_Replaced, COUNT(*) as Replacement_Frequency\n      FROM maintenance_logs\n      GROUP BY Parts_Replaced\n    ) as subquery\n  ) as subquery\n  WHERE Rank <= 3\n)\n```\n\n### Step 7: Combine the results\nCombine the results from steps 5 and 6 to get the top 3 most frequently replaced parts and the total cost associated with replacing these parts.\n\n```sql\nSELECT \n  Parts_Replaced, \n  Replacement_Frequency, \n  Total_Cost\nFROM (\n  SELECT \n    Parts_Replaced, \n    Replacement_Frequency, \n    SUM(Cost_Rupees) as Total_Cost\n  FROM maintenance_logs\n  WHERE Parts_Replaced IN (\n    SELECT Parts_Replaced\n    FROM (\n      SELECT Parts_Replaced, Replacement_Frequency, RANK() OVER (ORDER BY Replacement_Frequency DESC) as Rank\n      FROM (\n        SELECT Parts_Replaced, COUNT(*) as Replacement_Frequency\n        FROM maintenance_logs\n        GROUP BY Parts_Replaced\n      ) as subquery\n    ) as subquery\n    WHERE Rank <= 3\n  )\n  GROUP BY Parts_Replaced, Replacement_Frequency\n) as subquery\nORDER BY Replacement_Frequency DESC\n```\n\nThis query will return the top 3 most frequently replaced parts and the total cost associated with replacing these parts.",
    "sql_response": "```sql\nWITH replaced_parts AS (\n  SELECT \n    Technician,\n    Machine,\n    Parts_Replaced,\n    Cost_Rupees\n  FROM \n    maintenance_logs\n  WHERE \n    Maintenance_Type = 'Repair'\n),\nreplaced_parts_count AS (\n  SELECT \n    Parts_Replaced,\n    COUNT(*) AS part_count,\n    SUM(Cost_Rupees) AS total_cost\n  FROM \n    replaced_parts\n  GROUP BY \n    Parts_Replaced\n)\nSELECT \n  TOP 3 \n  rp.Parts_Replaced,\n  rpc.part_count,\n  rpc.total_cost\nFROM \n  replaced_parts_count rpc\nJOIN \n  replaced_parts rp ON rpc.Parts_Replaced = rp.Parts_Replaced\nORDER BY \n  rpc.total_cost DESC;\n```\n\nThis SQL query first creates a temporary view `replaced_parts` to filter the `maintenance_logs` table for only the 'Repair' type maintenance. Then it creates another temporary view `replaced_parts_count` to group the `replaced_parts` view by the `Parts_Replaced` column and calculate the count and total cost for each part. Finally, it selects the top 3 parts with the highest total cost from the `replaced_parts_count` view.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns that contain information about the parts replaced and their associated costs. Based on the available tables and columns, we can use the `maintenance_logs` table to get the information about the parts replaced and their costs.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Machine\", \"Parts_Replaced\", \"Cost_Rupees\"]\n}\n```\n\nHowever, we also need to consider the `maintenance_logs` table's `Machine` column to identify the machine associated with each part replacement. The `Parts_Replaced` column will give us the list of parts replaced, and the `Cost_Rupees` column will provide the total cost associated with replacing these parts.\n\nTo get the top 3 most frequently replaced parts, we can use the `Parts_Replaced` column and group the results by this column. We can then use the `GROUP BY` clause and the `COUNT` aggregation function to count the number of times each part is replaced.\n\nTo get the total cost associated with replacing these parts, we can use the `SUM` aggregation function on the `Cost_Rupees` column.\n\nHere's an example SQL query that can be used to answer the question:\n\n```sql\nSELECT \n  Parts_Replaced, \n  COUNT(*) as Replacement_Count, \n  SUM(Cost_Rupees) as Total_Cost\nFROM \n  maintenance_logs\nGROUP BY \n  Parts_Replaced\nORDER BY \n  Replacement_Count DESC\nLIMIT 3;\n```\n\nThis query will return the top 3 most frequently replaced parts, along with their replacement counts and total costs.",
    "methodology_score": 59.09090909090909,
    "sql_score": 66.68530450023451,
    "table_column_score": 88.0,
    "response_quality_score": 90.07157492637634,
    "overall_score": 72.07428679499245,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 13,
      "expected_keyword_count": 22,
      "response_keyword_count": 87
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 22.22222222222222,
      "aggregate_match": 100.0,
      "clause_match": 57.14285714285714,
      "fuzzy_score": 50.38880248833593,
      "overall_score": 66.68530450023451,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "replacementcount",
        "parts_replaced"
      ],
      "generated_columns": [
        "parts_replaced",
        "part_count",
        "technician",
        "maintenance_type",
        "machine",
        "cost_rupees",
        "total_cost",
        "rpc"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 88.0,
      "table_score": 100.0,
      "column_score": 80.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 66.66666666666666,
      "column_recall": 100.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "cost_rupees",
        "parts_replaced"
      ],
      "columns_missing": [],
      "columns_extra": [
        "machine"
      ],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "parts_replaced"
      ],
      "generated_columns": [
        "cost_rupees",
        "parts_replaced",
        "machine"
      ]
    },
    "total_latency_ms": 9928.425073623657,
    "total_tokens": 2509,
    "errors": [],
    "timestamp": "2025-11-30T12:58:48.162270"
  },
  {
    "question_id": "Complex_23",
    "question_text": "What is the correlation between Downtime_Minutes and Failed_Qty on a daily basis?",
    "category": "Complex",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Filter Production Logs by Date\nUse the **production_logs** table and filter the data by 'Date' to get the daily production data.\n\n```sql\nSELECT Date, Downtime_Minutes\nFROM production_logs\n```\n\n### Step 2: Filter Quality Control Data by Date and Product\nUse the **quality_control** table and filter the data by 'Inspection_Date' and 'Product' to get the daily quality control data.\n\n```sql\nSELECT Inspection_Date, Failed_Qty\nFROM quality_control\n```\n\n### Step 3: Merge Production and Quality Control Data by Date\nCombine the filtered production and quality control data by 'Date' using a LEFT JOIN to ensure all dates are included.\n\n```sql\nSELECT p.Date, p.Downtime_Minutes, qc.Failed_Qty\nFROM production_logs p\nLEFT JOIN quality_control qc ON p.Date = qc.Inspection_Date\n```\n\n### Step 4: Group Data by Date and Calculate Correlation\nGroup the combined data by 'Date' and calculate the correlation between 'Downtime_Minutes' and 'Failed_Qty' using a statistical function (e.g., CORR() in SQL Server or CORR() in PostgreSQL).\n\n```sql\nSELECT Date, CORR(Downtime_Minutes, Failed_Qty) AS Correlation\nFROM (\n  SELECT p.Date, p.Downtime_Minutes, qc.Failed_Qty\n  FROM production_logs p\n  LEFT JOIN quality_control qc ON p.Date = qc.Inspection_Date\n) AS DailyData\nGROUP BY Date\n```\n\n### Step 5: Optional: Calculate Average Correlation\nIf you want to get a single correlation value for the entire period, you can calculate the average correlation across all dates.\n\n```sql\nSELECT AVG(Correlation) AS Average_Correlation\nFROM (\n  SELECT Date, CORR(Downtime_Minutes, Failed_Qty) AS Correlation\n  FROM (\n    SELECT p.Date, p.Downtime_Minutes, qc.Failed_Qty\n    FROM production_logs p\n    LEFT JOIN quality_control qc ON p.Date = qc.Inspection_Date\n  ) AS DailyData\n  GROUP BY Date\n) AS DailyCorrelations\n```\n\nThis step-by-step methodology will provide you with the correlation between Downtime_Minutes and Failed_Qty on a daily basis, as well as an optional average correlation value for the entire period.",
    "sql_response": "```sql\nWITH daily_downtime AS (\n  SELECT \n    Date,\n    SUM(Downtime_Minutes) AS Total_Downtime_Minutes\n  FROM \n    production_logs\n  GROUP BY \n    Date\n),\ndaily_failed_qty AS (\n  SELECT \n    Date,\n    SUM(Failed_Qty) AS Total_Failed_Qty\n  FROM \n    quality_control\n  GROUP BY \n    Date\n)\nSELECT \n  dd.Date,\n  dd.Total_Downtime_Minutes,\n  ddfq.Total_Failed_Qty,\n  CORR(dd.Total_Downtime_Minutes, ddfq.Total_Failed_Qty) AS Correlation_Coefficient\nFROM \n  daily_downtime dd\n  JOIN daily_failed_qty ddfq ON dd.Date = ddfq.Date\nORDER BY \n  dd.Date;\n```\n\nThis query first calculates the total downtime minutes and total failed quantity for each day using Common Table Expressions (CTEs). Then it joins these two CTEs on the date column and calculates the correlation coefficient between the total downtime minutes and total failed quantity for each day. The result is sorted by date.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns that contain the required information. \n\nFor the correlation between Downtime_Minutes and Failed_Qty on a daily basis, we need the following:\n\n- Downtime_Minutes: This is available in the production_logs table.\n- Failed_Qty: This is available in the quality_control table.\n\nHowever, we need to consider the date aspect as well. Since we are looking for the correlation on a daily basis, we need to ensure that both Downtime_Minutes and Failed_Qty are aggregated by date.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"production_logs\", \"quality_control\"],\n  \"columns\": [\"Date\", \"Downtime_Minutes\", \"Failed_Qty\"]\n}\n```\n\nNote that we are including the Date column from both tables to ensure that we are aggregating the data by date.",
    "methodology_score": 62.5,
    "sql_score": 74.05920205920205,
    "table_column_score": 91.42857142857143,
    "response_quality_score": 89.81343603134155,
    "overall_score": 76.50920718099772,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 10,
      "expected_keyword_count": 16,
      "response_keyword_count": 84
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 33.33333333333333,
      "aggregate_match": 100.0,
      "clause_match": 80.0,
      "fuzzy_score": 58.17245817245818,
      "overall_score": 74.05920205920205,
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "pl",
        "corrtotaldowntime",
        "date",
        "downtime_minutes",
        "failed_qty",
        "totalfailures"
      ],
      "generated_columns": [
        "dd",
        "date",
        "downtime_minutes",
        "total_failed_qty",
        "total_downtime_minutes",
        "failed_qty"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 91.42857142857143,
      "table_score": 100.0,
      "column_score": 85.71428571428571,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 75.0,
      "tables_correct": [
        "production_logs",
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "date",
        "downtime_minutes",
        "failed_qty"
      ],
      "columns_missing": [
        "inspection_date"
      ],
      "columns_extra": [],
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "inspection_date",
        "date",
        "downtime_minutes",
        "failed_qty"
      ],
      "generated_columns": [
        "date",
        "downtime_minutes",
        "failed_qty"
      ]
    },
    "total_latency_ms": 10186.563968658447,
    "total_tokens": 2069,
    "errors": [],
    "timestamp": "2025-11-30T12:59:04.662130"
  },
  {
    "question_id": "Easy_28",
    "question_text": "What is the total cost of parts replaced for Machine-M3?",
    "category": "Easy",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Identify the Relevant Table\nThe question asks for the total cost of parts replaced for Machine-M3. This information is likely stored in the **maintenance_logs** table, which contains details about machine maintenance.\n\n### Step 2: Filter the Maintenance Logs for Machine-M3\nApply a filter to the **maintenance_logs** table to select only the rows where the **Machine** column matches 'M3'.\n\n```sql\nSELECT *\nFROM maintenance_logs\nWHERE Machine = 'M3'\n```\n\n### Step 3: Extract the Cost of Parts Replaced\nFrom the filtered maintenance logs, extract the **Cost_Rupees** column, which represents the cost of parts replaced during maintenance.\n\n```sql\nSELECT Cost_Rupees\nFROM maintenance_logs\nWHERE Machine = 'M3'\n```\n\n### Step 4: Calculate the Total Cost of Parts Replaced\nTo find the total cost of parts replaced for Machine-M3, sum up the values in the **Cost_Rupees** column.\n\n```sql\nSELECT SUM(Cost_Rupees) AS Total_Cost\nFROM maintenance_logs\nWHERE Machine = 'M3'\n```\n\n### Step 5: Combine the Code into a Single Query\nCombine the previous steps into a single SQL query to calculate the total cost of parts replaced for Machine-M3.\n\n```sql\nSELECT SUM(Cost_Rupees) AS Total_Cost\nFROM maintenance_logs\nWHERE Machine = 'M3'\n```\n\n### Step 6: Execute the Query and Interpret the Result\nExecute the query and obtain the result. The output will be the total cost of parts replaced for Machine-M3 in rupees.",
    "sql_response": "```sql\nSELECT SUM(Cost_Rupees) AS Total_Cost\nFROM maintenance_logs\nWHERE Machine = 'Machine-M3';\n```\n\nThis SQL query will return the total cost of parts replaced for Machine-M3. The `SUM` function is used to calculate the total cost, and the `WHERE` clause is used to filter the results to only include rows where the `Machine` is 'Machine-M3'.",
    "table_selection_response": "To answer the question, we need to identify the table and column that stores the cost of parts replaced for a specific machine. Based on the available tables and columns, we can see that the `maintenance_logs` table has a column `Cost_Rupees` which stores the cost of maintenance, but it's not specifically for parts replaced. However, it has a column `Parts_Replaced` which stores the parts replaced during maintenance.\n\nBut, we need to find the total cost of parts replaced for Machine-M3. So, we need to join the `maintenance_logs` table with another table that stores the cost of parts replaced.\n\nUpon reviewing the tables again, we can see that there is no direct table that stores the cost of parts replaced. However, we can infer that the cost of parts replaced is likely to be stored in the `maintenance_logs` table, but we need to calculate it.\n\nTo calculate the total cost of parts replaced, we need to multiply the `Parts_Replaced` column with the `Unit_Cost_Rupees` column from the `inventory_logs` table.\n\nSo, the tables and columns needed to answer this question are:\n\n```json\n{\n  \"tables\": [\"maintenance_logs\", \"inventory_logs\"],\n  \"columns\": [\"Parts_Replaced\", \"Unit_Cost_Rupees\"]\n}\n```\n\nHowever, we need to filter the data to only include Machine-M3. We can do this by adding a condition to the `maintenance_logs` table to only include rows where `Machine` is 'Machine-M3'.\n\n```json\n{\n  \"tables\": [\"maintenance_logs\", \"inventory_logs\"],\n  \"columns\": [\"Parts_Replaced\", \"Unit_Cost_Rupees\"],\n  \"conditions\": [\"maintenance_logs.Machine = 'Machine-M3'\"]\n}\n```\n\nNote: The `conditions` array is not a standard SQL feature, but it's used here to illustrate the additional filtering needed to answer the question.",
    "methodology_score": 85.71428571428571,
    "sql_score": 88.58490566037736,
    "table_column_score": 26.666666666666668,
    "response_quality_score": 91.15630269050598,
    "overall_score": 72.50129963113505,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 6,
      "expected_keyword_count": 7,
      "response_keyword_count": 75
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 90.56603773584906,
      "overall_score": 88.58490566037736,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "machine"
      ],
      "generated_columns": [
        "cost_rupees",
        "machine"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 26.666666666666668,
      "table_score": 66.66666666666666,
      "column_score": 0,
      "table_precision": 50.0,
      "table_recall": 100.0,
      "column_precision": 0.0,
      "column_recall": 0.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [
        "inventory_logs"
      ],
      "columns_correct": [],
      "columns_missing": [
        "cost_rupees",
        "machine"
      ],
      "columns_extra": [
        "parts_replaced",
        "unit_cost_rupees"
      ],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs",
        "inventory_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "machine"
      ],
      "generated_columns": [
        "parts_replaced",
        "unit_cost_rupees"
      ]
    },
    "total_latency_ms": 8843.697309494019,
    "total_tokens": 1944,
    "errors": [],
    "timestamp": "2025-11-30T12:59:19.930059"
  },
  {
    "question_id": "Easy_30",
    "question_text": "How much total waste was recorded in 2025?",
    "category": "Easy",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Identify the relevant table\nTo calculate the total waste recorded in 2025, we need to use the **inventory_logs** table, as it contains information about material consumption and wastage.\n\n### Step 2: Filter the data by year\nWe need to filter the data to only include records from 2025. We can do this by applying a date filter to the **Date** column in the **inventory_logs** table.\n\n```sql\nSELECT *\nFROM inventory_logs\nWHERE YEAR(Date) = 2025\n```\n\n### Step 3: Extract the wastage data\nWe need to extract the **Wastage_Kg** column from the filtered data, as it contains the amount of waste recorded.\n\n```sql\nSELECT Wastage_Kg\nFROM inventory_logs\nWHERE YEAR(Date) = 2025\n```\n\n### Step 4: Calculate the total waste\nWe need to calculate the total waste by summing up the **Wastage_Kg** values.\n\n```sql\nSELECT SUM(Wastage_Kg) AS Total_Waste\nFROM inventory_logs\nWHERE YEAR(Date) = 2025\n```\n\n### Step 5: Round the result (optional)\nIf desired, we can round the result to a specific number of decimal places.\n\n```sql\nSELECT ROUND(SUM(Wastage_Kg), 2) AS Total_Waste\nFROM inventory_logs\nWHERE YEAR(Date) = 2025\n```\n\n### Step 6: Execute the query\nWe can execute the final query to get the total waste recorded in 2025.\n\n```sql\nSELECT ROUND(SUM(Wastage_Kg), 2) AS Total_Waste\nFROM inventory_logs\nWHERE YEAR(Date) = 2025\n```\n\nThis will give us the total waste recorded in 2025.",
    "sql_response": "```sql\nSELECT SUM(Wastage_Kg) AS Total_Waste\nFROM inventory_logs\nWHERE Date BETWEEN '2025-01-01' AND '2025-12-31';\n```\n\nThis SQL query calculates the total waste recorded in 2025 by summing up the `Wastage_Kg` column from the `inventory_logs` table, where the `Date` column falls within the year 2025.",
    "table_selection_response": "To answer the question \"How much total waste was recorded in 2025?\", we need to identify the table and column that stores waste data.\n\nAfter analyzing the available tables and columns, I found that the `inventory_logs` table has a column named `Wastage_Kg` which stores the waste data.\n\nHowever, we also need to filter the data by the year 2025. Since the `Date` column in the `inventory_logs` table is not explicitly mentioned to be in a date format that can be easily filtered by year, we will need to convert it to a date format that can be easily filtered.\n\nBut, since the question asks for total waste, we can assume that the `Date` column is in a format that can be easily filtered by year.\n\nTherefore, the table and columns needed to answer this question are:\n\n```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Wastage_Kg\"]\n}\n```\n\nHowever, to filter the data by year, we would need to use a date function to extract the year from the `Date` column. But since the question does not specify how to filter by year, we will assume that the data is already filtered by year.\n\nIf we want to filter the data by year, we would need to use a date function like this:\n\n```sql\nSELECT SUM(Wastage_Kg) AS Total_Waste\nFROM inventory_logs\nWHERE YEAR(Date) = 2025;\n```\n\nBut since the question does not specify how to filter by year, we will assume that the data is already filtered by year.\n\nTherefore, the table and columns needed to answer this question are:\n\n```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Wastage_Kg\"]\n}\n```",
    "methodology_score": 85.71428571428571,
    "sql_score": 88.73684210526315,
    "table_column_score": 80.0,
    "response_quality_score": 91.95421004295349,
    "overall_score": 85.96760145542316,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 6,
      "expected_keyword_count": 7,
      "response_keyword_count": 70
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 91.57894736842105,
      "overall_score": 88.73684210526315,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg"
      ],
      "generated_columns": [
        "wastage_kg"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 80.0,
      "table_score": 100.0,
      "column_score": 66.66666666666666,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 50.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "wastage_kg"
      ],
      "columns_missing": [
        "date"
      ],
      "columns_extra": [],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "date"
      ],
      "generated_columns": [
        "wastage_kg"
      ]
    },
    "total_latency_ms": 8045.789957046509,
    "total_tokens": 1948,
    "errors": [],
    "timestamp": "2025-11-30T12:59:34.265327"
  },
  {
    "question_id": "Medium_31",
    "question_text": "What is the average downtime (in minutes) per shift for Machine-M1 on Line-1 during the last quarter?",
    "category": "Medium",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Identify the relevant data tables\nWe need to use the **production_logs** table to gather information about downtime minutes for Machine-M1 on Line-1.\n\n### Step 2: Filter the data for the last quarter\nTo determine the last quarter, we'll assume it's the last three months. We'll filter the data for the current year's last quarter (Q4). We'll use the **production_logs** table and filter by the **Date** column.\n\n```sql\nSELECT *\nFROM production_logs\nWHERE EXTRACT(YEAR FROM Date) = EXTRACT(YEAR FROM CURRENT_DATE)\n  AND EXTRACT(MONTH FROM Date) BETWEEN 10 AND 12;\n```\n\n### Step 3: Filter the data for Machine-M1 on Line-1\nWe'll filter the data for Machine-M1 on Line-1. We'll use the **production_logs** table and filter by the **Line_Machine** column.\n\n```sql\nSELECT *\nFROM production_logs\nWHERE Line_Machine = 'Machine-M1'\n  AND Line = 'Line-1';\n```\n\n### Step 4: Calculate the total downtime minutes per shift\nWe'll calculate the total downtime minutes per shift for Machine-M1 on Line-1. We'll use the **production_logs** table and calculate the sum of **Downtime_Minutes**.\n\n```sql\nSELECT SUM(Downtime_Minutes) AS Total_Downtime_Minutes\nFROM production_logs\nWHERE Line_Machine = 'Machine-M1'\n  AND Line = 'Line-1';\n```\n\n### Step 5: Calculate the total shifts\nWe'll calculate the total shifts for Machine-M1 on Line-1. We'll use the **production_logs** table and count the distinct **Shift** values.\n\n```sql\nSELECT COUNT(DISTINCT Shift) AS Total_Shifts\nFROM production_logs\nWHERE Line_Machine = 'Machine-M1'\n  AND Line = 'Line-1';\n```\n\n### Step 6: Calculate the average downtime minutes per shift\nWe'll calculate the average downtime minutes per shift by dividing the total downtime minutes by the total shifts.\n\n```sql\nSELECT (SELECT SUM(Downtime_Minutes) AS Total_Downtime_Minutes\n        FROM production_logs\n        WHERE Line_Machine = 'Machine-M1'\n          AND Line = 'Line-1') / \n       (SELECT COUNT(DISTINCT Shift) AS Total_Shifts\n        FROM production_logs\n        WHERE Line_Machine = 'Machine-M1'\n          AND Line = 'Line-1') AS Average_Downtime_Minutes_Per_Shift;\n```\n\n### Step 7: Combine the data (if necessary)\nIn this case, we don't need to combine data from multiple tables. The **production_logs** table provides all the necessary information.\n\n### Step 8: Execute the query\nWe'll execute the final query to get the average downtime minutes per shift for Machine-M1 on Line-1 during the last quarter.\n\n```sql\nSELECT (SELECT SUM(Downtime_Minutes) AS Total_Downtime_Minutes\n        FROM production_logs\n        WHERE Line_Machine = 'Machine-M1'\n          AND Line = 'Line-1'\n          AND EXTRACT(YEAR FROM Date) = EXTRACT(YEAR FROM CURRENT_DATE)\n          AND EXTRACT(MONTH FROM Date) BETWEEN 10 AND 12) / \n       (SELECT COUNT(DISTINCT Shift) AS Total_Shifts\n        FROM production_logs\n        WHERE Line_Machine = 'Machine-M1'\n          AND Line = 'Line-1'\n          AND EXTRACT(YEAR FROM Date) = EXTRACT(YEAR FROM CURRENT_DATE)\n          AND EXTRACT(MONTH FROM Date) BETWEEN 10 AND 12) AS Average_Downtime_Minutes_Per_Shift;\n```",
    "sql_response": "```sql\nSELECT \n    AVG(Downtime_Minutes) AS Average_Downtime_Minutes\nFROM \n    production_logs\nWHERE \n    Line_Machine IN ('Line-1/Machine-M1') \n    AND Date >= DATE_TRUNC('quarter', CURRENT_DATE) - INTERVAL '1 quarter';\n```\n\nThis SQL query calculates the average downtime (in minutes) per shift for Machine-M1 on Line-1 during the last quarter. \n\nHere's how it works:\n\n1. `DATE_TRUNC('quarter', CURRENT_DATE) - INTERVAL '1 quarter'` calculates the start date of the last quarter.\n2. `WHERE Line_Machine IN ('Line-1/Machine-M1')` filters the production logs to only include records for Machine-M1 on Line-1.\n3. `WHERE Date >= ...` filters the production logs to only include records from the last quarter.\n4. `AVG(Downtime_Minutes) AS Average_Downtime_Minutes` calculates the average downtime (in minutes) for the filtered records.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns required to calculate the average downtime per shift for Machine-M1 on Line-1 during the last quarter.\n\n### Tables:\n- `production_logs`: This table contains information about production, including downtime minutes.\n\n### Columns:\n- `Date`: To filter the data for the last quarter.\n- `Shift`: To calculate the average downtime per shift.\n- `Line_Machine`: To filter the data for Machine-M1 on Line-1.\n- `Downtime_Minutes`: To calculate the average downtime.\n\n### JSON Response:\n```json\n{\n  \"tables\": [\"production_logs\"],\n  \"columns\": [\"Date\", \"Shift\", \"Line_Machine\", \"Downtime_Minutes\"]\n}\n```",
    "methodology_score": 46.666666666666664,
    "sql_score": 73.52316141356255,
    "table_column_score": 100.0,
    "response_quality_score": 89.4285180568695,
    "overall_score": 73.67595830043385,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 7,
      "expected_keyword_count": 15,
      "response_keyword_count": 95
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 33.33333333333333,
      "aggregate_match": 100.0,
      "clause_match": 75.0,
      "fuzzy_score": 59.59885386819484,
      "overall_score": 73.52316141356255,
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "downtime_minutes"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ]
    },
    "total_latency_ms": 10571.481943130493,
    "total_tokens": 2310,
    "errors": [],
    "timestamp": "2025-11-30T12:59:51.149269"
  },
  {
    "question_id": "Complex_9",
    "question_text": "Which supplier provides the material with the lowest unit cost, considering only materials consumed after 2026-01-01?",
    "category": "Complex",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Answering the Question: Lowest Unit Cost Supplier\n\n### Step 1: Filter Inventory Logs for Consumed Materials After 2026-01-01\nUse the **inventory_logs** table and apply a filter on the **Date** column to include only records after 2026-01-01.\n\n```sql\nSELECT *\nFROM inventory_logs\nWHERE Date > '2026-01-01'\n```\n\n### Step 2: Calculate Total Consumption and Unit Cost for Each Material\nApply aggregations to the filtered **inventory_logs** table to calculate the total consumption and unit cost for each material.\n\n```sql\nSELECT Material_Code, Material_Name, SUM(Consumption_Kg) AS Total_Consumption_Kg, SUM(Received_Kg) AS Total_Received_Kg, SUM(Unit_Cost_Rupees * Consumption_Kg) AS Total_Cost_Rupees\nFROM inventory_logs\nWHERE Date > '2026-01-01'\nGROUP BY Material_Code, Material_Name\n```\n\n### Step 3: Calculate Unit Cost for Each Material\nCalculate the unit cost for each material by dividing the total cost by the total consumption.\n\n```sql\nSELECT Material_Code, Material_Name, Total_Consumption_Kg, Total_Received_Kg, Total_Cost_Rupees, Total_Cost_Rupees / Total_Consumption_Kg AS Unit_Cost_Rupees\nFROM (\n  SELECT Material_Code, Material_Name, SUM(Consumption_Kg) AS Total_Consumption_Kg, SUM(Received_Kg) AS Total_Received_Kg, SUM(Unit_Cost_Rupees * Consumption_Kg) AS Total_Cost_Rupees\n  FROM inventory_logs\n  WHERE Date > '2026-01-01'\n  GROUP BY Material_Code, Material_Name\n) AS Material_Summary\n```\n\n### Step 4: Identify Supplier for Each Material\nJoin the **inventory_logs** table with the **supplier** table (assuming it exists) to identify the supplier for each material. If the supplier table does not exist, use the **Supplier** column directly.\n\n```sql\nSELECT Material_Code, Material_Name, Supplier, Unit_Cost_Rupees\nFROM (\n  SELECT Material_Code, Material_Name, Total_Consumption_Kg, Total_Received_Kg, Total_Cost_Rupees, Total_Cost_Rupees / Total_Consumption_Kg AS Unit_Cost_Rupees\n  FROM (\n    SELECT Material_Code, Material_Name, SUM(Consumption_Kg) AS Total_Consumption_Kg, SUM(Received_Kg) AS Total_Received_Kg, SUM(Unit_Cost_Rupees * Consumption_Kg) AS Total_Cost_Rupees\n    FROM inventory_logs\n    WHERE Date > '2026-01-01'\n    GROUP BY Material_Code, Material_Name\n  ) AS Material_Summary\n) AS Material_Unit_Cost\n```\n\n### Step 5: Find the Supplier with the Lowest Unit Cost\nApply a subquery to find the supplier with the lowest unit cost.\n\n```sql\nSELECT Supplier, Unit_Cost_Rupees\nFROM (\n  SELECT Material_Code, Material_Name, Supplier, Unit_Cost_Rupees\n  FROM (\n    SELECT Material_Code, Material_Name, Total_Consumption_Kg, Total_Received_Kg, Total_Cost_Rupees, Total_Cost_Rupees / Total_Consumption_Kg AS Unit_Cost_Rupees\n    FROM (\n      SELECT Material_Code, Material_Name, SUM(Consumption_Kg) AS Total_Consumption_Kg, SUM(Received_Kg) AS Total_Received_Kg, SUM(Unit_Cost_Rupees * Consumption_Kg) AS Total_Cost_Rupees\n      FROM inventory_logs\n      WHERE Date > '2026-01-01'\n      GROUP BY Material_Code, Material_Name\n    ) AS Material_Summary\n) AS Material_Unit_Cost\n) AS Supplier_Unit_Cost\nWHERE Unit_Cost_Rupees = (\n  SELECT MIN(Unit_Cost_Rupees)\n  FROM (\n    SELECT Material_Code, Material_Name, Supplier, Unit_Cost_Rupees\n    FROM (\n      SELECT Material_Code, Material_Name, Total_Consumption_Kg, Total_Received_Kg, Total_Cost_Rupees, Total_Cost_Rupees / Total_Consumption_Kg AS Unit_Cost_Rupees\n      FROM (\n        SELECT Material_Code, Material_Name, SUM(Consumption_Kg) AS Total_Consumption_Kg, SUM(Received_Kg) AS Total_Received_Kg, SUM(Unit_Cost_Rupees * Consumption_Kg) AS Total_Cost_Rupees\n        FROM inventory_logs\n        WHERE Date > '2026-01-01'\n        GROUP BY Material_Code, Material_Name\n      ) AS Material_Summary\n    ) AS Material_Unit_Cost\n)\n```\n\n### Step 6: Finalize",
    "sql_response": "```sql\nSELECT \n    Supplier, \n    MIN(Unit_Cost_Rupees) AS Lowest_Unit_Cost\nFROM \n    inventory_logs\nWHERE \n    Date > '2026-01-01'\nGROUP BY \n    Supplier\nORDER BY \n    Lowest_Unit_Cost ASC\nLIMIT 1;\n```\n\nThis SQL query will return the supplier who provides the material with the lowest unit cost, considering only materials consumed after 2026-01-01. The `MIN` function is used to find the minimum unit cost for each supplier, and the `LIMIT 1` clause is used to return only the supplier with the lowest unit cost.",
    "table_selection_response": "To answer this question, we need to identify the supplier and the material with the lowest unit cost, considering only materials consumed after 2026-01-01. \n\nWe can achieve this by analyzing the inventory_logs table, which contains information about the materials consumed. We need to filter the data to include only the materials consumed after 2026-01-01 and then find the supplier with the lowest unit cost.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Supplier\", \"Unit_Cost_Rupees\"]\n}\n```\n\nHowever, we also need to consider the fact that the unit cost is calculated based on the consumption of materials, which is stored in the Consumption_Kg column. Therefore, we should also include this column in our analysis.\n\nHere's the updated JSON object:\n\n```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Supplier\", \"Unit_Cost_Rupees\", \"Consumption_Kg\"]\n}\n```\n\nNote that we don't need to include any conditions or filters in the JSON object, as we can apply these in the SQL query or data analysis code that follows.",
    "methodology_score": 63.1578947368421,
    "sql_score": 84.96987951807229,
    "table_column_score": 80.0,
    "response_quality_score": 90.28938174247742,
    "overall_score": 77.71576442662567,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 12,
      "expected_keyword_count": 19,
      "response_keyword_count": 73
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 50.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 83.13253012048193,
      "overall_score": 84.96987951807229,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_name",
        "min",
        "date",
        "unit_cost_rupees",
        "supplier"
      ],
      "generated_columns": [
        "lowest_unit_cost",
        "unit_cost_rupees",
        "supplier",
        "date"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 80.0,
      "table_score": 100.0,
      "column_score": 66.66666666666666,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 50.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "unit_cost_rupees",
        "supplier"
      ],
      "columns_missing": [
        "material_name",
        "date"
      ],
      "columns_extra": [],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_name",
        "supplier",
        "date",
        "unit_cost_rupees"
      ],
      "generated_columns": [
        "unit_cost_rupees",
        "supplier"
      ]
    },
    "total_latency_ms": 9710.618257522583,
    "total_tokens": 2549,
    "errors": [],
    "timestamp": "2025-11-30T13:00:07.152916"
  },
  {
    "question_id": "Complex_19",
    "question_text": "How does the 'Downtime_Minutes' in production relate to the 'Downtime_Hours' in maintenance logs when the 'Issue_Description' involves 'Motor overheating and failure to start'?",
    "category": "Complex",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Filter Maintenance Logs for 'Motor Overheating and Failure to Start' Issue\nUse the **maintenance_logs** table and filter the data to include only rows where the 'Issue_Description' involves 'Motor overheating and failure to start'.\n\n```sql\nSELECT *\nFROM maintenance_logs\nWHERE Issue_Description LIKE '%Motor overheating and failure to start%';\n```\n\n### Step 2: Extract Downtime Hours for 'Motor Overheating and Failure to Start' Issues\nFrom the filtered maintenance logs, extract the 'Downtime_Hours' for each occurrence of 'Motor overheating and failure to start' issues.\n\n```sql\nSELECT Downtime_Hours\nFROM (\n  SELECT *\n  FROM maintenance_logs\n  WHERE Issue_Description LIKE '%Motor overheating and failure to start%'\n) AS filtered_logs;\n```\n\n### Step 3: Filter Production Logs for Relevant Time Period\nUse the **production_logs** table and filter the data to include only rows where the 'Date' falls within the time period of the 'Maintenance_Date' for the 'Motor overheating and failure to start' issues.\n\n```sql\nSELECT *\nFROM production_logs\nWHERE Date >= (SELECT MIN(Maintenance_Date) FROM filtered_logs)\n  AND Date <= (SELECT MAX(Maintenance_Date) FROM filtered_logs);\n```\n\n### Step 4: Calculate Total Downtime Minutes for Each Day\nFrom the filtered production logs, calculate the total 'Downtime_Minutes' for each day.\n\n```sql\nSELECT Date, SUM(Downtime_Minutes) AS Total_Downtime_Minutes\nFROM (\n  SELECT *\n  FROM production_logs\n  WHERE Date >= (SELECT MIN(Maintenance_Date) FROM filtered_logs)\n    AND Date <= (SELECT MAX(Maintenance_Date) FROM filtered_logs)\n) AS filtered_production_logs\nGROUP BY Date;\n```\n\n### Step 5: Join Production and Maintenance Data for Analysis\nJoin the production data with the maintenance data to analyze the relationship between 'Downtime_Minutes' and 'Downtime_Hours'.\n\n```sql\nSELECT p.Date, p.Total_Downtime_Minutes, m.Downtime_Hours\nFROM (\n  SELECT Date, SUM(Downtime_Minutes) AS Total_Downtime_Minutes\n  FROM (\n    SELECT *\n    FROM production_logs\n    WHERE Date >= (SELECT MIN(Maintenance_Date) FROM filtered_logs)\n      AND Date <= (SELECT MAX(Maintenance_Date) FROM filtered_logs)\n  ) AS filtered_production_logs\n  GROUP BY Date\n) AS p\nJOIN (\n  SELECT Downtime_Hours\n  FROM (\n    SELECT *\n    FROM maintenance_logs\n    WHERE Issue_Description LIKE '%Motor overheating and failure to start%'\n  ) AS filtered_logs\n) AS m\nON p.Date = (SELECT Maintenance_Date FROM filtered_logs);\n```\n\n### Step 6: Calculate Correlation Coefficient\nCalculate the correlation coefficient between 'Downtime_Minutes' and 'Downtime_Hours' to analyze the relationship between the two variables.\n\n```sql\nSELECT CORR(p.Total_Downtime_Minutes, m.Downtime_Hours) AS Correlation_Coefficient\nFROM (\n  SELECT Date, SUM(Downtime_Minutes) AS Total_Downtime_Minutes\n  FROM (\n    SELECT *\n    FROM production_logs\n    WHERE Date >= (SELECT MIN(Maintenance_Date) FROM filtered_logs)\n      AND Date <= (SELECT MAX(Maintenance_Date) FROM filtered_logs)\n  ) AS filtered_production_logs\n  GROUP BY Date\n) AS p\nJOIN (\n  SELECT Downtime_Hours\n  FROM (\n    SELECT *\n    FROM maintenance_logs\n    WHERE Issue_Description LIKE '%Motor overheating and failure to start%'\n  ) AS filtered_logs\n) AS m\nON p.Date = (SELECT Maintenance_Date FROM filtered_logs);\n```\n\n### Step 7: Interpret Results\nInterpret the correlation coefficient to understand the relationship between 'Downtime_Minutes' and 'Downtime_Hours' when the 'Issue_Description' involves 'Motor overheating and failure to start'.",
    "sql_response": "```sql\nWITH \n-- Calculate total downtime minutes for each machine in production logs\nproduction_downtime AS (\n  SELECT \n    Line_Machine,\n    SUM(Downtime_Minutes) AS total_downtime_minutes\n  FROM \n    production_logs\n  GROUP BY \n    Line_Machine\n),\n-- Calculate total downtime hours for each machine in maintenance logs\nmaintenance_downtime AS (\n  SELECT \n    Machine,\n    SUM(Downtime_Hours) AS total_downtime_hours\n  FROM \n    maintenance_logs\n  WHERE \n    Issue_Description LIKE '%Motor overheating and failure to start%'\n  GROUP BY \n    Machine\n)\n-- Join production and maintenance downtime to find the relationship\nSELECT \n  pd.Line_Machine,\n  pd.total_downtime_minutes,\n  md.total_downtime_hours,\n  (pd.total_downtime_minutes / 60) AS total_downtime_minutes_converted,\n  (pd.total_downtime_minutes / md.total_downtime_hours) AS downtime_minutes_per_hour\nFROM \n  production_downtime pd\n  JOIN maintenance_downtime md ON pd.Line_Machine = md.Machine;\n```\n\nThis SQL query first calculates the total downtime minutes for each machine in the production logs and the total downtime hours for each machine in the maintenance logs where the issue description involves 'Motor overheating and failure to start'. It then joins these two results to find the relationship between the downtime minutes in production and the downtime hours in maintenance. The query also converts the downtime minutes to hours and calculates the downtime minutes per hour.",
    "table_selection_response": "To answer this question, we need to identify the tables and columns that relate to downtime minutes and downtime hours. \n\nFor downtime minutes, we need to look at the 'production_logs' table, specifically the 'Downtime_Minutes' column.\n\nFor downtime hours, we need to look at the 'maintenance_logs' table, specifically the 'Downtime_Hours' column, but only when the 'Issue_Description' involves 'Motor overheating and failure to start'.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"production_logs\", \"maintenance_logs\"],\n  \"columns\": [\"Downtime_Minutes\", \"Downtime_Hours\", \"Issue_Description\"]\n}\n```\n\nHowever, since we are only interested in the 'Downtime_Hours' when the 'Issue_Description' involves 'Motor overheating and failure to start', we should also include the 'Issue_Description' column in the query to filter the results.",
    "methodology_score": 63.63636363636363,
    "sql_score": 53.71960132890365,
    "table_column_score": 84.99999999999999,
    "response_quality_score": 89.85980200767517,
    "overall_score": 68.12874975679289,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 14,
      "expected_keyword_count": 22,
      "response_keyword_count": 97
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 28.57142857142857,
      "aggregate_match": 25.0,
      "clause_match": 80.0,
      "fuzzy_score": 30.511627906976745,
      "overall_score": 53.71960132890365,
      "expected_tables": [
        "maintenance_logs",
        "production_logs"
      ],
      "generated_tables": [
        "maintenance_logs",
        "production_logs"
      ],
      "expected_columns": [
        "downtime_hours",
        "downtime_minutes"
      ],
      "generated_columns": [
        "line_machine",
        "total_downtime_hours",
        "downtime_minutes",
        "total_downtime_minutes / 60",
        "total_downtime_minutes",
        "downtime_hours",
        "machine"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 84.99999999999999,
      "table_score": 100.0,
      "column_score": 74.99999999999999,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 60.0,
      "tables_correct": [
        "maintenance_logs",
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "downtime_minutes",
        "downtime_hours",
        "issue_description"
      ],
      "columns_missing": [
        "breakdown_date",
        "date"
      ],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs",
        "production_logs"
      ],
      "generated_tables": [
        "maintenance_logs",
        "production_logs"
      ],
      "expected_columns": [
        "date",
        "downtime_minutes",
        "breakdown_date",
        "issue_description",
        "downtime_hours"
      ],
      "generated_columns": [
        "issue_description",
        "downtime_hours",
        "downtime_minutes"
      ]
    },
    "total_latency_ms": 10140.19799232483,
    "total_tokens": 2562,
    "errors": [],
    "timestamp": "2025-11-30T13:00:23.669284"
  },
  {
    "question_id": "Complex_3",
    "question_text": "What is the correlation between downtime minutes and failed quantity per line machine and product?",
    "category": "Complex",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Correlation between Downtime Minutes and Failed Quantity per Line Machine and Product\n\nTo answer this question, we will follow these steps:\n\n### Step 1: Filter production logs by relevant columns\nUse the **production_logs** table and filter by columns:\n- **Line_Machine**: to group by line machine\n- **Product**: to group by product\n- **Downtime_Minutes**: to calculate total downtime minutes per line machine and product\n\n```sql\nSELECT Line_Machine, Product, SUM(Downtime_Minutes) AS Total_Downtime_Minutes\nFROM production_logs\nGROUP BY Line_Machine, Product\n```\n\n### Step 2: Filter quality control logs by relevant columns\nUse the **quality_control** table and filter by columns:\n- **Line**: to group by line\n- **Product**: to group by product\n- **Failed_Qty**: to calculate total failed quantity per line and product\n\n```sql\nSELECT Line, Product, SUM(Failed_Qty) AS Total_Failed_Qty\nFROM quality_control\nGROUP BY Line, Product\n```\n\n### Step 3: Merge production and quality control data\nCombine the results from steps 1 and 2 using a full outer join to include all line machines and products, even if there is no corresponding data in the other table.\n\n```sql\nSELECT \n  COALESCE(p.Line_Machine, q.Line) AS Line_Machine,\n  COALESCE(p.Product, q.Product) AS Product,\n  p.Total_Downtime_Minutes,\n  q.Total_Failed_Qty\nFROM \n  (SELECT Line_Machine, Product, SUM(Downtime_Minutes) AS Total_Downtime_Minutes\n   FROM production_logs\n   GROUP BY Line_Machine, Product) p\n  FULL OUTER JOIN \n  (SELECT Line, Product, SUM(Failed_Qty) AS Total_Failed_Qty\n   FROM quality_control\n   GROUP BY Line, Product) q\n  ON p.Line_Machine = q.Line AND p.Product = q.Product\n```\n\n### Step 4: Calculate correlation between downtime minutes and failed quantity\nUse the combined data from step 3 and calculate the correlation between **Total_Downtime_Minutes** and **Total_Failed_Qty** using a linear regression model or a correlation coefficient (e.g., Pearson's r).\n\n```sql\nSELECT \n  Line_Machine,\n  Product,\n  Total_Downtime_Minutes,\n  Total_Failed_Qty,\n  CORR(Total_Downtime_Minutes, Total_Failed_Qty) AS Correlation_Coefficient\nFROM \n  (SELECT \n     COALESCE(p.Line_Machine, q.Line) AS Line_Machine,\n     COALESCE(p.Product, q.Product) AS Product,\n     p.Total_Downtime_Minutes,\n     q.Total_Failed_Qty\n   FROM \n     (SELECT Line_Machine, Product, SUM(Downtime_Minutes) AS Total_Downtime_Minutes\n      FROM production_logs\n      GROUP BY Line_Machine, Product) p\n     FULL OUTER JOIN \n     (SELECT Line, Product, SUM(Failed_Qty) AS Total_Failed_Qty\n      FROM quality_control\n      GROUP BY Line, Product) q\n     ON p.Line_Machine = q.Line AND p.Product = q.Product) AS combined_data\n```\n\n### Step 5: Group by line machine and product to get correlation coefficients\nGroup the results from step 4 by **Line_Machine** and **Product** to get the correlation coefficients for each line machine and product.\n\n```sql\nSELECT \n  Line_Machine,\n  Product,\n  AVG(Correlation_Coefficient) AS Average_Correlation_Coefficient\nFROM \n  (SELECT \n     Line_Machine,\n     Product,\n     Total_Downtime_Minutes,\n     Total_Failed_Qty,\n     CORR(Total_Downtime_Minutes, Total_Failed_Qty) AS Correlation_Coefficient\n   FROM \n     (SELECT \n        COALESCE(p.Line_Machine, q.Line) AS Line_Machine,\n        COALESCE(p.Product, q.Product) AS Product,\n        p.Total_Downtime_Minutes,\n        q.Total_Failed_Qty\n      FROM \n        (SELECT Line_Machine, Product, SUM(Downtime_Minutes) AS Total_Downtime_Minutes\n         FROM production_logs\n         GROUP BY Line_Machine, Product) p\n        FULL OUTER JOIN \n        (SELECT Line, Product, SUM(Failed_Qty) AS Total_Failed_Qty\n         FROM quality_control\n         GROUP BY Line, Product) q\n        ON p.Line_Machine = q.Line AND p.Product = q.Product) AS combined_data) AS corr_data\nGROUP BY \n  Line_Machine,\n  Product\n```\n\nThis will give you the average correlation coefficient between downtime minutes and failed quantity for each line machine and product.",
    "sql_response": "```sql\nWITH \n-- Calculate failed quantity per line machine and product\nfailed_quantity_cte AS (\n  SELECT \n    q.Product,\n    p.Line_Machine,\n    SUM(q.Failed_Qty) AS Failed_Qty\n  FROM \n    quality_control q\n  JOIN \n    production_logs p ON q.Product = p.Product\n  GROUP BY \n    q.Product, p.Line_Machine\n),\n-- Calculate downtime minutes per line machine and product\ndowntime_minutes_cte AS (\n  SELECT \n    p.Product,\n    p.Line_Machine,\n    SUM(p.Downtime_Minutes) AS Downtime_Minutes\n  FROM \n    production_logs p\n  GROUP BY \n    p.Product, p.Line_Machine\n)\n-- Calculate correlation between failed quantity and downtime minutes\nSELECT \n  f.Product,\n  f.Line_Machine,\n  COVAR_POP(f.Failed_Qty, d.Downtime_Minutes) AS Correlation\nFROM \n  failed_quantity_cte f\nJOIN \n  downtime_minutes_cte d ON f.Product = d.Product AND f.Line_Machine = d.Line_Machine\nORDER BY \n  f.Product, f.Line_Machine;\n```\n\nThis query uses two Common Table Expressions (CTEs) to calculate the failed quantity and downtime minutes per line machine and product. It then calculates the correlation between these two values using the COVAR_POP function, which returns the population covariance between the two variables. The result is ordered by product and line machine for easier interpretation.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns that contain the required information. \n\nFor the correlation between downtime minutes and failed quantity per line machine and product, we need to consider the following:\n\n- Downtime minutes: This information is available in the `production_logs` table under the `Downtime_Minutes` column.\n- Failed quantity: This information is available in the `quality_control` table under the `Failed_Qty` column.\n- Line machine and product: These are available in the `production_logs` table under the `Line_Machine` and `Product` columns, respectively.\n\nHowever, we also need to consider the relationship between the line machine and product in the `quality_control` table. Since the `quality_control` table contains information about the product and the failed quantity, we need to join this table with the `production_logs` table to get the downtime minutes for each line machine and product.\n\nTherefore, the required tables and columns are:\n\n```json\n{\n  \"tables\": [\"production_logs\", \"quality_control\"],\n  \"columns\": [\"Line_Machine\", \"Product\", \"Downtime_Minutes\", \"Failed_Qty\"]\n}\n```\n\nHowever, we need to consider that the failed quantity is per batch, and the downtime minutes are per shift. To get the correlation between downtime minutes and failed quantity per line machine and product, we need to aggregate the data by line machine and product.\n\nWe can achieve this by grouping the data by `Line_Machine` and `Product` and then calculating the average downtime minutes and failed quantity for each group.\n\n```json\n{\n  \"tables\": [\"production_logs\", \"quality_control\"],\n  \"columns\": [\"Line_Machine\", \"Product\", \"avg(Downtime_Minutes)\", \"avg(Failed_Qty)\"]\n}\n```\n\nNote that we are using the `avg` aggregation function to calculate the average downtime minutes and failed quantity for each group.",
    "methodology_score": 63.63636363636363,
    "sql_score": 56.18124485878804,
    "table_column_score": 85.0,
    "response_quality_score": 90.27613615989685,
    "overall_score": 69.03195840747459,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 7,
      "expected_keyword_count": 11,
      "response_keyword_count": 109
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 28.57142857142857,
      "aggregate_match": 50.0,
      "clause_match": 60.0,
      "fuzzy_score": 33.589251439539346,
      "overall_score": 56.18124485878804,
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "downtime_minutes",
        "failed_qty"
      ],
      "generated_columns": [
        "p",
        "product",
        "line_machine",
        "q",
        "downtime_minutes",
        "failed_qty",
        "f"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 85.0,
      "table_score": 100.0,
      "column_score": 75.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 75.0,
      "column_recall": 75.0,
      "tables_correct": [
        "production_logs",
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "downtime_minutes",
        "failed_qty"
      ],
      "columns_missing": [
        "date"
      ],
      "columns_extra": [
        "line_machine"
      ],
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "date",
        "downtime_minutes",
        "failed_qty"
      ],
      "generated_columns": [
        "line_machine",
        "product",
        "downtime_minutes",
        "failed_qty"
      ]
    },
    "total_latency_ms": 9723.86384010315,
    "total_tokens": 2839,
    "errors": [],
    "timestamp": "2025-11-30T13:00:39.695307"
  },
  {
    "question_id": "Easy_26",
    "question_text": "What is the average target quantity for all products?",
    "category": "Easy",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Calculating Average Target Quantity for All Products\n\n### Step 1: Identify the relevant table and columns\nUse the **production_logs** table, which contains information about production targets and actual quantities.\n\n### Step 2: Filter the data to include all products\nApply a filter to include all rows in the **production_logs** table, regardless of product, date, shift, or other factors.\n\n### Step 3: Select the target quantity column\nChoose the **Target_Qty** column from the **production_logs** table, as it contains the target quantities for each product.\n\n### Step 4: Perform aggregation to calculate the average target quantity\nUse the `AVG` aggregation function to calculate the average of the **Target_Qty** column. This will give us the average target quantity for all products.\n\n### Step 5: Combine data (not necessary in this case)\nSince we're only using the **production_logs** table, no data combination is required.\n\n### Step 6: Execute the query\nWrite and execute a SQL query that incorporates the above steps, such as:\n```sql\nSELECT AVG(Target_Qty) AS Average_Target_Qty\nFROM production_logs;\n```\nThis query will return the average target quantity for all products.\n\n### Step 7: Interpret the results\nThe result will be a single value representing the average target quantity for all products.",
    "sql_response": "```sql\nSELECT AVG(Target_Qty) AS Average_Target_Qty\nFROM production_logs;\n```\n\nThis SQL query calculates the average target quantity for all products by taking the average of the `Target_Qty` column in the `production_logs` table. The result is returned as a column named `Average_Target_Qty`.",
    "table_selection_response": "To answer the question \"What is the average target quantity for all products?\", we need to identify the tables and columns required.\n\nThe required tables are:\n- production_logs\n\nThe required columns are:\n- Product\n- Target_Qty\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"production_logs\"],\n  \"columns\": [\"Product\", \"Target_Qty\"]\n}\n```",
    "methodology_score": 60.0,
    "sql_score": 96.8348623853211,
    "table_column_score": 80.0,
    "response_quality_score": 92.10568404197693,
    "overall_score": 81.10277023906008,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 3,
      "expected_keyword_count": 5,
      "response_keyword_count": 82
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 78.89908256880733,
      "overall_score": 96.8348623853211,
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "target_qty"
      ],
      "generated_columns": [
        "target_qty"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 80.0,
      "table_score": 100.0,
      "column_score": 66.66666666666666,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 50.0,
      "column_recall": 100.0,
      "tables_correct": [
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "target_qty"
      ],
      "columns_missing": [],
      "columns_extra": [
        "product"
      ],
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "target_qty"
      ],
      "generated_columns": [
        "product",
        "target_qty"
      ]
    },
    "total_latency_ms": 7894.315958023071,
    "total_tokens": 1534,
    "errors": [],
    "timestamp": "2025-11-30T13:00:54.546977"
  },
  {
    "question_id": "Complex_24",
    "question_text": "For Material Steel-101, what is the trend of wastage (increase or decrease) over the past 6 months?",
    "category": "Complex",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Filter the inventory_logs table to get data for Material Steel-101\nUse the **inventory_logs** table and filter it to include only rows where the **Material_Name** is 'Steel-101'. This will give us the inventory data for the specified material.\n\n```sql\nSELECT *\nFROM inventory_logs\nWHERE Material_Name = 'Steel-101'\n```\n\n### Step 2: Filter the inventory_logs table to get data for the past 6 months\nApply a date filter to the **Date** column in the **inventory_logs** table to include only rows where the date is within the past 6 months. This will give us the inventory data for the specified material and time period.\n\n```sql\nSELECT *\nFROM inventory_logs\nWHERE Material_Name = 'Steel-101'\n  AND Date >= DATEADD(month, -6, GETDATE())\n```\n\n### Step 3: Calculate the total wastage for each month\nUse the **Wastage_Kg** column in the filtered **inventory_logs** table to calculate the total wastage for each month. This will give us the monthly wastage data for the specified material.\n\n```sql\nSELECT \n  EXTRACT(YEAR FROM Date) AS Year,\n  EXTRACT(MONTH FROM Date) AS Month,\n  SUM(Wastage_Kg) AS Total_Wastage\nFROM inventory_logs\nWHERE Material_Name = 'Steel-101'\n  AND Date >= DATEADD(month, -6, GETDATE())\nGROUP BY \n  EXTRACT(YEAR FROM Date),\n  EXTRACT(MONTH FROM Date)\nORDER BY \n  Year, Month\n```\n\n### Step 4: Calculate the percentage change in wastage from month to month\nUse the **Total_Wastage** column from the previous step to calculate the percentage change in wastage from month to month. This will give us the trend of wastage over the past 6 months.\n\n```sql\nWITH Monthly_Wastage AS (\n  SELECT \n    EXTRACT(YEAR FROM Date) AS Year,\n    EXTRACT(MONTH FROM Date) AS Month,\n    SUM(Wastage_Kg) AS Total_Wastage\n  FROM inventory_logs\n  WHERE Material_Name = 'Steel-101'\n    AND Date >= DATEADD(month, -6, GETDATE())\n  GROUP BY \n    EXTRACT(YEAR FROM Date),\n    EXTRACT(MONTH FROM Date)\n)\nSELECT \n  Year,\n  Month,\n  Total_Wastage,\n  LAG(Total_Wastage) OVER (ORDER BY Year, Month) AS Previous_Month_Wastage,\n  (Total_Wastage - LAG(Total_Wastage) OVER (ORDER BY Year, Month)) / LAG(Total_Wastage) OVER (ORDER BY Year, Month) * 100 AS Percentage_Change\nFROM Monthly_Wastage\nORDER BY \n  Year, Month\n```\n\n### Step 5: Determine the trend of wastage\nAnalyze the **Percentage_Change** column from the previous step to determine the trend of wastage over the past 6 months. If the percentage change is positive, the wastage is increasing. If the percentage change is negative, the wastage is decreasing.\n\n```sql\nWITH Monthly_Wastage AS (\n  SELECT \n    EXTRACT(YEAR FROM Date) AS Year,\n    EXTRACT(MONTH FROM Date) AS Month,\n    SUM(Wastage_Kg) AS Total_Wastage\n  FROM inventory_logs\n  WHERE Material_Name = 'Steel-101'\n    AND Date >= DATEADD(month, -6, GETDATE())\n  GROUP BY \n    EXTRACT(YEAR FROM Date),\n    EXTRACT(MONTH FROM Date)\n),\nPercentage_Change AS (\n  SELECT \n    Year,\n    Month,\n    Total_Wastage,\n    LAG(Total_Wastage) OVER (ORDER BY Year, Month) AS Previous_Month_Wastage,\n    (Total_Wastage - LAG(Total_Wastage) OVER (ORDER BY Year, Month)) / LAG(Total_Wastage) OVER (ORDER BY Year, Month) * 100 AS Percentage_Change\n  FROM Monthly_Wastage\n)\nSELECT \n  Year,\n  Month,\n  Total_Wastage,\n  Percentage_Change,\n  CASE \n    WHEN Percentage_Change > 0 THEN 'Increasing'\n    WHEN Percentage_Change < 0 THEN 'Decreasing'\n    ELSE 'Stable'\n  END AS Wastage_Trend\nFROM Percentage_Change\nORDER BY \n  Year, Month\n```",
    "sql_response": "```sql\nSELECT \n    EXTRACT(YEAR FROM il.Date) AS Year,\n    EXTRACT(MONTH FROM il.Date) AS Month,\n    il.Material_Name,\n    il.Wastage_Kg,\n    LAG(il.Wastage_Kg) OVER (ORDER BY il.Date) AS Previous_Month_Wastage,\n    CASE\n        WHEN il.Wastage_Kg > LAG(il.Wastage_Kg) OVER (ORDER BY il.Date) THEN 'Increase'\n        WHEN il.Wastage_Kg < LAG(il.Wastage_Kg) OVER (ORDER BY il.Date) THEN 'Decrease'\n        ELSE 'No Change'\n    END AS Wastage_Trend\nFROM \n    inventory_logs il\nWHERE \n    il.Material_Code = 'Steel-101'\n    AND il.Date >= DATE_TRUNC('month', CURRENT_DATE - INTERVAL '6 month')\nORDER BY \n    il.Date;\n```\n\nThis SQL query will return the trend of wastage for Steel-101 over the past 6 months. It uses the `LAG` function to compare the current month's wastage with the previous month's wastage, and then uses a `CASE` statement to determine whether the wastage has increased, decreased, or remained the same. The results are ordered by date.",
    "table_selection_response": "To answer the question, we need to identify the trend of wastage for Material Steel-101 over the past 6 months. \n\nBased on the available tables and columns, we can see that the `inventory_logs` table contains information about material consumption and wastage. We can use this table to analyze the trend of wastage.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Material_Name\", \"Wastage_Kg\"]\n}\n```\n\nHowever, we also need to filter the data to only include Material Steel-101. We can do this by adding a condition to the `Material_Name` column.\n\n```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Material_Name\", \"Wastage_Kg\"],\n  \"conditions\": [\"Material_Name = 'Steel-101'\"]\n}\n```\n\nTo analyze the trend over the past 6 months, we can add a date filter to the `Date` column.\n\n```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Material_Name\", \"Wastage_Kg\"],\n  \"conditions\": [\"Material_Name = 'Steel-101'\"],\n  \"filters\": [\"Date >= DATE_SUB(CURRENT_DATE, INTERVAL 6 MONTH)\"]\n}\n```\n\nThis will give us the wastage trend for Material Steel-101 over the past 6 months.",
    "methodology_score": 57.14285714285714,
    "sql_score": 42.766179540709814,
    "table_column_score": 64.0,
    "response_quality_score": 90.92815160751343,
    "overall_score": 57.20383514285692,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 12,
      "expected_keyword_count": 21,
      "response_keyword_count": 96
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 0.0,
      "aggregate_match": 0.0,
      "clause_match": 66.66666666666666,
      "fuzzy_score": 51.77453027139875,
      "overall_score": 42.766179540709814,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "material_code",
        "strftime",
        "date",
        "strftime%y-%m",
        "totalwastage",
        "case when select totalwastage",
        "month"
      ],
      "generated_columns": [
        "il",
        "extractyear"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 64.0,
      "table_score": 100.0,
      "column_score": 40.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 50.0,
      "column_recall": 33.33333333333333,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "wastage_kg"
      ],
      "columns_missing": [
        "material_code",
        "date"
      ],
      "columns_extra": [
        "material_name"
      ],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_code",
        "wastage_kg",
        "date"
      ],
      "generated_columns": [
        "wastage_kg",
        "material_name"
      ]
    },
    "total_latency_ms": 9071.848392486572,
    "total_tokens": 2660,
    "errors": [],
    "timestamp": "2025-11-30T13:01:10.206158"
  },
  {
    "question_id": "Medium_20",
    "question_text": "Which supplier had the lowest average unit cost for 'Rubber-RB' in the last 6 months?",
    "category": "Medium",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Filter Inventory Logs for 'Rubber-RB' Material\nUse the **inventory_logs** table and filter the data for the last 6 months. We will also filter for the specific material 'Rubber-RB'.\n\n```sql\nSELECT *\nFROM inventory_logs\nWHERE Date >= DATEADD(month, -6, GETDATE())\n  AND Material_Name = 'Rubber-RB';\n```\n\n### Step 2: Calculate Unit Cost for Each Supplier\nCalculate the unit cost for each supplier by dividing the total cost by the total quantity received. We will use the **inventory_logs** table for this calculation.\n\n```sql\nSELECT Supplier, SUM(Received_Kg) AS Total_Received_Kg, SUM(Unit_Cost_Rupees * Received_Kg) AS Total_Cost_Rupees\nFROM inventory_logs\nWHERE Date >= DATEADD(month, -6, GETDATE())\n  AND Material_Name = 'Rubber-RB'\nGROUP BY Supplier;\n```\n\n### Step 3: Calculate Average Unit Cost for Each Supplier\nCalculate the average unit cost for each supplier by dividing the total cost by the total quantity received.\n\n```sql\nSELECT Supplier, Total_Cost_Rupees / Total_Received_Kg AS Average_Unit_Cost_Rupees\nFROM (\n  SELECT Supplier, SUM(Received_Kg) AS Total_Received_Kg, SUM(Unit_Cost_Rupees * Received_Kg) AS Total_Cost_Rupees\n  FROM inventory_logs\n  WHERE Date >= DATEADD(month, -6, GETDATE())\n    AND Material_Name = 'Rubber-RB'\n  GROUP BY Supplier\n) AS Subquery;\n```\n\n### Step 4: Find the Supplier with the Lowest Average Unit Cost\nFind the supplier with the lowest average unit cost by using the **MIN** function.\n\n```sql\nSELECT MIN(Average_Unit_Cost_Rupees) AS Lowest_Average_Unit_Cost_Rupees, Supplier\nFROM (\n  SELECT Supplier, Total_Cost_Rupees / Total_Received_Kg AS Average_Unit_Cost_Rupees\n  FROM (\n    SELECT Supplier, SUM(Received_Kg) AS Total_Received_Kg, SUM(Unit_Cost_Rupees * Received_Kg) AS Total_Cost_Rupees\n    FROM inventory_logs\n    WHERE Date >= DATEADD(month, -6, GETDATE())\n      AND Material_Name = 'Rubber-RB'\n    GROUP BY Supplier\n  ) AS Subquery\n) AS Subquery;\n```\n\n### Step 5: Combine the Results\nCombine the results to get the supplier with the lowest average unit cost.\n\n```sql\nSELECT Supplier\nFROM (\n  SELECT Supplier, Total_Cost_Rupees / Total_Received_Kg AS Average_Unit_Cost_Rupees\n  FROM (\n    SELECT Supplier, SUM(Received_Kg) AS Total_Received_Kg, SUM(Unit_Cost_Rupees * Received_Kg) AS Total_Cost_Rupees\n    FROM inventory_logs\n    WHERE Date >= DATEADD(month, -6, GETDATE())\n      AND Material_Name = 'Rubber-RB'\n    GROUP BY Supplier\n  ) AS Subquery\n) AS Subquery\nWHERE Average_Unit_Cost_Rupees = (\n  SELECT MIN(Average_Unit_Cost_Rupees)\n  FROM (\n    SELECT Supplier, Total_Cost_Rupees / Total_Received_Kg AS Average_Unit_Cost_Rupees\n    FROM (\n      SELECT Supplier, SUM(Received_Kg) AS Total_Received_Kg, SUM(Unit_Cost_Rupees * Received_Kg) AS Total_Cost_Rupees\n      FROM inventory_logs\n      WHERE Date >= DATEADD(month, -6, GETDATE())\n        AND Material_Name = 'Rubber-RB'\n      GROUP BY Supplier\n    ) AS Subquery\n  ) AS Subquery;\n```\n\nThis will give us the supplier with the lowest average unit cost for 'Rubber-RB' in the last 6 months.",
    "sql_response": "```sql\nSELECT \n    Supplier, \n    AVG(Unit_Cost_Rupees) AS Average_Unit_Cost\nFROM \n    inventory_logs\nWHERE \n    Material_Code = 'Rubber-RB' \n    AND Date >= DATE_SUB(CURRENT_DATE, INTERVAL 6 MONTH)\nGROUP BY \n    Supplier\nORDER BY \n    Average_Unit_Cost ASC\nLIMIT 1;\n```\n\nThis SQL query will return the supplier with the lowest average unit cost for 'Rubber-RB' in the last 6 months. The `DATE_SUB` function is used to get the date 6 months ago from the current date, and the `GROUP BY` clause is used to group the results by supplier. The `AVG` function is used to calculate the average unit cost for each supplier, and the `ORDER BY` clause is used to sort the results in ascending order by average unit cost. The `LIMIT 1` clause is used to return only the supplier with the lowest average unit cost.",
    "table_selection_response": "To answer the question, we need to identify the supplier with the lowest average unit cost for 'Rubber-RB' in the last 6 months. We can achieve this by analyzing the inventory logs for the specified period and product.\n\nHere's the required information:\n\n- **Tables:** inventory_logs\n- **Columns:**\n  - Material_Name (to filter for 'Rubber-RB')\n  - Unit_Cost_Rupees (to calculate the average unit cost)\n  - Supplier (to identify the supplier with the lowest average unit cost)\n\nHere's the JSON object with the required information:\n\n```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Material_Name\", \"Unit_Cost_Rupees\", \"Supplier\"]\n}\n```",
    "methodology_score": 82.35294117647058,
    "sql_score": 87.35732009925559,
    "table_column_score": 74.28571428571429,
    "response_quality_score": 89.89283895492554,
    "overall_score": 82.84165685460175,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 14,
      "expected_keyword_count": 17,
      "response_keyword_count": 67
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 60.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 82.38213399503722,
      "overall_score": 87.35732009925559,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_code",
        "unit_cost_rupees",
        "supplier",
        "avgcost"
      ],
      "generated_columns": [
        "material_code",
        "average_unit_cost",
        "unit_cost_rupees",
        "supplier"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 74.28571428571429,
      "table_score": 100.0,
      "column_score": 57.14285714285715,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 66.66666666666666,
      "column_recall": 50.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "unit_cost_rupees",
        "supplier"
      ],
      "columns_missing": [
        "material_code",
        "date"
      ],
      "columns_extra": [
        "material_name"
      ],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_code",
        "unit_cost_rupees",
        "supplier",
        "date"
      ],
      "generated_columns": [
        "supplier",
        "material_name",
        "unit_cost_rupees"
      ]
    },
    "total_latency_ms": 10107.161045074463,
    "total_tokens": 2331,
    "errors": [],
    "timestamp": "2025-11-30T13:01:26.821893"
  },
  {
    "question_id": "Complex_29",
    "question_text": "How does the rework count vary by defect type and product?",
    "category": "Complex",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Identify Relevant Tables and Filters\nTo answer the question, we need to use the **quality_control** table, which contains information about the inspection results, defect types, and rework counts. We will filter the data by **Product** to analyze the rework count variation by defect type and product.\n\n### Step 2: Group Data by Product and Defect Type\nWe will group the filtered data by **Product** and **Defect_Type** to prepare for further analysis.\n\n```sql\nSELECT Product, Defect_Type, SUM(Rework_Count) AS Total_Rework_Count\nFROM quality_control\nGROUP BY Product, Defect_Type\n```\n\n### Step 3: Calculate Average Rework Count per Defect Type\nTo understand the rework count variation by defect type, we will calculate the average rework count per defect type for each product.\n\n```sql\nSELECT Product, Defect_Type, AVG(Total_Rework_Count) AS Average_Rework_Count\nFROM (\n  SELECT Product, Defect_Type, SUM(Rework_Count) AS Total_Rework_Count\n  FROM quality_control\n  GROUP BY Product, Defect_Type\n) AS Subquery\nGROUP BY Product, Defect_Type\n```\n\n### Step 4: Calculate Total Rework Count per Product\nTo understand the overall rework count variation by product, we will calculate the total rework count per product.\n\n```sql\nSELECT Product, SUM(Rework_Count) AS Total_Rework_Count\nFROM quality_control\nGROUP BY Product\n```\n\n### Step 5: Combine Data for Final Analysis\nWe will combine the results from Step 3 and Step 4 to analyze the rework count variation by defect type and product.\n\n```sql\nSELECT p.Product, d.Defect_Type, d.Average_Rework_Count, p.Total_Rework_Count\nFROM (\n  SELECT Product, Defect_Type, AVG(Total_Rework_Count) AS Average_Rework_Count\n  FROM (\n    SELECT Product, Defect_Type, SUM(Rework_Count) AS Total_Rework_Count\n    FROM quality_control\n    GROUP BY Product, Defect_Type\n  ) AS Subquery\n  GROUP BY Product, Defect_Type\n) AS d\nJOIN (\n  SELECT Product, SUM(Rework_Count) AS Total_Rework_Count\n  FROM quality_control\n  GROUP BY Product\n) AS p ON d.Product = p.Product\n```\n\n### Step 6: Visualize the Results (Optional)\nTo better understand the rework count variation by defect type and product, we can create a bar chart or scatter plot using the results from Step 5.\n\nNote: The above steps assume a SQL database management system. The exact syntax may vary depending on the specific database system being used.",
    "sql_response": "```sql\nSELECT \n    qc.Defect_Type,\n    p.Product,\n    SUM(qc.Rework_Count) AS Total_Rework_Count\nFROM \n    quality_control qc\nJOIN \n    production_logs pl ON qc.Product = pl.Product\nJOIN \n    production_logs pl2 ON qc.Product = pl2.Product\nGROUP BY \n    qc.Defect_Type,\n    p.Product\nORDER BY \n    qc.Defect_Type,\n    p.Product;\n```\n\nHowever, the above query will not work as it is trying to join the production_logs table twice which is not allowed in SQL. \n\nHere's the correct query:\n\n```sql\nSELECT \n    qc.Defect_Type,\n    qc.Product,\n    SUM(qc.Rework_Count) AS Total_Rework_Count\nFROM \n    quality_control qc\nGROUP BY \n    qc.Defect_Type,\n    qc.Product\nORDER BY \n    qc.Defect_Type,\n    qc.Product;\n```\n\nThis query will give you the total rework count for each defect type and product.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns that contain information about the rework count, defect type, and product. Based on the available tables and columns, we can see that the `quality_control` table contains information about the rework count and defect type, and the `quality_control` and `production_logs` tables contain information about the product.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Defect_Type\", \"Product\", \"Rework_Count\"]\n}\n```\n\nHowever, we also need to consider the `production_logs` table to get the product information. So, the final JSON object would be:\n\n```json\n{\n  \"tables\": [\"quality_control\", \"production_logs\"],\n  \"columns\": [\"Defect_Type\", \"Product\", \"Rework_Count\"]\n}\n```\n\nBut we need to join the `quality_control` and `production_logs` tables to get the product information. We can use the `Product` column in both tables to join them.",
    "methodology_score": 77.77777777777779,
    "sql_score": 51.70161290322581,
    "table_column_score": 100.0,
    "response_quality_score": 92.00543189048767,
    "overall_score": 75.62944103851115,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 7,
      "expected_keyword_count": 9,
      "response_keyword_count": 95
    },
    "sql_details": {
      "table_match": 50.0,
      "column_match": 75.0,
      "aggregate_match": 25.0,
      "clause_match": 60.0,
      "fuzzy_score": 43.01075268817204,
      "overall_score": 51.70161290322581,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "qc",
        "defect_type",
        "rework_count"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "defect_type",
        "rework_count"
      ]
    },
    "total_latency_ms": 7994.568109512329,
    "total_tokens": 2112,
    "errors": [],
    "timestamp": "2025-11-30T13:01:41.160208"
  },
  {
    "question_id": "Medium_24",
    "question_text": "Which material has the highest wastage (in Kg) from July 2026 to Dec 2026?",
    "category": "Medium",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Step-by-Step Methodology to Answer the Question\n\n### Step 1: Filter the inventory_logs table to include data from July 2026 to Dec 2026\nUse the `Date` column in the `inventory_logs` table to filter the data for the specified time period.\n\n```sql\nSELECT *\nFROM inventory_logs\nWHERE Date BETWEEN '2026-07-01' AND '2026-12-31';\n```\n\n### Step 2: Calculate the total wastage for each material\nUse the `Wastage_Kg` column in the filtered `inventory_logs` table to calculate the total wastage for each material.\n\n```sql\nSELECT Material_Name, SUM(Wastage_Kg) AS Total_Wastage_Kg\nFROM inventory_logs\nWHERE Date BETWEEN '2026-07-01' AND '2026-12-31'\nGROUP BY Material_Name;\n```\n\n### Step 3: Identify the material with the highest wastage\nUse the `Total_Wastage_Kg` column from the previous step to identify the material with the highest wastage.\n\n```sql\nSELECT Material_Name, Total_Wastage_Kg\nFROM (\n  SELECT Material_Name, SUM(Wastage_Kg) AS Total_Wastage_Kg\n  FROM inventory_logs\n  WHERE Date BETWEEN '2026-07-01' AND '2026-12-31'\n  GROUP BY Material_Name\n) AS subquery\nORDER BY Total_Wastage_Kg DESC\nLIMIT 1;\n```\n\n### Step 4: Combine the results into a single row\nSince we are only interested in the material with the highest wastage, we can combine the results into a single row.\n\n```sql\nSELECT Material_Name AS Material_With_Highest_Wastage\nFROM (\n  SELECT Material_Name, SUM(Wastage_Kg) AS Total_Wastage_Kg\n  FROM inventory_logs\n  WHERE Date BETWEEN '2026-07-01' AND '2026-12-31'\n  GROUP BY Material_Name\n) AS subquery\nORDER BY Total_Wastage_Kg DESC\nLIMIT 1;\n```\n\n### Step 5: Execute the query to get the final answer\nRun the final query to get the material with the highest wastage from July 2026 to Dec 2026.\n\n```sql\nSELECT Material_Name AS Material_With_Highest_Wastage\nFROM (\n  SELECT Material_Name, SUM(Wastage_Kg) AS Total_Wastage_Kg\n  FROM inventory_logs\n  WHERE Date BETWEEN '2026-07-01' AND '2026-12-31'\n  GROUP BY Material_Name\n) AS subquery\nORDER BY Total_Wastage_Kg DESC\nLIMIT 1;\n```",
    "sql_response": "```sql\nSELECT \n    Material_Name, \n    SUM(Wastage_Kg) AS Total_Wastage\nFROM \n    inventory_logs\nWHERE \n    Date BETWEEN '2026-07-01' AND '2026-12-31'\nGROUP BY \n    Material_Name\nORDER BY \n    Total_Wastage DESC\nLIMIT 1;\n```\n\nThis SQL query will return the material with the highest total wastage from July 2026 to December 2026. The `BETWEEN` operator is used to filter the dates, `GROUP BY` is used to group the results by material, and `ORDER BY` is used to sort the results in descending order by total wastage. The `LIMIT 1` clause is used to return only the top result, which is the material with the highest total wastage.",
    "table_selection_response": "To answer the question, we need to identify the material with the highest wastage from July 2026 to December 2026. We can achieve this by analyzing the inventory logs, specifically the wastage of each material.\n\nBased on the available tables and columns, we can use the `inventory_logs` table to answer this question. We need the following columns:\n\n- `Material_Name` to identify the material\n- `Wastage_Kg` to calculate the total wastage\n\nHowever, we don't have a direct column for `Wastage_Kg` in the `inventory_logs` table. We can calculate the wastage by subtracting the `Closing_Stock_Kg` from the `Opening_Stock_Kg` and then subtracting the `Received_Kg` from the result.\n\nHere's the JSON object with the required tables and columns:\n\n```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Material_Name\", \"Opening_Stock_Kg\", \"Received_Kg\", \"Closing_Stock_Kg\"]\n}\n```\n\nWe can then use a SQL query to calculate the total wastage for each material and identify the material with the highest wastage.\n\n```sql\nSELECT \n  Material_Name,\n  (Opening_Stock_Kg - Closing_Stock_Kg) - Received_Kg AS Total_Wastage_Kg\nFROM \n  inventory_logs\nWHERE \n  Date BETWEEN '2026-07-01' AND '2026-12-31'\nORDER BY \n  Total_Wastage_Kg DESC;\n```\n\nThis query will return the material with the highest wastage from July 2026 to December 2026.",
    "methodology_score": 64.70588235294117,
    "sql_score": 75.0,
    "table_column_score": 57.14285714285715,
    "response_quality_score": 91.16543912887573,
    "overall_score": 69.06402290448422,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 11,
      "expected_keyword_count": 17,
      "response_keyword_count": 70
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 50.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 83.33333333333334,
      "overall_score": 75.0,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "sum",
        "material_name"
      ],
      "generated_columns": [
        "wastage_kg",
        "material_name",
        "total_wastage"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 57.14285714285715,
      "table_score": 100.0,
      "column_score": 28.571428571428577,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 25.0,
      "column_recall": 33.33333333333333,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "material_name"
      ],
      "columns_missing": [
        "wastage_kg",
        "date"
      ],
      "columns_extra": [
        "closing_stock_kg",
        "received_kg",
        "opening_stock_kg"
      ],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "material_name",
        "date"
      ],
      "generated_columns": [
        "closing_stock_kg",
        "material_name",
        "opening_stock_kg",
        "received_kg"
      ]
    },
    "total_latency_ms": 8834.560871124268,
    "total_tokens": 2224,
    "errors": [],
    "timestamp": "2025-11-30T13:01:56.389873"
  },
  {
    "question_id": "Complex_31",
    "question_text": "What is the correlation between downtime (in minutes) and failed quantity for each product?",
    "category": "Complex",
    "model_id": "llama-3.1-8b-instant",
    "methodology_response": "## Correlation between Downtime and Failed Quantity for Each Product\n\nTo answer this question, we'll follow these steps:\n\n### Step 1: Filter production logs for relevant data\nUse the **production_logs** table and filter for records where Actual_Qty is greater than 0 (to exclude zero production days) and Downtime_Minutes is greater than 0 (to include downtime records).\n\n```sql\nSELECT *\nFROM production_logs\nWHERE Actual_Qty > 0 AND Downtime_Minutes > 0;\n```\n\n### Step 2: Group production logs by product and calculate total downtime\nGroup the filtered production logs by Product and calculate the total Downtime_Minutes for each product.\n\n```sql\nSELECT Product, SUM(Downtime_Minutes) AS Total_Downtime_Minutes\nFROM (\n  SELECT *\n  FROM production_logs\n  WHERE Actual_Qty > 0 AND Downtime_Minutes > 0\n) AS filtered_logs\nGROUP BY Product;\n```\n\n### Step 3: Filter quality control logs for relevant data\nUse the **quality_control** table and filter for records where Failed_Qty is greater than 0 (to exclude zero failed quantity records).\n\n```sql\nSELECT *\nFROM quality_control\nWHERE Failed_Qty > 0;\n```\n\n### Step 4: Group quality control logs by product and calculate total failed quantity\nGroup the filtered quality control logs by Product and calculate the total Failed_Qty for each product.\n\n```sql\nSELECT Product, SUM(Failed_Qty) AS Total_Failed_Qty\nFROM (\n  SELECT *\n  FROM quality_control\n  WHERE Failed_Qty > 0\n) AS filtered_logs\nGROUP BY Product;\n```\n\n### Step 5: Merge production and quality control data\nMerge the results from steps 2 and 4 on the Product column to create a single dataset with total downtime and total failed quantity for each product.\n\n```sql\nSELECT p.Product, p.Total_Downtime_Minutes, q.Total_Failed_Qty\nFROM (\n  SELECT Product, SUM(Downtime_Minutes) AS Total_Downtime_Minutes\n  FROM (\n    SELECT *\n    FROM production_logs\n    WHERE Actual_Qty > 0 AND Downtime_Minutes > 0\n  ) AS filtered_logs\n  GROUP BY Product\n) AS p\nJOIN (\n  SELECT Product, SUM(Failed_Qty) AS Total_Failed_Qty\n  FROM (\n    SELECT *\n    FROM quality_control\n    WHERE Failed_Qty > 0\n  ) AS filtered_logs\n  GROUP BY Product\n) AS q\nON p.Product = q.Product;\n```\n\n### Step 6: Calculate correlation coefficient\nCalculate the correlation coefficient between Total_Downtime_Minutes and Total_Failed_Qty for each product using the Pearson correlation coefficient formula.\n\n```sql\nSELECT Product, \n       Total_Downtime_Minutes, \n       Total_Failed_Qty, \n       CORR(Total_Downtime_Minutes, Total_Failed_Qty) AS Correlation_Coefficient\nFROM (\n  SELECT p.Product, p.Total_Downtime_Minutes, q.Total_Failed_Qty\n  FROM (\n    SELECT Product, SUM(Downtime_Minutes) AS Total_Downtime_Minutes\n    FROM (\n      SELECT *\n      FROM production_logs\n      WHERE Actual_Qty > 0 AND Downtime_Minutes > 0\n    ) AS filtered_logs\n    GROUP BY Product\n  ) AS p\n  JOIN (\n    SELECT Product, SUM(Failed_Qty) AS Total_Failed_Qty\n    FROM (\n      SELECT *\n      FROM quality_control\n      WHERE Failed_Qty > 0\n    ) AS filtered_logs\n    GROUP BY Product\n  ) AS q\n  ON p.Product = q.Product\n) AS merged_data;\n```\n\n### Step 7: Interpret results\nInterpret the correlation coefficient values to understand the relationship between downtime and failed quantity for each product. A correlation coefficient close to 1 indicates a strong positive correlation, while a value close to -1 indicates a strong negative correlation. A value close to 0 indicates no correlation.",
    "sql_response": "```sql\nWITH \n  -- Calculate downtime per minute for each production log entry\n  production_downtime AS (\n    SELECT \n      Date,\n      Shift,\n      Line_Machine,\n      Product,\n      Target_Qty,\n      Actual_Qty,\n      Downtime_Minutes,\n      Operator,\n      Downtime_Minutes / (Target_Qty * 60) AS Downtime_Percentage\n    FROM \n      production_logs\n  ),\n  \n  -- Calculate failed quantity for each quality control entry\n  quality_control_failed AS (\n    SELECT \n      Inspection_Date,\n      Batch_ID,\n      Product,\n      Line,\n      Inspected_Qty,\n      Failed_Qty\n    FROM \n      quality_control\n  )\n\n-- Calculate correlation between downtime and failed quantity for each product\nSELECT \n  pc.Product,\n  AVG(pd.Downtime_Percentage) AS Average_Downtime_Percentage,\n  AVG(qc.Failed_Qty) AS Average_Failed_Quantity,\n  CORR(pd.Downtime_Percentage, qc.Failed_Qty) AS Correlation_Coefficient\nFROM \n  production_downtime pd\n  JOIN quality_control_failed qc ON pd.Product = qc.Product\nGROUP BY \n  pc.Product\nORDER BY \n  pc.Product;\n```\n\nThis SQL query uses two Common Table Expressions (CTEs) to calculate the downtime percentage for each production log entry and the failed quantity for each quality control entry. It then joins these two CTEs on the product column and calculates the correlation between the downtime percentage and the failed quantity for each product using the CORR() function. The results are grouped by product and ordered by product.",
    "table_selection_response": "To answer the question, we need to identify the tables and columns that contain the required information. \n\nFor the correlation between downtime (in minutes) and failed quantity for each product, we need the following:\n\n- Downtime minutes: This can be found in the `production_logs` table under the `Downtime_Minutes` column.\n- Failed quantity: This can be found in the `quality_control` table under the `Failed_Qty` column.\n- Product: This can be found in both the `production_logs` and `quality_control` tables under the `Product` column.\n\nTherefore, the required tables and columns are:\n\n```json\n{\n  \"tables\": [\"production_logs\", \"quality_control\"],\n  \"columns\": [\"Product\", \"Downtime_Minutes\", \"Failed_Qty\"]\n}\n```\n\nHowever, since we are looking for a correlation between downtime and failed quantity for each product, we should consider the average downtime and failed quantity for each product. Therefore, we should use aggregate functions like `AVG` to calculate the average downtime and failed quantity.\n\n```json\n{\n  \"tables\": [\"production_logs\", \"quality_control\"],\n  \"columns\": [\"Product\", \"AVG(Downtime_Minutes)\", \"AVG(Failed_Qty)\"]\n}\n```\n\nNote: The actual SQL query to calculate the correlation would involve using the `AVG` function and possibly the `GROUP BY` clause to group the results by product.",
    "methodology_score": 87.5,
    "sql_score": 52.52725118483412,
    "table_column_score": 100.0,
    "response_quality_score": 91.2448308467865,
    "overall_score": 78.75902099937059,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 14,
      "expected_keyword_count": 16,
      "response_keyword_count": 94
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 15.0,
      "aggregate_match": 25.0,
      "clause_match": 80.0,
      "fuzzy_score": 45.181674565560826,
      "overall_score": 52.52725118483412,
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "pl",
        "product",
        "totalfailedqty",
        "downtime_minutes",
        "totaldowntime",
        "failed_qty",
        "qc"
      ],
      "generated_columns": [
        "downtime_percentage",
        "inspected_qty",
        "batch_id",
        "line_machine",
        "product",
        "inspection_date",
        "pc",
        "date",
        "downtime_minutes",
        "operator",
        "downtime_minutes / target_qty * 60",
        "target_qty",
        "failed_qty",
        "line",
        "actual_qty",
        "shift"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "production_logs",
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "downtime_minutes",
        "failed_qty"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "downtime_minutes",
        "failed_qty"
      ],
      "generated_columns": [
        "product",
        "downtime_minutes",
        "failed_qty"
      ]
    },
    "total_latency_ms": 8755.169153213501,
    "total_tokens": 2608,
    "errors": [],
    "timestamp": "2025-11-30T13:02:11.831870"
  }
]