[
  {
    "question_id": "Complex_13",
    "question_text": "How does the average rework count for 'Widget-A' differ between Line-1 and Line-2?",
    "category": "Complex",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data table**: The question is about the average rework count for a specific product ('Widget-A') and how it differs between two production lines (Line-1 and Line-2). The **quality_control** table contains the necessary information, including product, line, and rework count.\n\n2. **Filter the data for 'Widget-A'**: Apply a filter to the **quality_control** table to only include rows where the **Product** is 'Widget-A'. This ensures that the analysis is focused on the specified product.\n\n3. **Filter the data by production line**: Within the filtered data for 'Widget-A', separate the data into two groups based on the **Line** column: one group for Line-1 and another for Line-2. This will allow for the comparison of rework counts between the two lines.\n\n4. **Calculate the average rework count for each line**: For each line (Line-1 and Line-2), calculate the average **Rework_Count**. This involves summing up all the rework counts for each line and then dividing by the number of observations (rows) for that line.\n\n5. **Compare the average rework counts**: Once the average rework counts are calculated for Line-1 and Line-2, compare these values to understand how the average rework count for 'Widget-A' differs between the two production lines.\n\n6. **Optional: Calculate the statistical significance of the difference**: If necessary, perform a statistical test (such as a t-test) to determine if the difference in average rework counts between Line-1 and Line-2 is statistically significant. This step would require calculating the standard deviation of rework counts for each line and the number of observations.\n\n7. **Present the findings**: Finally, present the calculated average rework counts for 'Widget-A' on Line-1 and Line-2, along with any conclusions drawn from comparing these averages. If statistical significance was tested, include the results of this test in the presentation.\n```",
    "sql_response": "```sql\nSELECT \n    Line, \n    AVG(Rework_Count) AS Average_Rework_Count\nFROM \n    quality_control\nWHERE \n    Product = 'Widget-A' AND Line IN ('Line-1', 'Line-2')\nGROUP BY \n    Line;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Product\", \"Line\", \"Rework_Count\"]\n}\n```",
    "methodology_score": 88.88888888888889,
    "sql_score": 97.73062730627306,
    "table_column_score": 100.0,
    "response_quality_score": 97.54558777809143,
    "overall_score": 95.62694500167137,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 16,
      "expected_keyword_count": 18,
      "response_keyword_count": 125
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 84.87084870848709,
      "overall_score": 97.73062730627306,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "line",
        "product",
        "rework_count"
      ],
      "generated_columns": [
        "line",
        "product",
        "rework_count"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "line",
        "rework_count"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "line",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "line",
        "rework_count"
      ]
    },
    "total_latency_ms": 2454.4122219085693,
    "total_tokens": 1642,
    "errors": [],
    "timestamp": "2025-11-30T13:02:20.838325"
  },
  {
    "question_id": "Easy_15",
    "question_text": "Which inspector had the highest number of reworked items?",
    "category": "Easy",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. Identify the relevant table for the question, which is **quality_control**, as it contains information about inspections, including rework counts and inspector names.\n2. Filter the **quality_control** table to only include rows where Rework_Count is greater than 0, to focus on inspections that resulted in reworked items.\n3. Group the filtered **quality_control** table by Inspector_Name to aggregate the Rework_Count for each inspector.\n4. Calculate the total Rework_Count for each inspector by summing up the Rework_Count values within each group.\n5. Sort the grouped data in descending order based on the total Rework_Count for each inspector.\n6. Select the inspector with the highest total Rework_Count, which will be the first row after sorting in descending order.\n7. If there are multiple inspectors tied for the highest number of reworked items, consider applying additional criteria (if any) from the question or further analysis to differentiate among them. However, based on the given question, identifying the inspector(s) with the highest number of reworked items is sufficient.\n```",
    "sql_response": "```sql\nSELECT \n    Inspector_Name, \n    SUM(Rework_Count) AS Total_Reworked_Items\nFROM \n    quality_control\nGROUP BY \n    Inspector_Name\nORDER BY \n    Total_Reworked_Items DESC\nLIMIT 1;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Inspector_Name\", \"Rework_Count\"]\n}\n```",
    "methodology_score": 81.81818181818183,
    "sql_score": 78.11132561132561,
    "table_column_score": 100.0,
    "response_quality_score": 98.42527723312378,
    "overall_score": 86.72694623273088,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 9,
      "expected_keyword_count": 11,
      "response_keyword_count": 82
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 50.0,
      "aggregate_match": 66.66666666666666,
      "clause_match": 100.0,
      "fuzzy_score": 81.85328185328186,
      "overall_score": 78.11132561132561,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "sum",
        "rework_count",
        "inspector_name"
      ],
      "generated_columns": [
        "total_reworked_items",
        "rework_count",
        "inspector_name"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "rework_count",
        "inspector_name"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "rework_count",
        "inspector_name"
      ],
      "generated_columns": [
        "rework_count",
        "inspector_name"
      ]
    },
    "total_latency_ms": 1574.7227668762207,
    "total_tokens": 1408,
    "errors": [],
    "timestamp": "2025-11-30T13:02:28.694649"
  },
  {
    "question_id": "Easy_4",
    "question_text": "What is the total number of components reworked in Line-2?",
    "category": "Easy",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. Identify the relevant table for the question: The **quality_control** table contains information about inspected quantities, passed quantities, failed quantities, and rework counts, which is relevant to calculating the total number of components reworked.\n\n2. Filter the data for Line-2: Apply a filter to the **quality_control** table to only include rows where the Line column equals 'Line-2'.\n\n3. Extract the Rework_Count column: From the filtered data, extract the Rework_Count column, which represents the number of components reworked for each batch.\n\n4. Calculate the total rework count: Sum up the Rework_Count values from the extracted column to get the total number of components reworked in Line-2.\n\n5. Consider potential data issues: Check for any potential data issues, such as missing values or inconsistencies in the Rework_Count column, and decide on a course of action (e.g., ignoring missing values or imputing them).\n\n6. Verify the calculation: Double-check the calculation to ensure it accurately represents the total number of components reworked in Line-2.\n```",
    "sql_response": "```sql\nSELECT SUM(Rework_Count) \nFROM quality_control \nWHERE Line = 'Line-2' AND Product IN ('Component-X', 'Component-Y');\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Line\", \"Rework_Count\"]\n}\n```\n\nTo find the total number of components reworked in Line-2, we need to access the `quality_control` table, specifically the `Line` column to filter for Line-2 and the `Rework_Count` column to sum up the reworked components.",
    "methodology_score": 71.42857142857143,
    "sql_score": 96.27167630057804,
    "table_column_score": 100.0,
    "response_quality_score": 98.30176186561584,
    "overall_score": 89.95383432033532,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 5,
      "expected_keyword_count": 7,
      "response_keyword_count": 73
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 75.14450867052022,
      "overall_score": 96.27167630057804,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "line",
        "rework_count"
      ],
      "generated_columns": [
        "line",
        "rework_count"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "line",
        "rework_count"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "line",
        "rework_count"
      ],
      "generated_columns": [
        "line",
        "rework_count"
      ]
    },
    "total_latency_ms": 1698.2381343841553,
    "total_tokens": 1456,
    "errors": [],
    "timestamp": "2025-11-30T13:02:37.020800"
  },
  {
    "question_id": "Complex_26",
    "question_text": "What is the impact of preventive maintenance on the subsequent breakdown frequency for Machine-M1?",
    "category": "Complex",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify relevant tables**: To analyze the impact of preventive maintenance on breakdown frequency for Machine-M1, we need to use the **maintenance_logs** table, as it contains information about maintenance activities, including preventive maintenance, and the **production_logs** table to understand the operational context of the machine.\n\n2. **Filter maintenance logs for Machine-M1**: Apply a filter to the **maintenance_logs** table to only include rows where the **Machine** column equals 'M1'. This will help us focus on the maintenance history specific to Machine-M1.\n\n3. **Distinguish between preventive and corrective maintenance**: Within the filtered **maintenance_logs** table for Machine-M1, separate the maintenance activities into preventive and corrective (breakdown) maintenance. This can be done by analyzing the **Maintenance_Type** column, assuming it categorizes maintenance as preventive or corrective, and the **Breakdown_Date** column, which would be relevant for corrective maintenance.\n\n4. **Calculate the frequency of breakdowns before and after preventive maintenance**: For each instance of preventive maintenance, calculate the time until the next breakdown (if any) and the frequency of breakdowns within a certain timeframe (e.g., monthly) before and after the preventive maintenance. This involves:\n   - Sorting the **maintenance_logs** by **Maintenance_Date**.\n   - For each preventive maintenance record, finding the next record that indicates a breakdown (where **Breakdown_Date** is not null or **Maintenance_Type** indicates corrective maintenance).\n   - Calculating the time difference between these events.\n\n5. **Analyze production logs for downtime**: Use the **production_logs** table to analyze the downtime minutes (**Downtime_Minutes**) for Machine-M1 around the times of preventive and corrective maintenance. This can help in understanding the operational impact of maintenance on production.\n\n6. **Combine data for analysis**: Combine the filtered and analyzed data from the **maintenance_logs** and **production_logs** tables to get a comprehensive view. This might involve calculating averages, such as the average time between breakdowns before and after preventive maintenance, or the average downtime minutes per shift/month before and after preventive maintenance.\n\n7. **Perform statistical analysis**: To quantify the impact of preventive maintenance, perform statistical analyses (e.g., t-tests, regression analysis) to compare the frequency of breakdowns and downtime before and after preventive maintenance. This will help in determining if there is a significant reduction in breakdown frequency due to preventive maintenance.\n\n8. **Visualize the findings**: Finally, visualize the data using plots (e.g., time series plots for breakdown frequency over time, bar charts for comparing averages) to clearly illustrate the impact of preventive maintenance on the subsequent breakdown frequency for Machine-M1.\n```",
    "sql_response": "```sql\nWITH maintenance_data AS (\n  SELECT \n    Maintenance_Date,\n    Machine,\n    Maintenance_Type,\n    Breakdown_Date,\n    Downtime_Hours,\n    LAG(Maintenance_Date, 1) OVER (PARTITION BY Machine ORDER BY Maintenance_Date) AS prev_maintenance_date,\n    LAG(Maintenance_Type, 1) OVER (PARTITION BY Machine ORDER BY Maintenance_Date) AS prev_maintenance_type\n  FROM \n    maintenance_logs\n  WHERE \n    Machine = 'Machine-M1'\n),\nbreakdown_frequency AS (\n  SELECT \n    Maintenance_Date,\n    Machine,\n    Maintenance_Type,\n    Breakdown_Date,\n    Downtime_Hours,\n    prev_maintenance_date,\n    prev_maintenance_type,\n    CASE \n      WHEN prev_maintenance_type = 'Preventive' THEN DATEDIFF(Breakdown_Date, prev_maintenance_date)\n      ELSE NULL\n    END AS days_to_breakdown\n  FROM \n    maintenance_data\n  WHERE \n    Maintenance_Type = 'Breakdown'\n)\nSELECT \n  AVG(days_to_breakdown) AS average_days_to_breakdown\nFROM \n  breakdown_frequency\nWHERE \n  days_to_breakdown IS NOT NULL;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Machine\", \"Maintenance_Type\", \"Breakdown_Date\", \"Downtime_Hours\"]\n}\n```",
    "methodology_score": 55.00000000000001,
    "sql_score": 68.77214629829909,
    "table_column_score": 85.0,
    "response_quality_score": 97.16247892379761,
    "overall_score": 71.53649909678444,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 11,
      "expected_keyword_count": 20,
      "response_keyword_count": 155
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 21.428571428571427,
      "aggregate_match": 100.0,
      "clause_match": 60.0,
      "fuzzy_score": 62.766689607708194,
      "overall_score": 68.77214629829909,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "breakdown_date",
        "maintenance_date",
        "daystobreakdown",
        "machine"
      ],
      "generated_columns": [
        "maintenance_date",
        "case when prev_maintenance_type = preventive then datediffbreakdown_date",
        "days_to_breakdown",
        "lagmaintenance_date",
        "lagmaintenance_type",
        "prev_maintenance_type",
        "breakdown_date",
        "prev_maintenance_date else null end",
        "maintenance_type",
        "machine",
        "1 over partition by machine order by maintenance_date",
        "downtime_hours",
        "prev_maintenance_date"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 85.0,
      "table_score": 100.0,
      "column_score": 75.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 75.0,
      "column_recall": 75.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "breakdown_date",
        "maintenance_type",
        "machine"
      ],
      "columns_missing": [
        "maintenance_date"
      ],
      "columns_extra": [
        "downtime_hours"
      ],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "breakdown_date",
        "maintenance_date",
        "maintenance_type",
        "machine"
      ],
      "generated_columns": [
        "breakdown_date",
        "downtime_hours",
        "maintenance_type",
        "machine"
      ]
    },
    "total_latency_ms": 2837.5210762023926,
    "total_tokens": 1956,
    "errors": [],
    "timestamp": "2025-11-30T13:02:46.123115"
  },
  {
    "question_id": "Medium_2",
    "question_text": "What is the total cost of maintenance for Machine-M2 in 2026?",
    "category": "Medium",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. Identify the relevant data table for maintenance costs, which is the **maintenance_logs** table.\n2. Filter the **maintenance_logs** table to only include records where the Machine is 'M2' to focus on the specific machine in question.\n3. Further filter the **maintenance_logs** table to only include records where the Maintenance_Date falls within the year 2026, to ensure the analysis is limited to the specified time frame.\n4. Apply a filter to ensure that only records with a valid Cost_Rupees value are included, to prevent any potential errors in cost calculations.\n5. Calculate the total cost of maintenance for Machine-M2 in 2026 by summing up the Cost_Rupees column for the filtered records.\n6. Since the cost is already in rupees and the question asks for the total cost, no further currency conversions or aggregations beyond summation are required.\n7. If there are any duplicate records or inconsistencies in the data that could affect the accuracy of the total cost calculation, perform data cleaning steps as necessary before calculating the total cost.\n8. Finally, present the calculated total cost as the answer to the question, ensuring it is clearly labeled and easy to understand.\n```",
    "sql_response": "```sql\nSELECT SUM(Cost_Rupees) AS Total_Maintenance_Cost\nFROM maintenance_logs\nWHERE Machine = 'Machine-M2' AND EXTRACT(YEAR FROM Maintenance_Date) = 2026;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Machine\", \"Maintenance_Date\", \"Cost_Rupees\"]\n}\n```",
    "methodology_score": 44.44444444444444,
    "sql_score": 86.65384615384616,
    "table_column_score": 100.0,
    "response_quality_score": 98.27699422836304,
    "overall_score": 78.48987891001579,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 4,
      "expected_keyword_count": 9,
      "response_keyword_count": 88
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 77.6923076923077,
      "overall_score": 86.65384615384616,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "machine"
      ],
      "generated_columns": [
        "cost_rupees",
        "machine"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "cost_rupees",
        "maintenance_date",
        "machine"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "maintenance_date",
        "machine"
      ],
      "generated_columns": [
        "cost_rupees",
        "maintenance_date",
        "machine"
      ]
    },
    "total_latency_ms": 1723.005771636963,
    "total_tokens": 1446,
    "errors": [],
    "timestamp": "2025-11-30T13:02:54.123159"
  },
  {
    "question_id": "Easy_32",
    "question_text": "What is the percentage of failed quantity for each product?",
    "category": "Easy",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data table**: To calculate the percentage of failed quantity for each product, we need to use the **quality_control** table, as it contains the necessary information about inspected quantities, passed quantities, and failed quantities.\n\n2. **Filter the data if necessary**: Depending on the specific requirements, we might need to filter the data by a specific date range or inspector. However, since the question does not specify any filters, we will consider all the data in the **quality_control** table.\n\n3. **Calculate the total inspected quantity for each product**: We need to group the data by **Product** and calculate the sum of **Inspected_Qty** for each group. This will give us the total quantity inspected for each product.\n\n4. **Calculate the total failed quantity for each product**: Similarly, we need to group the data by **Product** and calculate the sum of **Failed_Qty** for each group. This will give us the total quantity failed for each product.\n\n5. **Calculate the percentage of failed quantity for each product**: To calculate the percentage of failed quantity, we will divide the total failed quantity for each product by the total inspected quantity for each product, and then multiply by 100.\n\n6. **Perform the calculations using SQL or a data analysis tool**: The calculations can be performed using a SQL query or a data analysis tool like Excel or Python. The query would look something like this:\n   - SELECT Product, SUM(Failed_Qty) / SUM(Inspected_Qty) * 100 AS Failure_Percentage\n   - FROM quality_control\n   - GROUP BY Product\n\n7. **Round the results to a suitable number of decimal places**: To make the results more readable, we can round the failure percentages to two decimal places.\n\n8. **Sort the results by product or failure percentage**: Finally, we can sort the results by product or failure percentage to make it easier to analyze and compare the results.\n```",
    "sql_response": "```sql\nSELECT \n    Product,\n    SUM(Failed_Qty) AS Total_Failed_Qty,\n    SUM(Inspected_Qty) AS Total_Inspected_Qty,\n    ROUND((SUM(Failed_Qty) * 1.0 / SUM(Inspected_Qty)) * 100, 2) AS Percentage_Failed_Qty\nFROM \n    quality_control\nGROUP BY \n    Product\nORDER BY \n    Product;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Product\", \"Inspected_Qty\", \"Failed_Qty\"]\n}\n```\n\nTo calculate the percentage of failed quantity for each product, we need the `quality_control` table, which contains the inspection results. The required columns are:\n\n- `Product`: to identify the product\n- `Inspected_Qty`: to know the total quantity inspected\n- `Failed_Qty`: to know the quantity that failed the inspection\n\nThese columns will allow us to calculate the percentage of failed quantity for each product using the formula: `(Failed_Qty / Inspected_Qty) * 100`.",
    "methodology_score": 76.92307692307693,
    "sql_score": 63.658963585434165,
    "table_column_score": 100.0,
    "response_quality_score": 97.01954388618469,
    "overall_score": 80.05951472044352,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 10,
      "expected_keyword_count": 13,
      "response_keyword_count": 105
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 33.33333333333333,
      "aggregate_match": 50.0,
      "clause_match": 75.0,
      "fuzzy_score": 60.504201680672274,
      "overall_score": 63.658963585434165,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "0 / inspected_qty",
        "failed_qty"
      ],
      "generated_columns": [
        "inspected_qty",
        "product",
        "0 / inspected_qty * 100",
        "failed_qty",
        "2"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "inspected_qty",
        "failed_qty"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "inspected_qty",
        "failed_qty"
      ],
      "generated_columns": [
        "product",
        "inspected_qty",
        "failed_qty"
      ]
    },
    "total_latency_ms": 2980.4561138153076,
    "total_tokens": 1737,
    "errors": [],
    "timestamp": "2025-11-30T13:03:03.500822"
  },
  {
    "question_id": "Easy_29",
    "question_text": "What is the maximum downtime hours recorded for any breakdown?",
    "category": "Easy",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. Identify the relevant table for the question, which is the **maintenance_logs** table, as it contains information about machine breakdowns and downtime hours.\n2. Apply a filter to only consider rows where a breakdown has occurred, i.e., where **Breakdown_Date** is not null.\n3. Use the **Downtime_Hours** column to find the maximum downtime hours recorded for any breakdown.\n4. Perform a calculation to find the maximum value in the **Downtime_Hours** column, which can be done using the MAX() aggregation function.\n5. Execute a query on the **maintenance_logs** table to retrieve the maximum downtime hours, using a SQL statement such as: SELECT MAX(Downtime_Hours) AS max_downtime FROM maintenance_logs WHERE Breakdown_Date IS NOT NULL.\n6. Run the query to obtain the result, which will provide the maximum downtime hours recorded for any breakdown.\n```",
    "sql_response": "```sql\nSELECT MAX(Downtime_Hours) AS Max_Downtime_Hours\nFROM maintenance_logs\nWHERE Maintenance_Type = 'Breakdown';\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"maintenance_logs\"], \n  \"columns\": [\"Downtime_Hours\"]\n}\n```",
    "methodology_score": 66.66666666666666,
    "sql_score": 98.17460317460318,
    "table_column_score": 80.0,
    "response_quality_score": 98.48502802848816,
    "overall_score": 84.20961391395991,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 6,
      "expected_keyword_count": 9,
      "response_keyword_count": 63
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 87.83068783068784,
      "overall_score": 98.17460317460318,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "downtime_hours",
        "maintenance_type"
      ],
      "generated_columns": [
        "downtime_hours",
        "maintenance_type"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 80.0,
      "table_score": 100.0,
      "column_score": 66.66666666666666,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 50.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "downtime_hours"
      ],
      "columns_missing": [
        "maintenance_type"
      ],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "downtime_hours",
        "maintenance_type"
      ],
      "generated_columns": [
        "downtime_hours"
      ]
    },
    "total_latency_ms": 1514.9719715118408,
    "total_tokens": 1356,
    "errors": [],
    "timestamp": "2025-11-30T13:03:11.689738"
  },
  {
    "question_id": "Easy_18",
    "question_text": "How many preventive maintenance activities were performed on Machine-M2 this year?",
    "category": "Easy",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. Identify the relevant data table for maintenance activities, which is the **maintenance_logs** table.\n2. Filter the **maintenance_logs** table to only include rows where the **Machine** column is 'M2', as we are interested in maintenance activities performed on Machine-M2.\n3. Filter the **maintenance_logs** table further to only include rows where the **Maintenance_Date** falls within the current year, to ensure we are only counting maintenance activities performed this year.\n4. Filter the **maintenance_logs** table to only include rows where the **Maintenance_Type** is 'Preventive', as we are specifically interested in preventive maintenance activities.\n5. Count the number of rows in the filtered **maintenance_logs** table, which represents the total number of preventive maintenance activities performed on Machine-M2 this year.\n6. Since we are only using a single table (**maintenance_logs**), there is no need to combine data from multiple tables for this question.\n7. The final count of rows from step 5 is the answer to the question, providing the total number of preventive maintenance activities performed on Machine-M2 this year.\n```",
    "sql_response": "```sql\nSELECT COUNT(*) \nFROM maintenance_logs \nWHERE Machine = 'Machine-M2' \n  AND Maintenance_Type = 'Preventive' \n  AND EXTRACT(YEAR FROM Maintenance_Date) = EXTRACT(YEAR FROM CURRENT_DATE);\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Maintenance_Date\", \"Machine\", \"Maintenance_Type\"]\n}\n```\n\nTo answer the question, we need to analyze the `maintenance_logs` table, specifically the `Maintenance_Date`, `Machine`, and `Maintenance_Type` columns. The `Maintenance_Date` column will help us filter the data to this year, the `Machine` column will allow us to focus on Machine-M2, and the `Maintenance_Type` column will enable us to identify preventive maintenance activities.",
    "methodology_score": 41.66666666666667,
    "sql_score": 96.48571428571428,
    "table_column_score": 100.0,
    "response_quality_score": 98.43377423286438,
    "overall_score": 81.11337742328644,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 5,
      "expected_keyword_count": 12,
      "response_keyword_count": 60
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 76.57142857142857,
      "overall_score": 96.48571428571428,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "machine"
      ],
      "generated_columns": [
        "machine"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "maintenance_date",
        "maintenance_type",
        "machine"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "maintenance_date",
        "maintenance_type",
        "machine"
      ],
      "generated_columns": [
        "maintenance_date",
        "maintenance_type",
        "machine"
      ]
    },
    "total_latency_ms": 1566.2257671356201,
    "total_tokens": 1507,
    "errors": [],
    "timestamp": "2025-11-30T13:03:19.821635"
  },
  {
    "question_id": "Complex_32",
    "question_text": "Which machine has the highest average downtime due to breakdowns in the last quarter, and what is the total cost associated with those breakdowns?",
    "category": "Complex",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data table**: To answer this question, we need to use the **maintenance_logs** table, as it contains information about machine breakdowns, downtime, and associated costs.\n\n2. **Filter the data for the last quarter**: Apply a filter to the **maintenance_logs** table to only include records where the **Breakdown_Date** falls within the last quarter (e.g., the last 3 months). This will ensure we're only analyzing recent breakdown data.\n\n3. **Calculate the total downtime for each machine**: Group the filtered data by **Machine** and calculate the sum of **Downtime_Hours** for each machine. This will give us the total downtime for each machine due to breakdowns in the last quarter.\n\n4. **Calculate the average downtime for each machine**: Divide the total downtime for each machine (calculated in step 3) by the number of breakdowns for that machine. This will give us the average downtime per breakdown for each machine.\n\n5. **Identify the machine with the highest average downtime**: Sort the machines by their average downtime in descending order and select the machine with the highest average downtime.\n\n6. **Calculate the total cost associated with breakdowns for the identified machine**: Filter the **maintenance_logs** table to only include records for the machine identified in step 5, and calculate the sum of **Cost_Rupees** for those records. This will give us the total cost associated with breakdowns for the machine with the highest average downtime.\n\n7. **Combine the results**: Combine the results from steps 5 and 6 to provide the final answer, which includes the machine with the highest average downtime due to breakdowns in the last quarter and the total cost associated with those breakdowns.\n```",
    "sql_response": "```sql\nSELECT \n    M.Machine, \n    AVG(M.Downtime_Hours) AS Average_Downtime, \n    SUM(M.Cost_Rupees) AS Total_Cost\nFROM \n    maintenance_logs M\nWHERE \n    M.Maintenance_Type = 'Breakdown' \n    AND M.Maintenance_Date >= DATEADD(quarter, -1, GETDATE())\nGROUP BY \n    M.Machine\nORDER BY \n    Average_Downtime DESC\nLIMIT 1;\n```\nThis query will return the machine with the highest average downtime due to breakdowns in the last quarter, along with the total cost associated with those breakdowns. \n\nPlease note that the `DATEADD` and `GETDATE()` functions are specific to SQL Server. If you are using a different database management system, you may need to use different functions to achieve the same result. For example, in MySQL, you would use `NOW()` and `INTERVAL` instead:\n\n```sql\nSELECT \n    M.Machine, \n    AVG(M.Downtime_Hours) AS Average_Downtime, \n    SUM(M.Cost_Rupees) AS Total_Cost\nFROM \n    maintenance_logs M\nWHERE \n    M.Maintenance_Type = 'Breakdown' \n    AND M.Maintenance_Date >= NOW() - INTERVAL 3 MONTH\nGROUP BY \n    M.Machine\nORDER BY \n    Average_Downtime DESC\nLIMIT 1;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Machine\", \"Downtime_Hours\", \"Cost_Rupees\", \"Maintenance_Date\", \"Breakdown_Date\", \"Maintenance_Type\"]\n}\n```",
    "methodology_score": 65.51724137931035,
    "sql_score": 80.00914634146342,
    "table_column_score": 94.54545454545455,
    "response_quality_score": 97.59203958511353,
    "overall_score": 81.05394122818028,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 19,
      "expected_keyword_count": 29,
      "response_keyword_count": 91
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 37.5,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 70.89430894308944,
      "overall_score": 80.00914634146342,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "totalcost",
        "avgdowntime",
        "maintenance_type",
        "machine",
        "cost_rupees",
        "downtime_hours"
      ],
      "generated_columns": [
        "average_downtime",
        "machine",
        "cost_rupees",
        "downtime_hours",
        "m"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 94.54545454545455,
      "table_score": 100.0,
      "column_score": 90.9090909090909,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 83.33333333333334,
      "column_recall": 100.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "maintenance_date",
        "maintenance_type",
        "machine",
        "cost_rupees",
        "downtime_hours"
      ],
      "columns_missing": [],
      "columns_extra": [
        "breakdown_date"
      ],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "maintenance_date",
        "maintenance_type",
        "machine",
        "cost_rupees",
        "downtime_hours"
      ],
      "generated_columns": [
        "maintenance_date",
        "breakdown_date",
        "maintenance_type",
        "machine",
        "cost_rupees",
        "downtime_hours"
      ]
    },
    "total_latency_ms": 2407.9604148864746,
    "total_tokens": 1862,
    "errors": [],
    "timestamp": "2025-11-30T13:03:28.514989"
  },
  {
    "question_id": "Easy_14",
    "question_text": "What is the total wastage in kilograms for 'Steel Sheet 101' this year?",
    "category": "Easy",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. Identify the relevant data table for calculating wastage, which is the **inventory_logs** table, as it contains the 'Wastage_Kg' column.\n2. Filter the **inventory_logs** table to only include rows where 'Material_Name' is 'Steel Sheet 101' to focus on the specific material of interest.\n3. Apply a date filter to the **inventory_logs** table to only include records from the current year, using the 'Date' column. This ensures that only relevant, recent data is considered.\n4. Calculate the total wastage in kilograms for 'Steel Sheet 101' this year by summing up the 'Wastage_Kg' column for the filtered records.\n5. Since the question only requires data from the **inventory_logs** table, there is no need to combine data from multiple tables for this specific query.\n6. Execute the query with the specified filters and aggregation to obtain the total wastage in kilograms for 'Steel Sheet 101' for the current year.\n```",
    "sql_response": "```sql\nSELECT \n    SUM(Wastage_Kg) AS Total_Wastage\nFROM \n    inventory_logs\nWHERE \n    Material_Name = 'Steel Sheet 101'\n    AND EXTRACT(YEAR FROM Date) = EXTRACT(YEAR FROM CURRENT_DATE);\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Material_Name\", \"Wastage_Kg\", \"Date\"]\n}\n```",
    "methodology_score": 53.84615384615385,
    "sql_score": 72.01923076923077,
    "table_column_score": 100.0,
    "response_quality_score": 98.24731588363647,
    "overall_score": 76.18530851144058,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 7,
      "expected_keyword_count": 13,
      "response_keyword_count": 65
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 50.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 63.46153846153846,
      "overall_score": 72.01923076923077,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg"
      ],
      "generated_columns": [
        "wastage_kg",
        "material_name"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "wastage_kg",
        "material_name",
        "date"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "material_name",
        "date"
      ],
      "generated_columns": [
        "wastage_kg",
        "material_name",
        "date"
      ]
    },
    "total_latency_ms": 1752.6841163635254,
    "total_tokens": 1427,
    "errors": [],
    "timestamp": "2025-11-30T13:03:36.598386"
  },
  {
    "question_id": "Complex_18",
    "question_text": "What is the average unit cost of materials purchased from each supplier, and which supplier has the lowest average cost?",
    "category": "Complex",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data table**: The **inventory_logs** table contains information about material purchases, including the supplier and unit cost. This table will be used to answer the question.\n\n2. **Filter the data to only include purchase transactions**: Apply a filter to the **inventory_logs** table to only include rows where the **Received_Kg** column is greater than 0, indicating a purchase transaction.\n\n3. **Calculate the total cost of materials purchased from each supplier**: Use the **Unit_Cost_Rupees** and **Received_Kg** columns to calculate the total cost of materials purchased from each supplier. This can be done by multiplying the **Unit_Cost_Rupees** by the **Received_Kg** for each row.\n\n4. **Calculate the total quantity of materials purchased from each supplier**: Use the **Received_Kg** column to calculate the total quantity of materials purchased from each supplier. This can be done by summing up the **Received_Kg** for each supplier.\n\n5. **Calculate the average unit cost of materials purchased from each supplier**: Divide the total cost of materials purchased from each supplier (calculated in step 3) by the total quantity of materials purchased from each supplier (calculated in step 4). This will give the average unit cost of materials purchased from each supplier.\n\n6. **Identify the supplier with the lowest average cost**: Sort the results from step 5 in ascending order by average unit cost and identify the supplier with the lowest average cost.\n\n7. **Consider potential data issues**: Check for potential data issues such as missing or null values in the **Unit_Cost_Rupees** or **Received_Kg** columns, and handle them accordingly to ensure accurate results.\n\n8. **Combine the results**: Combine the results from steps 5 and 6 to provide a comprehensive answer to the question, including the average unit cost of materials purchased from each supplier and the supplier with the lowest average cost.\n```",
    "sql_response": "```sql\nSELECT \n    Supplier, \n    AVG(Unit_Cost_Rupees) AS Average_Unit_Cost\nFROM \n    inventory_logs\nGROUP BY \n    Supplier\nORDER BY \n    Average_Unit_Cost ASC;\n\n-- To get the supplier with the lowest average cost\nSELECT \n    Supplier, \n    AVG(Unit_Cost_Rupees) AS Average_Unit_Cost\nFROM \n    inventory_logs\nGROUP BY \n    Supplier\nORDER BY \n    Average_Unit_Cost ASC\nLIMIT 1;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Supplier\", \"Unit_Cost_Rupees\", \"Material_Code\", \"Received_Kg\"]\n}\n```",
    "methodology_score": 68.75,
    "sql_score": 80.6941309255079,
    "table_column_score": 80.0,
    "response_quality_score": 97.53065800666809,
    "overall_score": 78.62101162459459,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 11,
      "expected_keyword_count": 16,
      "response_keyword_count": 89
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 50.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 54.627539503386004,
      "overall_score": 80.6941309255079,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "unit_cost_rupees",
        "supplier",
        "avgunitcost"
      ],
      "generated_columns": [
        "average_unit_cost",
        "unit_cost_rupees",
        "supplier"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 80.0,
      "table_score": 100.0,
      "column_score": 66.66666666666666,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 50.0,
      "column_recall": 100.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "unit_cost_rupees",
        "supplier"
      ],
      "columns_missing": [],
      "columns_extra": [
        "material_code",
        "received_kg"
      ],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "unit_cost_rupees",
        "supplier"
      ],
      "generated_columns": [
        "material_code",
        "unit_cost_rupees",
        "supplier",
        "received_kg"
      ]
    },
    "total_latency_ms": 2469.341993331909,
    "total_tokens": 1687,
    "errors": [],
    "timestamp": "2025-11-30T13:03:45.388897"
  },
  {
    "question_id": "Complex_1",
    "question_text": "What is the average downtime (in minutes) per shift for each machine in the last quarter?",
    "category": "Complex",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data table**: The **production_logs** table contains the necessary information about downtime minutes, shifts, and machines. This table will be the primary source for answering the question.\n\n2. **Filter the data for the last quarter**: Apply a filter to the **Date** column in the **production_logs** table to only include dates within the last quarter. This can be achieved by using a date function to extract the quarter from the date and selecting the most recent quarter.\n\n3. **Group the data by machine and shift**: Group the filtered data by the **Line_Machine** and **Shift** columns. This grouping will allow for the calculation of average downtime per shift for each machine.\n\n4. **Calculate the total downtime per group**: For each group (machine and shift combination), calculate the total downtime by summing up the **Downtime_Minutes**.\n\n5. **Calculate the total number of shifts per group**: For each group, count the number of rows (which represents the number of shifts) to use as a divisor for calculating the average downtime per shift.\n\n6. **Calculate the average downtime per shift for each machine**: Divide the total downtime for each group by the total number of shifts in that group to find the average downtime per shift for each machine.\n\n7. **Round the average downtime to the nearest whole number or desired decimal places**: Depending on the precision required, round the calculated average downtime per shift for each machine to the nearest whole number or to a specific number of decimal places.\n\n8. **Optional: Consider additional factors**: If necessary, consider incorporating data from other tables (like **maintenance_logs** for maintenance-related downtime) to provide a more comprehensive understanding of downtime causes. However, for the specific question asked, **production_logs** should suffice.\n\n9. **Present the results**: Organize the calculated average downtime per shift for each machine in a clear and readable format, such as a table or chart, to facilitate easy understanding and further analysis.\n```",
    "sql_response": "```sql\nSELECT \n    Line_Machine,\n    Shift,\n    AVG(Downtime_Minutes) AS Average_Downtime_Minutes\nFROM \n    production_logs\nWHERE \n    Date >= DATEADD(quarter, -1, GETDATE())\nGROUP BY \n    Line_Machine,\n    Shift\nORDER BY \n    Line_Machine,\n    Shift;\n```\nThis query will return the average downtime in minutes per shift for each machine in the last quarter. The `DATEADD` function is used to calculate the date one quarter ago from the current date. The `GETDATE` function returns the current date. The results are grouped by `Line_Machine` and `Shift`, and ordered by `Line_Machine` and `Shift` for easier interpretation. \n\nPlease note that the exact SQL syntax may vary depending on the specific database management system being used. The above query is written in SQL Server syntax. \n\nIf you are using a different database management system, you may need to modify the query accordingly. For example, in MySQL, you would use the `NOW()` function instead of `GETDATE()`, and the `INTERVAL` keyword to subtract a quarter from the current date. \n\nHere is an example of how the query would look in MySQL:\n```sql\nSELECT \n    Line_Machine,\n    Shift,\n    AVG(Downtime_Minutes) AS Average_Downtime_Minutes\nFROM \n    production_logs\nWHERE \n    Date >= NOW() - INTERVAL 1 QUARTER\nGROUP BY \n    Line_Machine,\n    Shift\nORDER BY \n    Line_Machine,\n    Shift;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"production_logs\"],\n  \"columns\": [\"Date\", \"Shift\", \"Line_Machine\", \"Downtime_Minutes\"]\n}\n```",
    "methodology_score": 61.53846153846154,
    "sql_score": 82.47619047619048,
    "table_column_score": 100.0,
    "response_quality_score": 97.27899599075317,
    "overall_score": 82.05610472728044,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 8,
      "expected_keyword_count": 13,
      "response_keyword_count": 128
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 60.0,
      "aggregate_match": 100.0,
      "clause_match": 80.0,
      "fuzzy_score": 69.84126984126983,
      "overall_score": 82.47619047619048,
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "pl",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ]
    },
    "total_latency_ms": 2721.004009246826,
    "total_tokens": 1904,
    "errors": [],
    "timestamp": "2025-11-30T13:03:54.388409"
  },
  {
    "question_id": "Easy_12",
    "question_text": "How many breakdown maintenance events occurred in the last quarter?",
    "category": "Easy",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. Identify the relevant data table for maintenance events, which is the **maintenance_logs** table.\n2. Filter the data to only include records where the **Maintenance_Type** is 'Breakdown' to focus on breakdown maintenance events.\n3. Apply a date filter to the **Maintenance_Date** column to only include records from the last quarter. This involves calculating the date range for the last quarter based on the current date.\n4. Count the number of rows in the filtered data to determine the total number of breakdown maintenance events that occurred in the last quarter.\n5. Since the question only requires the count of breakdown events and does not involve data from other tables, there is no need for joining or combining data from multiple tables.\n6. To ensure accuracy, verify that the date range calculation for the last quarter is correct and that the **Maintenance_Type** filter is properly applied to only include 'Breakdown' events.\n7. Finally, execute the query or calculation to obtain the final count of breakdown maintenance events in the last quarter.\n```",
    "sql_response": "```sql\nSELECT COUNT(*) \nFROM maintenance_logs \nWHERE Maintenance_Type = 'Breakdown' \nAND Maintenance_Date >= DATEADD(quarter, -1, GETDATE());\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Maintenance_Date\", \"Maintenance_Type\", \"Breakdown_Date\"]\n}\n```\n\nTo answer the question, we need to analyze the `maintenance_logs` table, specifically focusing on the `Maintenance_Date` to filter events within the last quarter, `Maintenance_Type` to identify breakdown events, and `Breakdown_Date` to confirm the breakdown occurrence.",
    "methodology_score": 60.0,
    "sql_score": 95.81967213114754,
    "table_column_score": 88.0,
    "response_quality_score": 98.06561613082886,
    "overall_score": 83.34344685898452,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 6,
      "expected_keyword_count": 10,
      "response_keyword_count": 69
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 72.1311475409836,
      "overall_score": 95.81967213114754,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "maintenance_type"
      ],
      "generated_columns": [
        "maintenance_type"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 88.0,
      "table_score": 100.0,
      "column_score": 80.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 66.66666666666666,
      "column_recall": 100.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "breakdown_date",
        "maintenance_type"
      ],
      "columns_missing": [],
      "columns_extra": [
        "maintenance_date"
      ],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "breakdown_date",
        "maintenance_type"
      ],
      "generated_columns": [
        "breakdown_date",
        "maintenance_date",
        "maintenance_type"
      ]
    },
    "total_latency_ms": 1934.3838691711426,
    "total_tokens": 1440,
    "errors": [],
    "timestamp": "2025-11-30T13:04:02.795822"
  },
  {
    "question_id": "Complex_7",
    "question_text": "What are the top 3 defect types that lead to rework for Widget-B, and what is their total rework count?",
    "category": "Complex",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data table**: To answer this question, we need to use the **quality_control** table, as it contains information about inspected quantities, passed quantities, failed quantities, defect types, and rework counts.\n\n2. **Filter the data for Widget-B**: Apply a filter to the **quality_control** table to only include rows where the **Product** column is 'Widget-B'. This will ensure that we are only analyzing data for the specified product.\n\n3. **Group the data by defect type**: Group the filtered data by the **Defect_Type** column. This will allow us to calculate the total rework count for each defect type.\n\n4. **Calculate the total rework count for each defect type**: For each group of defect types, calculate the sum of the **Rework_Count** column. This will give us the total rework count for each defect type.\n\n5. **Rank the defect types by total rework count**: Sort the grouped data in descending order based on the total rework count for each defect type. This will allow us to identify the top 3 defect types that lead to rework.\n\n6. **Select the top 3 defect types**: Select the top 3 rows from the sorted data, which will correspond to the top 3 defect types that lead to rework for Widget-B.\n\n7. **Report the results**: Report the top 3 defect types and their corresponding total rework counts. This will provide the answer to the question.\n```",
    "sql_response": "```sql\nSELECT \n    Defect_Type, \n    SUM(Rework_Count) AS Total_Rework_Count\nFROM \n    quality_control\nWHERE \n    Product = 'Widget-B'\nGROUP BY \n    Defect_Type\nORDER BY \n    Total_Rework_Count DESC\nLIMIT 3;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Defect_Type\", \"Rework_Count\", \"Product\"]\n}\n```",
    "methodology_score": 76.0,
    "sql_score": 99.95522388059702,
    "table_column_score": 100.0,
    "response_quality_score": 98.43485188484192,
    "overall_score": 92.62781354669315,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 19,
      "expected_keyword_count": 25,
      "response_keyword_count": 84
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 99.70149253731343,
      "overall_score": 99.95522388059702,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "defect_type",
        "total_rework_count",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "defect_type",
        "total_rework_count",
        "rework_count"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "defect_type",
        "rework_count"
      ]
    },
    "total_latency_ms": 1565.148115158081,
    "total_tokens": 1554,
    "errors": [],
    "timestamp": "2025-11-30T13:04:10.783894"
  },
  {
    "question_id": "Medium_21",
    "question_text": "What is the average downtime per shift for Machine-M1 in October 2026?",
    "category": "Medium",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data table**: To find the average downtime per shift for Machine-M1 in October 2026, we need to use the **production_logs** table, as it contains information about production, including downtime minutes, which is crucial for calculating average downtime.\n\n2. **Filter the data by machine and date**: Apply filters to the **production_logs** table to only include rows where the Line_Machine is 'M1' and the Date falls within October 2026. This will ensure we're only analyzing data for the specified machine and time period.\n\n3. **Extract relevant columns**: From the filtered data, extract the columns 'Date', 'Shift', and 'Downtime_Minutes'. These columns are necessary for calculating the average downtime per shift.\n\n4. **Group data by shift**: Group the extracted data by the 'Shift' column. This grouping is essential because we need to calculate the average downtime for each shift separately.\n\n5. **Calculate total downtime per shift**: For each group (shift), calculate the total downtime by summing up the 'Downtime_Minutes'.\n\n6. **Count the number of shifts**: Also, count the number of rows (representing individual production logs) in each shift group. This count will be used to calculate the average.\n\n7. **Calculate average downtime per shift**: Divide the total downtime for each shift by the number of shifts to find the average downtime per shift for Machine-M1 in October 2026.\n\n8. **Round the result (optional)**: Depending on the precision required, round the calculated average downtime to the nearest whole number or to one decimal place for better readability.\n\nSince the question specifically asks for the average downtime per shift and all necessary information is available in the **production_logs** table, there's no need to combine data from multiple tables for this particular query.\n```",
    "sql_response": "```sql\nSELECT \n    Shift, \n    AVG(Downtime_Minutes) AS Average_Downtime\nFROM \n    production_logs\nWHERE \n    Line_Machine LIKE '%Machine-M1' \n    AND Date BETWEEN '2026-10-01' AND '2026-10-31'\nGROUP BY \n    Shift;\n```\nThis query will return the average downtime per shift for Machine-M1 in October 2026. The `Line_Machine LIKE '%Machine-M1'` condition filters the production logs to only include records for Machine-M1, and the `Date BETWEEN '2026-10-01' AND '2026-10-31'` condition filters the records to only include those from October 2026. The `AVG(Downtime_Minutes)` function calculates the average downtime for each shift, and the results are grouped by shift.",
    "table_selection_response": "```json\n{\n  \"tables\": [\"production_logs\"],\n  \"columns\": [\"Date\", \"Shift\", \"Line_Machine\", \"Downtime_Minutes\"]\n}\n```",
    "methodology_score": 50.0,
    "sql_score": 96.80952380952381,
    "table_column_score": 100.0,
    "response_quality_score": 97.9499990940094,
    "overall_score": 83.67833324273427,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 7,
      "expected_keyword_count": 14,
      "response_keyword_count": 120
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 78.73015873015873,
      "overall_score": 96.80952380952381,
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "downtime_minutes",
        "shift"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ]
    },
    "total_latency_ms": 2050.0009059906006,
    "total_tokens": 1722,
    "errors": [],
    "timestamp": "2025-11-30T13:04:19.098901"
  },
  {
    "question_id": "Easy_5",
    "question_text": "How many breakdown maintenances were performed on Machine-M2 in Q3 2026?",
    "category": "Easy",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. Identify the relevant data table for the question, which is the **maintenance_logs** table, as it contains information about maintenance activities, including breakdowns and the machines involved.\n\n2. Apply a filter to the **maintenance_logs** table to only include rows where the 'Machine' column equals 'M2', as we are specifically interested in maintenance activities for Machine-M2.\n\n3. Apply another filter to the **maintenance_logs** table to only include rows where the 'Maintenance_Date' falls within the third quarter (Q3) of 2026. This means the date should be between July 1, 2026, and September 30, 2026.\n\n4. Apply a filter to the **maintenance_logs** table to only include rows where the 'Maintenance_Type' indicates a breakdown maintenance. However, since the exact terminology for breakdown maintenance in the **maintenance_logs** table is not specified, we will assume that breakdown maintenances are those where 'Breakdown_Date' is not null, indicating that a breakdown occurred.\n\n5. Count the number of rows in the filtered **maintenance_logs** table to determine how many breakdown maintenances were performed on Machine-M2 in Q3 2026. This can be achieved by using a COUNT() function on the filtered data.\n\n6. Since the question only requires the count of breakdown maintenances and does not necessitate combining data from multiple tables, no join operations are needed, and the analysis can be completed using the **maintenance_logs** table alone.\n```",
    "sql_response": "```sql\nSELECT COUNT(*) \nFROM maintenance_logs \nWHERE Machine = 'Machine-M2' \n  AND Maintenance_Type = 'Breakdown' \n  AND EXTRACT(QUARTER FROM Maintenance_Date) = 3 \n  AND EXTRACT(YEAR FROM Maintenance_Date) = 2026;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Maintenance_Date\", \"Machine\", \"Maintenance_Type\", \"Breakdown_Date\"]\n}\n```",
    "methodology_score": 53.333333333333336,
    "sql_score": 96.640625,
    "table_column_score": 91.42857142857143,
    "response_quality_score": 98.19895052909851,
    "overall_score": 82.50125666005272,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 8,
      "expected_keyword_count": 15,
      "response_keyword_count": 92
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 77.60416666666666,
      "overall_score": 96.640625,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "machine"
      ],
      "generated_columns": [
        "machine"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 91.42857142857143,
      "table_score": 100.0,
      "column_score": 85.71428571428571,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 75.0,
      "column_recall": 100.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "maintenance_date",
        "maintenance_type",
        "machine"
      ],
      "columns_missing": [],
      "columns_extra": [
        "breakdown_date"
      ],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "maintenance_date",
        "maintenance_type",
        "machine"
      ],
      "generated_columns": [
        "breakdown_date",
        "maintenance_date",
        "maintenance_type",
        "machine"
      ]
    },
    "total_latency_ms": 1801.0494709014893,
    "total_tokens": 1538,
    "errors": [],
    "timestamp": "2025-11-30T13:04:27.335404"
  },
  {
    "question_id": "Complex_33",
    "question_text": "What are the top 3 most frequently replaced parts across all machines, and what is the total cost associated with replacing these parts?",
    "category": "Complex",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data table**: The **maintenance_logs** table contains information about machine maintenance, including the parts replaced and the associated costs. This table will be used to answer the question.\n\n2. **Filter the data to only include records with parts replaced**: Apply a filter to the **maintenance_logs** table to only include rows where the **Parts_Replaced** column is not null or empty, as this indicates that a part was replaced.\n\n3. **Extract the unique parts replaced**: From the filtered data, extract the unique values in the **Parts_Replaced** column to get a list of all the different parts that have been replaced.\n\n4. **Count the frequency of each part replacement**: Use the **Parts_Replaced** column to count the number of times each part has been replaced across all machines. This can be done using a group by operation on the **Parts_Replaced** column and calculating the count of each group.\n\n5. **Rank the parts by replacement frequency**: Sort the list of parts in descending order based on their replacement frequency, so that the most frequently replaced parts are at the top.\n\n6. **Select the top 3 most frequently replaced parts**: From the sorted list, select the top 3 parts with the highest replacement frequency.\n\n7. **Calculate the total cost associated with replacing these parts**: For each of the top 3 parts, sum up the **Cost_Rupees** from the **maintenance_logs** table where the **Parts_Replaced** column matches the part. This will give the total cost associated with replacing each of these parts.\n\n8. **Combine the results**: Combine the list of the top 3 most frequently replaced parts with their corresponding total replacement costs to get the final answer.\n```",
    "sql_response": "```sql\nSELECT \n    Parts_Replaced, \n    COUNT(*) AS Replacement_Count, \n    SUM(Cost_Rupees) AS Total_Cost\nFROM \n    maintenance_logs\nWHERE \n    Parts_Replaced IS NOT NULL\nGROUP BY \n    Parts_Replaced\nORDER BY \n    Replacement_Count DESC\nLIMIT 3;\n```\nThis query works as follows:\n\n1. It selects the `Parts_Replaced` column, counts the number of times each part is replaced (`Replacement_Count`), and sums up the total cost associated with replacing each part (`Total_Cost`).\n2. It filters out rows where `Parts_Replaced` is `NULL`, as we are only interested in parts that have been replaced.\n3. It groups the results by `Parts_Replaced`, so that we can count and sum the replacements for each part separately.\n4. It orders the results by `Replacement_Count` in descending order, so that the most frequently replaced parts appear at the top.\n5. Finally, it limits the results to the top 3 most frequently replaced parts.",
    "table_selection_response": "```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Parts_Replaced\", \"Cost_Rupees\"]\n}\n```",
    "methodology_score": 54.54545454545454,
    "sql_score": 83.54838709677419,
    "table_column_score": 100.0,
    "response_quality_score": 97.12293934822083,
    "overall_score": 80.3178657823294,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 12,
      "expected_keyword_count": 22,
      "response_keyword_count": 105
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 50.0,
      "aggregate_match": 100.0,
      "clause_match": 83.33333333333334,
      "fuzzy_score": 90.32258064516128,
      "overall_score": 83.54838709677419,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "replacementcount",
        "parts_replaced"
      ],
      "generated_columns": [
        "cost_rupees",
        "parts_replaced",
        "replacement_count"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "cost_rupees",
        "parts_replaced"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "parts_replaced"
      ],
      "generated_columns": [
        "cost_rupees",
        "parts_replaced"
      ]
    },
    "total_latency_ms": 2877.060651779175,
    "total_tokens": 1756,
    "errors": [],
    "timestamp": "2025-11-30T13:04:36.586163"
  },
  {
    "question_id": "Complex_23",
    "question_text": "What is the correlation between Downtime_Minutes and Failed_Qty on a daily basis?",
    "category": "Complex",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant tables**: To answer the question, we need to use the **production_logs** table for Downtime_Minutes and the **quality_control** table for Failed_Qty. We will also need a common column to join these tables, which could be the Date (from production_logs) and Inspection_Date (from quality_control), assuming Inspection_Date represents the date of inspection which could be the same as the production date.\n\n2. **Filter and aggregate production_logs**: Filter the **production_logs** table to only include the columns Date, Downtime_Minutes, and Product. Then, aggregate this data by Date to calculate the total Downtime_Minutes per day. This will give us a daily view of downtime.\n\n3. **Filter and aggregate quality_control**: Filter the **quality_control** table to only include the columns Inspection_Date, Failed_Qty, and Product. Then, aggregate this data by Inspection_Date to calculate the total Failed_Qty per day. This will give us a daily view of failed quantities.\n\n4. **Join the two aggregated datasets**: Join the aggregated **production_logs** and **quality_control** tables on the Date (production_logs) and Inspection_Date (quality_control). This will allow us to analyze the relationship between Downtime_Minutes and Failed_Qty on a daily basis.\n\n5. **Calculate the correlation**: Using the joined dataset, calculate the correlation coefficient (such as Pearson's r) between Downtime_Minutes and Failed_Qty. This statistical measure will help us understand the strength and direction of the linear relationship between these two variables.\n\n6. **Visualize the correlation (optional)**: To better understand the relationship, consider visualizing the data using a scatter plot where the x-axis represents Downtime_Minutes and the y-axis represents Failed_Qty. Each point on the plot would represent a day's data.\n\n7. **Interpret the results**: Interpret the correlation coefficient in the context of manufacturing operations. A positive correlation would suggest that as Downtime_Minutes increase, Failed_Qty also tends to increase, which could indicate that downtime leads to rushed or less precise work, resulting in more defects. A negative correlation would suggest the opposite, and a correlation close to zero would indicate no linear relationship between the two variables.\n```",
    "sql_response": "```sql\nWITH daily_downtime AS (\n  SELECT \n    p.Date, \n    SUM(p.Downtime_Minutes) AS total_downtime_minutes\n  FROM \n    production_logs p\n  GROUP BY \n    p.Date\n),\ndaily_quality_control AS (\n  SELECT \n    q.Inspection_Date AS Date, \n    SUM(q.Failed_Qty) AS total_failed_qty\n  FROM \n    quality_control q\n  GROUP BY \n    q.Inspection_Date\n)\nSELECT \n  CORR(d.total_downtime_minutes, d.total_failed_qty) AS correlation\nFROM \n  daily_downtime d\n  JOIN daily_quality_control q ON d.Date = q.Date;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"production_logs\", \"quality_control\"],\n  \"columns\": [\"production_logs.Date\", \"production_logs.Downtime_Minutes\", \"quality_control.Inspection_Date\", \"quality_control.Failed_Qty\"]\n}\n```",
    "methodology_score": 56.25,
    "sql_score": 77.50031005829095,
    "table_column_score": 40.0,
    "response_quality_score": 97.30696439743042,
    "overall_score": 63.73080496014487,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 9,
      "expected_keyword_count": 16,
      "response_keyword_count": 129
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 27.27272727272727,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 71.21418826739428,
      "overall_score": 77.50031005829095,
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "pl",
        "corrtotaldowntime",
        "date",
        "downtime_minutes",
        "failed_qty",
        "totalfailures"
      ],
      "generated_columns": [
        "p",
        "inspection_date",
        "date",
        "downtime_minutes",
        "total_failed_qty",
        "q",
        "total_downtime_minutes",
        "failed_qty"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 40.0,
      "table_score": 100.0,
      "column_score": 0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 0.0,
      "column_recall": 0.0,
      "tables_correct": [
        "production_logs",
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [],
      "columns_missing": [
        "inspection_date",
        "date",
        "downtime_minutes",
        "failed_qty"
      ],
      "columns_extra": [
        "production_logs.downtime_minutes",
        "production_logs.date",
        "quality_control.failed_qty",
        "quality_control.inspection_date"
      ],
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "inspection_date",
        "date",
        "downtime_minutes",
        "failed_qty"
      ],
      "generated_columns": [
        "production_logs.downtime_minutes",
        "production_logs.date",
        "quality_control.failed_qty",
        "quality_control.inspection_date"
      ]
    },
    "total_latency_ms": 2693.03560256958,
    "total_tokens": 1797,
    "errors": [],
    "timestamp": "2025-11-30T13:04:45.563757"
  },
  {
    "question_id": "Easy_28",
    "question_text": "What is the total cost of parts replaced for Machine-M3?",
    "category": "Easy",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. Identify the relevant data table: The **maintenance_logs** table contains information about machine maintenance, including the cost of parts replaced. This table will be used to answer the question.\n2. Filter the data for Machine-M3: Apply a filter to the **maintenance_logs** table to only include rows where the **Machine** column is 'M3'.\n3. Select the relevant column: From the filtered data, select the **Cost_Rupees** column, which represents the cost of parts replaced.\n4. Aggregate the costs: Calculate the total cost of parts replaced for Machine-M3 by summing up the values in the **Cost_Rupees** column.\n5. Perform the calculation: Use a summation function (e.g., SUM) to add up the costs: `SUM(Cost_Rupees)` for the filtered data.\n6. Execute the query: Run the query on the **maintenance_logs** table to obtain the total cost of parts replaced for Machine-M3.\n```",
    "sql_response": "```sql\nSELECT SUM(Cost_Rupees) AS Total_Cost\nFROM maintenance_logs\nWHERE Machine = 'Machine-M3';\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Machine\", \"Parts_Replaced\", \"Cost_Rupees\"]\n}\n```",
    "methodology_score": 42.857142857142854,
    "sql_score": 88.58490566037736,
    "table_column_score": 88.0,
    "response_quality_score": 98.38732314109802,
    "overall_score": 75.70059215238474,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 3,
      "expected_keyword_count": 7,
      "response_keyword_count": 64
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 90.56603773584906,
      "overall_score": 88.58490566037736,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "machine"
      ],
      "generated_columns": [
        "cost_rupees",
        "machine"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 88.0,
      "table_score": 100.0,
      "column_score": 80.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 66.66666666666666,
      "column_recall": 100.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "cost_rupees",
        "machine"
      ],
      "columns_missing": [],
      "columns_extra": [
        "parts_replaced"
      ],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "machine"
      ],
      "generated_columns": [
        "cost_rupees",
        "parts_replaced",
        "machine"
      ]
    },
    "total_latency_ms": 1612.6768589019775,
    "total_tokens": 1379,
    "errors": [],
    "timestamp": "2025-11-30T13:04:53.487396"
  },
  {
    "question_id": "Easy_30",
    "question_text": "How much total waste was recorded in 2025?",
    "category": "Easy",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. Identify the relevant table for calculating total waste, which is the **inventory_logs** table, as it contains the column 'Wastage_Kg' that directly relates to waste.\n2. Apply a filter to the **inventory_logs** table to only include data from the year 2025. This can be done by filtering the 'Date' column to include dates within the range of '2025-01-01' to '2025-12-31'.\n3. Calculate the total waste by summing up the values in the 'Wastage_Kg' column for all the rows that passed the filter in step 2. This aggregation will give the total waste recorded in 2025.\n4. Since the question only requires data from one table, there is no need to combine data from multiple tables for this specific query.\n5. Execute the query to obtain the total waste in kilograms for the year 2025.\n```",
    "sql_response": "```sql\nSELECT SUM(Wastage_Kg) AS Total_Waste\nFROM inventory_logs\nWHERE DATE LIKE '2025%';\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Wastage_Kg\", \"Date\"]\n}\n```\n\nTo calculate the total waste recorded in 2025, we need to access the `inventory_logs` table, specifically the `Wastage_Kg` column to get the waste quantities and the `Date` column to filter the data for the year 2025.",
    "methodology_score": 42.857142857142854,
    "sql_score": 85.77844311377245,
    "table_column_score": 100.0,
    "response_quality_score": 98.13824462890625,
    "overall_score": 77.69342240985384,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 3,
      "expected_keyword_count": 7,
      "response_keyword_count": 60
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 71.8562874251497,
      "overall_score": 85.77844311377245,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg"
      ],
      "generated_columns": [
        "wastage_kg"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "wastage_kg",
        "date"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "date"
      ],
      "generated_columns": [
        "wastage_kg",
        "date"
      ]
    },
    "total_latency_ms": 1861.75537109375,
    "total_tokens": 1414,
    "errors": [],
    "timestamp": "2025-11-30T13:05:01.794471"
  },
  {
    "question_id": "Medium_31",
    "question_text": "What is the average downtime (in minutes) per shift for Machine-M1 on Line-1 during the last quarter?",
    "category": "Medium",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data table**: The question is about downtime per shift for a specific machine (Machine-M1) on a specific line (Line-1), so we will use the **production_logs** table, as it contains information about production, including downtime minutes, shift, and line-machine details.\n\n2. **Filter the data for the last quarter**: We need to filter the **production_logs** table to only include data from the last quarter. This involves selecting data where the 'Date' falls within the last quarter. The exact dates will depend on the current date, but for example, if the current date is November 15, 2023, the last quarter would be August 1, 2023, to October 31, 2023.\n\n3. **Filter the data for Machine-M1 on Line-1**: From the filtered data, we need to select only the rows where 'Line_Machine' equals 'Line-1_Machine-M1' to focus on the specific machine and line in question.\n\n4. **Calculate total downtime per shift**: For each shift in the filtered data, we need to sum up the 'Downtime_Minutes'. Since the question asks for average downtime per shift, we first need to calculate the total downtime for each shift.\n\n5. **Count the number of shifts**: To calculate the average, we also need to know how many shifts are included in our data. This involves counting the distinct shifts in the filtered data.\n\n6. **Calculate the average downtime per shift**: Divide the total downtime minutes by the number of shifts to find the average downtime per shift for Machine-M1 on Line-1 during the last quarter.\n\n7. **Considerations for data integrity**: Ensure that the data is clean and there are no missing values for 'Downtime_Minutes' or 'Shift' in the relevant rows. If there are, decide on a strategy for handling them (e.g., imputation, exclusion) based on the context and available information.\n\n8. **Final calculation**: Perform the final calculation with the cleaned and filtered data to obtain the average downtime per shift.\n```",
    "sql_response": "```sql\nSELECT \n    Shift, \n    AVG(Downtime_Minutes) AS Average_Downtime_Minutes\nFROM \n    production_logs\nWHERE \n    Line_Machine LIKE 'Line-1/Machine-M1'\n    AND Date >= DATEADD(quarter, -1, GETDATE())\nGROUP BY \n    Shift\nORDER BY \n    Shift;\n```\nThis query will return the average downtime per shift for Machine-M1 on Line-1 during the last quarter. The `DATEADD` function is used to get the date a quarter ago from the current date. The `LIKE` operator is used to filter the `Line_Machine` column for 'Line-1/Machine-M1'. The results are grouped by shift and ordered by shift for better readability. \n\nPlease note that the exact SQL syntax may vary depending on the SQL dialect being used. The above query is written in T-SQL, which is used by Microsoft SQL Server. If you are using a different database management system, you may need to modify the query accordingly. \n\nFor example, in MySQL, you would use the `NOW()` function to get the current date and the `INTERVAL` keyword to subtract a quarter:\n```sql\nSELECT \n    Shift, \n    AVG(Downtime_Minutes) AS Average_Downtime_Minutes\nFROM \n    production_logs\nWHERE \n    Line_Machine LIKE 'Line-1/Machine-M1'\n    AND Date >= NOW() - INTERVAL 3 MONTH\nGROUP BY \n    Shift\nORDER BY \n    Shift;\n```\nIn PostgreSQL, you would use the `NOW()` function and the `INTERVAL` keyword:\n```sql\nSELECT \n    Shift, \n    AVG(Downtime_Minutes) AS Average_Downtime_Minutes\nFROM \n    production_logs\nWHERE \n    Line_Machine LIKE 'Line-1/Machine-M1'\n    AND Date >= NOW() - INTERVAL '3 month'\nGROUP BY \n    Shift\nORDER BY \n    Shift;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"production_logs\"],\n  \"columns\": [\"Date\", \"Shift\", \"Line_Machine\", \"Downtime_Minutes\"]\n}\n```",
    "methodology_score": 46.666666666666664,
    "sql_score": 85.07162534435261,
    "table_column_score": 100.0,
    "response_quality_score": 96.30188179016113,
    "overall_score": 78.40525704953953,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 7,
      "expected_keyword_count": 15,
      "response_keyword_count": 130
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 66.66666666666666,
      "aggregate_match": 100.0,
      "clause_match": 80.0,
      "fuzzy_score": 76.03305785123968,
      "overall_score": 85.07162534435261,
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "downtime_minutes",
        "shift"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ]
    },
    "total_latency_ms": 3698.118209838867,
    "total_tokens": 2029,
    "errors": [],
    "timestamp": "2025-11-30T13:05:11.767678"
  },
  {
    "question_id": "Complex_9",
    "question_text": "Which supplier provides the material with the lowest unit cost, considering only materials consumed after 2026-01-01?",
    "category": "Complex",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data table**: The question is about suppliers and unit costs of materials, which can be found in the **inventory_logs** table.\n\n2. **Apply the date filter**: Filter the **inventory_logs** table to only include records where the Date is after 2026-01-01. This will ensure that we are considering only materials consumed after the specified date.\n\n3. **Group by supplier and material**: Group the filtered data by the Supplier and Material_Code columns. This will allow us to calculate the unit cost for each supplier-material combination.\n\n4. **Calculate the total consumption and total cost**: For each group, calculate the total consumption in kilograms (sum of Consumption_Kg) and the total cost in rupees (sum of Consumption_Kg * Unit_Cost_Rupees).\n\n5. **Calculate the average unit cost**: For each group, calculate the average unit cost by dividing the total cost by the total consumption.\n\n6. **Find the supplier with the lowest unit cost**: Sort the groups by the average unit cost in ascending order and select the supplier with the lowest average unit cost.\n\n7. **Consider multiple materials from the same supplier**: If a supplier provides multiple materials, the calculation in step 5 should be performed for each material separately. Then, find the material with the lowest unit cost for each supplier.\n\n8. **Identify the supplier with the lowest unit cost across all materials**: Compare the lowest unit costs from each supplier (found in step 7) to determine which supplier provides the material with the lowest unit cost overall.\n```",
    "sql_response": "```sql\nSELECT Supplier, Material_Code, Material_Name, Unit_Cost_Rupees\nFROM inventory_logs\nWHERE Date > '2026-01-01' AND Consumption_Kg > 0\nORDER BY Unit_Cost_Rupees ASC\nLIMIT 1;\n```\nThis query will return the supplier, material code, material name, and unit cost of the material with the lowest unit cost, considering only materials consumed after 2026-01-01. The `LIMIT 1` clause ensures that only one row is returned, which corresponds to the material with the lowest unit cost. \n\nHowever, if there are multiple materials with the same lowest unit cost, this query will only return one of them. If you want to return all materials with the lowest unit cost, you can use a subquery to first find the minimum unit cost and then select all materials with that cost:\n\n```sql\nSELECT Supplier, Material_Code, Material_Name, Unit_Cost_Rupees\nFROM inventory_logs\nWHERE Date > '2026-01-01' AND Consumption_Kg > 0\nAND Unit_Cost_Rupees = (\n  SELECT MIN(Unit_Cost_Rupees)\n  FROM inventory_logs\n  WHERE Date > '2026-01-01' AND Consumption_Kg > 0\n);\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Supplier\", \"Unit_Cost_Rupees\", \"Date\"]\n}\n```",
    "methodology_score": 68.42105263157895,
    "sql_score": 64.69298245614034,
    "table_column_score": 91.42857142857143,
    "response_quality_score": 97.38085746765137,
    "overall_score": 75.76408825303079,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 13,
      "expected_keyword_count": 19,
      "response_keyword_count": 90
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 66.66666666666666,
      "aggregate_match": 0.0,
      "clause_match": 83.33333333333334,
      "fuzzy_score": 70.17543859649122,
      "overall_score": 64.69298245614034,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_name",
        "min",
        "date",
        "unit_cost_rupees",
        "supplier"
      ],
      "generated_columns": [
        "material_code",
        "material_name",
        "date",
        "unit_cost_rupees",
        "supplier"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 91.42857142857143,
      "table_score": 100.0,
      "column_score": 85.71428571428571,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 75.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "unit_cost_rupees",
        "supplier",
        "date"
      ],
      "columns_missing": [
        "material_name"
      ],
      "columns_extra": [],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_name",
        "supplier",
        "date",
        "unit_cost_rupees"
      ],
      "generated_columns": [
        "unit_cost_rupees",
        "supplier",
        "date"
      ]
    },
    "total_latency_ms": 2619.142532348633,
    "total_tokens": 1770,
    "errors": [],
    "timestamp": "2025-11-30T13:05:20.919335"
  },
  {
    "question_id": "Complex_19",
    "question_text": "How does the 'Downtime_Minutes' in production relate to the 'Downtime_Hours' in maintenance logs when the 'Issue_Description' involves 'Motor overheating and failure to start'?",
    "category": "Complex",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Filter maintenance logs**: Use the **maintenance_logs** table and apply a filter to the 'Issue_Description' column to only include rows where the description contains 'Motor overheating and failure to start'. This will help identify the specific maintenance events related to motor overheating.\n\n2. **Extract relevant maintenance data**: From the filtered **maintenance_logs** table, extract the 'Machine' and 'Downtime_Hours' columns for the maintenance events related to motor overheating. This data will be used to understand the maintenance downtime.\n\n3. **Convert Downtime_Hours to Downtime_Minutes**: Since the production logs are in minutes, convert the 'Downtime_Hours' from the maintenance logs to minutes by multiplying by 60. This ensures consistency in units for comparison.\n\n4. **Filter production logs**: Use the **production_logs** table and apply a filter to the 'Line_Machine' column to only include rows where the machine matches the ones extracted from the maintenance logs with motor overheating issues. Also, filter the data to include only the dates around the maintenance events (e.g., a day before and after the maintenance date) to capture the production impact.\n\n5. **Extract relevant production data**: From the filtered **production_logs** table, extract the 'Downtime_Minutes' column. This data represents the production downtime.\n\n6. **Combine maintenance and production data**: Merge the extracted maintenance data (with 'Machine' and 'Downtime_Minutes' from step 3) with the production data (with 'Line_Machine' and 'Downtime_Minutes' from step 5) based on the 'Machine'/'Line_Machine' column. This will allow for a direct comparison of downtime minutes in both contexts.\n\n7. **Calculate correlation or comparison metric**: Perform a calculation to understand the relationship between 'Downtime_Minutes' in production logs and 'Downtime_Hours' (converted to minutes) in maintenance logs. This could involve calculating the correlation coefficient, mean difference, or another suitable metric that describes how closely the two types of downtime are related.\n\n8. **Visualize or summarize findings**: Finally, visualize the relationship using a scatter plot or summarize the findings with statistical measures (like mean, median, and standard deviation of the differences or ratios between the two types of downtime). This step helps in interpreting the results and drawing conclusions about how motor overheating and failure to start affect production downtime.\n```",
    "sql_response": "```sql\nSELECT \n    pl.Date, \n    pl.Line_Machine, \n    pl.Downtime_Minutes, \n    ml.Maintenance_Date, \n    ml.Machine, \n    ml.Downtime_Hours, \n    ml.Issue_Description\nFROM \n    production_logs pl\nINNER JOIN \n    maintenance_logs ml ON pl.Line_Machine LIKE CONCAT('%/', ml.Machine) \n                        AND pl.Date = ml.Maintenance_Date\nWHERE \n    ml.Issue_Description LIKE '%Motor overheating and failure to start%';\n```\nThis query will return the production log entries where the downtime minutes are related to the maintenance log entries where the issue description involves 'Motor overheating and failure to start'. The join is based on the machine name and the date. The `LIKE` operator is used with `CONCAT` to match the machine name in the `production_logs` table with the machine name in the `maintenance_logs` table. The `WHERE` clause filters the results to only include maintenance log entries where the issue description matches the specified condition.",
    "table_selection_response": "```json\n{\n  \"tables\": [\"production_logs\", \"maintenance_logs\"],\n  \"columns\": [\"Downtime_Minutes\", \"Downtime_Hours\", \"Issue_Description\", \"Line_Machine\", \"Machine\", \"Maintenance_Date\", \"Date\"]\n}\n```",
    "methodology_score": 36.36363636363637,
    "sql_score": 62.547309833024116,
    "table_column_score": 80.0,
    "response_quality_score": 97.07779622077942,
    "overall_score": 62.508428972727295,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 8,
      "expected_keyword_count": 22,
      "response_keyword_count": 142
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 28.57142857142857,
      "aggregate_match": 50.0,
      "clause_match": 80.0,
      "fuzzy_score": 56.029684601113175,
      "overall_score": 62.547309833024116,
      "expected_tables": [
        "maintenance_logs",
        "production_logs"
      ],
      "generated_tables": [
        "maintenance_logs",
        "production_logs"
      ],
      "expected_columns": [
        "downtime_hours",
        "downtime_minutes"
      ],
      "generated_columns": [
        "maintenance_date",
        "line_machine",
        "date",
        "downtime_minutes",
        "issue_description",
        "machine",
        "downtime_hours"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 80.0,
      "table_score": 100.0,
      "column_score": 66.66666666666666,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 57.14285714285714,
      "column_recall": 80.0,
      "tables_correct": [
        "maintenance_logs",
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "issue_description",
        "downtime_hours",
        "date",
        "downtime_minutes"
      ],
      "columns_missing": [
        "breakdown_date"
      ],
      "columns_extra": [
        "line_machine",
        "maintenance_date",
        "machine"
      ],
      "expected_tables": [
        "maintenance_logs",
        "production_logs"
      ],
      "generated_tables": [
        "maintenance_logs",
        "production_logs"
      ],
      "expected_columns": [
        "date",
        "downtime_minutes",
        "breakdown_date",
        "issue_description",
        "downtime_hours"
      ],
      "generated_columns": [
        "maintenance_date",
        "line_machine",
        "date",
        "downtime_minutes",
        "issue_description",
        "machine",
        "downtime_hours"
      ]
    },
    "total_latency_ms": 2922.203779220581,
    "total_tokens": 1975,
    "errors": [],
    "timestamp": "2025-11-30T13:05:30.176386"
  },
  {
    "question_id": "Complex_3",
    "question_text": "What is the correlation between downtime minutes and failed quantity per line machine and product?",
    "category": "Complex",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data tables**: To answer the question, we need to use the **production_logs** table for downtime minutes and the **quality_control** table for failed quantity. We also need to consider the **Line_Machine** and **Product** columns from the **production_logs** table and the **Line** and **Product** columns from the **quality_control** table.\n\n2. **Merge the production_logs and quality_control tables**: Since the **production_logs** table does not have a direct relationship with the **quality_control** table, we need to merge them based on the **Date** (or **Inspection_Date**), **Line_Machine** (or **Line**), and **Product** columns. However, since the dates might not match exactly, we can merge the tables based on the **Line_Machine** (or **Line**) and **Product** columns.\n\n3. **Apply filters to the merged table**: Filter the merged table to only include rows where the **Downtime_Minutes** and **Failed_Qty** columns are not null, to ensure we are only analyzing complete data.\n\n4. **Calculate the total downtime minutes and failed quantity per line machine and product**: Group the filtered table by **Line_Machine** and **Product**, and calculate the sum of **Downtime_Minutes** and **Failed_Qty** for each group.\n\n5. **Calculate the correlation between downtime minutes and failed quantity**: Use a correlation coefficient formula (such as Pearson's r) to calculate the correlation between the total downtime minutes and failed quantity for each **Line_Machine** and **Product** combination.\n\n6. **Visualize the correlation results**: To better understand the correlation, visualize the results using a heatmap or a scatter plot, with **Line_Machine** and **Product** on the x and y axes, and the correlation coefficient as the color or size of the points.\n\n7. **Interpret the correlation results**: Analyze the correlation coefficients to determine the strength and direction of the relationship between downtime minutes and failed quantity for each **Line_Machine** and **Product** combination. A positive correlation indicates that as downtime minutes increase, failed quantity also tends to increase, while a negative correlation indicates that as downtime minutes increase, failed quantity tends to decrease.\n\n8. **Refine the analysis by considering other factors**: Consider other factors that might influence the correlation, such as **Maintenance_Type**, **Issue_Description**, or **Operator**, and refine the analysis to account for these factors.\n```",
    "sql_response": "```sql\nWITH production_data AS (\n  SELECT \n    pl.Line_Machine,\n    pl.Product,\n    pl.Downtime_Minutes,\n    qc.Failed_Qty\n  FROM \n    production_logs pl\n  JOIN \n    quality_control qc ON pl.Date = qc.Inspection_Date AND pl.Product = qc.Product\n),\ncorrelation_data AS (\n  SELECT \n    Line_Machine,\n    Product,\n    AVG(Downtime_Minutes) AS avg_downtime,\n    AVG(Failed_Qty) AS avg_failed,\n    CORR(Downtime_Minutes, Failed_Qty) AS correlation\n  FROM \n    production_data\n  GROUP BY \n    Line_Machine, Product\n)\nSELECT \n  Line_Machine,\n  Product,\n  correlation\nFROM \n  correlation_data\nORDER BY \n  correlation DESC;\n```\nThis query first joins the `production_logs` and `quality_control` tables based on the date and product. It then calculates the average downtime minutes and failed quantity for each line machine and product. The correlation between downtime minutes and failed quantity is calculated using the `CORR` function. Finally, the results are sorted in descending order by correlation.",
    "table_selection_response": "```json\n{\n  \"tables\": [\"production_logs\", \"quality_control\"],\n  \"columns\": [\"production_logs.Line_Machine\", \"production_logs.Downtime_Minutes\", \"production_logs.Product\", \"quality_control.Line\", \"quality_control.Product\", \"quality_control.Failed_Qty\"]\n}\n```",
    "methodology_score": 54.54545454545454,
    "sql_score": 59.7745865970409,
    "table_column_score": 40.0,
    "response_quality_score": 97.09527730941772,
    "overall_score": 56.99426940354245,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 6,
      "expected_keyword_count": 11,
      "response_keyword_count": 128
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 33.33333333333333,
      "aggregate_match": 50.0,
      "clause_match": 60.0,
      "fuzzy_score": 49.608355091383814,
      "overall_score": 59.7745865970409,
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "downtime_minutes",
        "failed_qty"
      ],
      "generated_columns": [
        "line_machine",
        "product",
        "downtime_minutes",
        "correlation",
        "failed_qty",
        "corrdowntime_minutes"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 40.0,
      "table_score": 100.0,
      "column_score": 0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 0.0,
      "column_recall": 0.0,
      "tables_correct": [
        "production_logs",
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [],
      "columns_missing": [
        "product",
        "date",
        "downtime_minutes",
        "failed_qty"
      ],
      "columns_extra": [
        "production_logs.line_machine",
        "quality_control.product",
        "quality_control.line",
        "production_logs.downtime_minutes",
        "quality_control.failed_qty",
        "production_logs.product"
      ],
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "date",
        "downtime_minutes",
        "failed_qty"
      ],
      "generated_columns": [
        "production_logs.line_machine",
        "quality_control.product",
        "quality_control.line",
        "production_logs.downtime_minutes",
        "quality_control.failed_qty",
        "production_logs.product"
      ]
    },
    "total_latency_ms": 2904.7226905822754,
    "total_tokens": 1939,
    "errors": [],
    "timestamp": "2025-11-30T13:05:39.488076"
  },
  {
    "question_id": "Easy_26",
    "question_text": "What is the average target quantity for all products?",
    "category": "Easy",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. Identify the relevant data table: The **production_logs** table contains information about production targets and actual quantities, making it the most relevant table for answering this question.\n2. Select the required column: From the **production_logs** table, select the **Target_Qty** column, which represents the target quantity for each production run.\n3. Apply no filters: Since the question asks for the average target quantity for all products, no filters are needed to exclude any specific products or production runs.\n4. Calculate the average target quantity: Use the average aggregation function (e.g., AVG() in SQL) to calculate the average of the **Target_Qty** column.\n5. Execute the calculation: Run the calculation on the selected data to obtain the average target quantity for all products.\n```\nNote: The calculation can be expressed in SQL as: `SELECT AVG(Target_Qty) FROM production_logs`",
    "sql_response": "```sql\nSELECT AVG(Target_Qty) AS Average_Target_Qty\nFROM production_logs;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"production_logs\"],\n  \"columns\": [\"Target_Qty\"]\n}\n```",
    "methodology_score": 40.0,
    "sql_score": 96.8348623853211,
    "table_column_score": 100.0,
    "response_quality_score": 98.4054446220398,
    "overall_score": 80.73274629706637,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 2,
      "expected_keyword_count": 5,
      "response_keyword_count": 70
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 78.89908256880733,
      "overall_score": 96.8348623853211,
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "target_qty"
      ],
      "generated_columns": [
        "target_qty"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "target_qty"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "target_qty"
      ],
      "generated_columns": [
        "target_qty"
      ]
    },
    "total_latency_ms": 1594.555377960205,
    "total_tokens": 1333,
    "errors": [],
    "timestamp": "2025-11-30T13:05:47.352782"
  },
  {
    "question_id": "Complex_24",
    "question_text": "For Material Steel-101, what is the trend of wastage (increase or decrease) over the past 6 months?",
    "category": "Complex",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data table**: The question is about the trend of wastage for a specific material, Steel-101. The **inventory_logs** table contains information about material consumption, including wastage, and is the most relevant table for answering this question.\n\n2. **Filter the data for the specific material**: Apply a filter to the **inventory_logs** table to only include rows where **Material_Code** is 'Steel-101' or **Material_Name** is 'Steel-101', depending on how the material is identified in the table.\n\n3. **Filter the data for the past 6 months**: Apply a date filter to the **Date** column in the **inventory_logs** table to only include rows where the date is within the last 6 months from the current date or a specified date.\n\n4. **Aggregate wastage data by month**: Group the filtered data by month (using the **Date** column) and calculate the total wastage for each month. This can be done by summing up the **Wastage_Kg** column for each month.\n\n5. **Calculate the trend of wastage**: To determine the trend (increase or decrease) of wastage over the past 6 months, calculate the difference in total wastage between consecutive months. A positive difference indicates an increase in wastage, while a negative difference indicates a decrease.\n\n6. **Visualize or summarize the trend**: To clearly communicate the trend, consider visualizing the monthly wastage data in a line chart or summarizing the trend with a simple description, such as \"The wastage of Steel-101 has generally increased/decreased over the past 6 months, with [briefly mention any notable months or changes].\"\n\n7. **Optional: Calculate the overall percentage change**: For a more quantitative analysis, calculate the overall percentage change in wastage from the first month to the last month within the 6-month period. This provides a numerical value representing the trend.\n\n8. **Consider seasonality or external factors**: If the data shows significant fluctuations, consider whether there are any seasonal patterns or external factors (like changes in production schedules, maintenance, or supplier issues) that could influence the wastage trend and should be accounted for in the analysis.\n```",
    "sql_response": "```sql\nWITH monthly_wastage AS (\n  SELECT \n    DATE_TRUNC('month', il.Date) AS month,\n    SUM(il.Wastage_Kg) AS total_wastage\n  FROM \n    inventory_logs il\n  WHERE \n    il.Material_Code = 'Steel-101'\n    AND il.Date >= CURRENT_DATE - INTERVAL '6 month'\n  GROUP BY \n    DATE_TRUNC('month', il.Date)\n)\nSELECT \n  month, \n  total_wastage, \n  LAG(total_wastage) OVER (ORDER BY month) AS prev_month_wastage,\n  CASE \n    WHEN LAG(total_wastage) OVER (ORDER BY month) IS NULL THEN NULL\n    WHEN total_wastage > LAG(total_wastage) OVER (ORDER BY month) THEN 'Increase'\n    WHEN total_wastage < LAG(total_wastage) OVER (ORDER BY month) THEN 'Decrease'\n    ELSE 'No Change'\n  END AS trend\nFROM \n  monthly_wastage\nORDER BY \n  month;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Date\", \"Material_Code\", \"Material_Name\", \"Wastage_Kg\"]\n}\n```",
    "methodology_score": 61.904761904761905,
    "sql_score": 72.75182719444244,
    "table_column_score": 91.42857142857143,
    "response_quality_score": 96.88037300109863,
    "overall_score": 76.57974824673614,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 13,
      "expected_keyword_count": 21,
      "response_keyword_count": 146
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 23.076923076923077,
      "aggregate_match": 100.0,
      "clause_match": 83.33333333333334,
      "fuzzy_score": 63.2173095014111,
      "overall_score": 72.75182719444244,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "material_code",
        "strftime",
        "date",
        "strftime%y-%m",
        "totalwastage",
        "case when select totalwastage",
        "month"
      ],
      "generated_columns": [
        "wastage_kg",
        "date",
        "total_wastage",
        "case when lagtotal_wastage over order by month is null then null when total_wastage > lagtotal_wastage over order by month then increase when total_wastage < lagtotal_wastage over order by month then decrease else no change end",
        "date_truncmonth",
        "date_trunc",
        "lagtotal_wastage over order by month",
        "month"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 91.42857142857143,
      "table_score": 100.0,
      "column_score": 85.71428571428571,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 75.0,
      "column_recall": 100.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "material_code",
        "wastage_kg",
        "date"
      ],
      "columns_missing": [],
      "columns_extra": [
        "material_name"
      ],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_code",
        "wastage_kg",
        "date"
      ],
      "generated_columns": [
        "material_code",
        "material_name",
        "date",
        "wastage_kg"
      ]
    },
    "total_latency_ms": 3119.626998901367,
    "total_tokens": 1867,
    "errors": [],
    "timestamp": "2025-11-30T13:05:56.961727"
  },
  {
    "question_id": "Medium_20",
    "question_text": "Which supplier had the lowest average unit cost for 'Rubber-RB' in the last 6 months?",
    "category": "Medium",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data table**: The question is related to inventory and supplier information, so we will use the **inventory_logs** table.\n2. **Filter the data for the last 6 months**: Apply a date filter to the **Date** column in the **inventory_logs** table to only include records from the last 6 months.\n3. **Filter the data for 'Rubber-RB' material**: Apply a filter to the **Material_Name** column to only include records where the material is 'Rubber-RB'.\n4. **Calculate the total cost and total quantity for each supplier**: Group the filtered data by the **Supplier** column and calculate the total cost (by multiplying **Unit_Cost_Rupees** and **Received_Kg**) and total quantity (**Received_Kg**) for each supplier.\n5. **Calculate the average unit cost for each supplier**: Divide the total cost by the total quantity for each supplier to get the average unit cost.\n6. **Rank the suppliers by average unit cost**: Sort the suppliers by their average unit cost in ascending order to find the supplier with the lowest average unit cost.\n7. **Select the supplier with the lowest average unit cost**: Identify the supplier with the lowest average unit cost for 'Rubber-RB' in the last 6 months.\n```",
    "sql_response": "```sql\nSELECT Supplier, AVG(Unit_Cost_Rupees) AS Average_Unit_Cost\nFROM inventory_logs\nWHERE Material_Code = 'Rubber-RB' AND Date >= DATEADD(month, -6, GETDATE())\nGROUP BY Supplier\nORDER BY Average_Unit_Cost ASC\nLIMIT 1;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Supplier\", \"Material_Name\", \"Unit_Cost_Rupees\", \"Date\"]\n}\n```",
    "methodology_score": 76.47058823529412,
    "sql_score": 87.39795918367346,
    "table_column_score": 85.0,
    "response_quality_score": 98.1803810596466,
    "overall_score": 84.5985002908386,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 13,
      "expected_keyword_count": 17,
      "response_keyword_count": 65
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 60.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 82.6530612244898,
      "overall_score": 87.39795918367346,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_code",
        "unit_cost_rupees",
        "supplier",
        "avgcost"
      ],
      "generated_columns": [
        "material_code",
        "average_unit_cost",
        "unit_cost_rupees",
        "supplier"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 85.0,
      "table_score": 100.0,
      "column_score": 75.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 75.0,
      "column_recall": 75.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "unit_cost_rupees",
        "supplier",
        "date"
      ],
      "columns_missing": [
        "material_code"
      ],
      "columns_extra": [
        "material_name"
      ],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_code",
        "unit_cost_rupees",
        "supplier",
        "date"
      ],
      "generated_columns": [
        "material_name",
        "supplier",
        "date",
        "unit_cost_rupees"
      ]
    },
    "total_latency_ms": 1819.6189403533936,
    "total_tokens": 1513,
    "errors": [],
    "timestamp": "2025-11-30T13:06:05.307757"
  },
  {
    "question_id": "Complex_29",
    "question_text": "How does the rework count vary by defect type and product?",
    "category": "Complex",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data table**: To answer the question about how the rework count varies by defect type and product, we need to use the **quality_control** table, as it contains information about the rework count, defect type, and product.\n\n2. **Filter the data if necessary**: Depending on the scope of the analysis, we might need to filter the data by specific dates or inspection batches. However, since the question is general, we will consider all available data in the **quality_control** table.\n\n3. **Group the data by defect type and product**: To analyze the variation of rework count by defect type and product, we need to group the data in the **quality_control** table by these two columns. This will allow us to calculate the rework count for each combination of defect type and product.\n\n4. **Calculate the rework count for each group**: For each group of defect type and product, we will calculate the total rework count. This can be done by summing up the **Rework_Count** column for each group.\n\n5. **Optionally, calculate additional metrics**: Depending on the depth of analysis required, we might also want to calculate other metrics such as the average rework count per inspection, the total inspected quantity, or the percentage of failed quantity that required rework for each defect type and product.\n\n6. **Visualize the results**: To effectively communicate the findings, we should consider visualizing the data, such as using a heatmap or bar chart to show how the rework count varies by defect type and product. This step will help in identifying patterns or outliers that need further investigation.\n\n7. **Refine the analysis if necessary**: Based on the initial findings, we might need to refine our analysis by applying additional filters (e.g., focusing on specific products or defect types), or by incorporating data from other tables to gain a more comprehensive understanding of the factors influencing rework counts.\n\n8. **Document the findings**: Finally, we should document our methodology, findings, and any recommendations for improvement based on the analysis. This documentation will be useful for stakeholders and for future reference.\n```",
    "sql_response": "```sql\nSELECT \n    q.Defect_Type,\n    q.Product,\n    SUM(q.Rework_Count) AS Total_Rework_Count\nFROM \n    quality_control q\nGROUP BY \n    q.Defect_Type,\n    q.Product\nORDER BY \n    q.Defect_Type,\n    q.Product;\n```\nThis SQL query will provide the total rework count for each defect type and product, giving insight into how the rework count varies by these factors. The results will be ordered by defect type and then by product for easier analysis.",
    "table_selection_response": "```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Defect_Type\", \"Product\", \"Rework_Count\"]\n}\n```",
    "methodology_score": 55.55555555555556,
    "sql_score": 68.89733840304183,
    "table_column_score": 100.0,
    "response_quality_score": 97.48018074035645,
    "overall_score": 75.52875318176694,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 5,
      "expected_keyword_count": 9,
      "response_keyword_count": 139
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 75.0,
      "aggregate_match": 25.0,
      "clause_match": 75.0,
      "fuzzy_score": 59.31558935361216,
      "overall_score": 68.89733840304183,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "defect_type",
        "rework_count",
        "q"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "defect_type",
        "rework_count"
      ]
    },
    "total_latency_ms": 2519.8192596435547,
    "total_tokens": 1692,
    "errors": [],
    "timestamp": "2025-11-30T13:06:14.136403"
  },
  {
    "question_id": "Medium_24",
    "question_text": "Which material has the highest wastage (in Kg) from July 2026 to Dec 2026?",
    "category": "Medium",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the relevant data table**: The question is about material wastage, so we will use the **inventory_logs** table, which contains information about material consumption, receipt, and wastage.\n\n2. **Apply date filter**: Filter the **inventory_logs** table to only include records from July 2026 to December 2026. This can be done by applying a date range filter on the **Date** column: `Date >= '2026-07-01'` and `Date <= '2026-12-31'`.\n\n3. **Extract relevant columns**: From the filtered **inventory_logs** table, extract the **Material_Name** and **Wastage_Kg** columns, as these are the only columns needed to answer the question.\n\n4. **Group by material and calculate total wastage**: Group the extracted data by **Material_Name** and calculate the total wastage for each material by summing up the **Wastage_Kg** values. This can be done using a SQL query with a GROUP BY clause: `GROUP BY Material_Name` and `SUM(Wastage_Kg) AS Total_Wastage`.\n\n5. **Rank materials by total wastage**: Rank the materials by their total wastage in descending order (highest wastage first) using a SQL query with an ORDER BY clause: `ORDER BY Total_Wastage DESC`.\n\n6. **Identify the material with the highest wastage**: The material with the highest wastage will be the first row in the ranked result set. This can be extracted using a SQL query with a LIMIT clause: `LIMIT 1`.\n```",
    "sql_response": "```sql\nSELECT Material_Name, SUM(Wastage_Kg) AS Total_Wastage\nFROM inventory_logs\nWHERE Date BETWEEN '2026-07-01' AND '2026-12-31'\nGROUP BY Material_Name\nORDER BY Total_Wastage DESC\nLIMIT 1;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Material_Name\", \"Wastage_Kg\", \"Date\"]\n}\n```",
    "methodology_score": 52.94117647058824,
    "sql_score": 75.0,
    "table_column_score": 100.0,
    "response_quality_score": 98.25201678276062,
    "overall_score": 76.95755461945252,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 9,
      "expected_keyword_count": 17,
      "response_keyword_count": 90
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 50.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 83.33333333333334,
      "overall_score": 75.0,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "sum",
        "material_name"
      ],
      "generated_columns": [
        "wastage_kg",
        "material_name",
        "total_wastage"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "wastage_kg",
        "material_name",
        "date"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "material_name",
        "date"
      ],
      "generated_columns": [
        "wastage_kg",
        "material_name",
        "date"
      ]
    },
    "total_latency_ms": 1747.9832172393799,
    "total_tokens": 1570,
    "errors": [],
    "timestamp": "2025-11-30T13:06:22.519077"
  },
  {
    "question_id": "Complex_31",
    "question_text": "What is the correlation between downtime (in minutes) and failed quantity for each product?",
    "category": "Complex",
    "model_id": "llama-3.3-70b-versatile",
    "methodology_response": "```steps\n1. **Identify the necessary tables**: To answer the question, we need to use the **production_logs** table for downtime data and the **quality_control** table for failed quantity data. \n2. **Merge the tables**: We need to merge the **production_logs** and **quality_control** tables based on the 'Product' and 'Date' columns to combine downtime and failed quantity data for each product. However, since the date in **production_logs** is not explicitly mentioned as an inspection date, we will assume it can be used to join with the **quality_control** table's 'Inspection_Date' for the purpose of this analysis.\n3. **Apply filters**: Filter the data to only include records where both downtime minutes and failed quantity are available (i.e., not null) to ensure accurate correlation calculations.\n4. **Aggregate data by product**: Group the merged data by 'Product' to calculate the correlation between downtime and failed quantity for each product separately.\n5. **Calculate total downtime and failed quantity for each product**: For each product, calculate the total downtime in minutes and the total failed quantity. This step involves summing up 'Downtime_Minutes' from **production_logs** and 'Failed_Qty' from **quality_control** for each product group.\n6. **Calculate the correlation coefficient**: Use a statistical method (such as Pearson correlation coefficient) to calculate the correlation between total downtime and total failed quantity for each product. This will provide a value between -1 and 1, where 1 indicates a perfect positive correlation, -1 indicates a perfect negative correlation, and 0 indicates no correlation.\n7. **Interpret the results**: Analyze the correlation coefficients for each product to understand the relationship between downtime and failed quantity. A positive correlation might suggest that increased downtime is associated with more failed products, while a negative correlation could indicate that downtime has no direct impact or even a positive effect on product quality.\n8. **Visualize the data (optional)**: To better understand the relationship, consider plotting the data points for each product on a scatter plot, with downtime on one axis and failed quantity on the other. This visual representation can help in identifying patterns or outliers that might not be immediately apparent from the correlation coefficients alone.\n```",
    "sql_response": "```sql\nWITH production_data AS (\n  SELECT \n    pl.Product,\n    pl.Downtime_Minutes,\n    qc.Failed_Qty\n  FROM \n    production_logs pl\n  JOIN \n    quality_control qc ON pl.Date = qc.Inspection_Date AND pl.Product = qc.Product\n)\nSELECT \n  Product,\n  CORR(Downtime_Minutes, Failed_Qty) AS correlation\nFROM \n  production_data\nGROUP BY \n  Product\nORDER BY \n  correlation DESC;\n```\nThis query first creates a temporary view (`production_data`) that joins the `production_logs` and `quality_control` tables based on the date and product. It then calculates the correlation between downtime minutes and failed quantity for each product using the `CORR` function. The results are sorted in descending order by correlation. \n\nNote: The `CORR` function is used to calculate the Pearson correlation coefficient, which measures the linear correlation between two variables. The result will be a value between -1 and 1, where 1 means perfect positive correlation, -1 means perfect negative correlation, and 0 means no correlation.",
    "table_selection_response": "```json\n{\n  \"tables\": [\"production_logs\", \"quality_control\"],\n  \"columns\": [\"production_logs.Downtime_Minutes\", \"production_logs.Product\", \"quality_control.Product\", \"quality_control.Failed_Qty\"]\n}\n```",
    "methodology_score": 68.75,
    "sql_score": 61.297520661157016,
    "table_column_score": 40.0,
    "response_quality_score": 96.59062814712524,
    "overall_score": 61.73819504611748,
    "methodology_details": {
      "method": "keyword_fallback",
      "overlap_count": 11,
      "expected_keyword_count": 16,
      "response_keyword_count": 131
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 33.33333333333333,
      "aggregate_match": 33.33333333333333,
      "clause_match": 80.0,
      "fuzzy_score": 61.98347107438016,
      "overall_score": 61.297520661157016,
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "pl",
        "product",
        "totalfailedqty",
        "downtime_minutes",
        "totaldowntime",
        "failed_qty",
        "qc"
      ],
      "generated_columns": [
        "product",
        "downtime_minutes",
        "correlation",
        "failed_qty",
        "corrdowntime_minutes"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 40.0,
      "table_score": 100.0,
      "column_score": 0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 0.0,
      "column_recall": 0.0,
      "tables_correct": [
        "production_logs",
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [],
      "columns_missing": [
        "product",
        "downtime_minutes",
        "failed_qty"
      ],
      "columns_extra": [
        "production_logs.downtime_minutes",
        "quality_control.product",
        "quality_control.failed_qty",
        "production_logs.product"
      ],
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "downtime_minutes",
        "failed_qty"
      ],
      "generated_columns": [
        "production_logs.downtime_minutes",
        "quality_control.product",
        "quality_control.failed_qty",
        "production_logs.product"
      ]
    },
    "total_latency_ms": 3409.371852874756,
    "total_tokens": 1864,
    "errors": [],
    "timestamp": "2025-11-30T13:06:32.563502"
  }
]