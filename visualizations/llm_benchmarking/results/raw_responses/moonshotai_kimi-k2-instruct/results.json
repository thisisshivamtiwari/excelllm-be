[
  {
    "question_id": "Complex_13",
    "question_text": "How does the average rework count for 'Widget-A' differ between Line-1 and Line-2?",
    "category": "Complex",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. From the quality_control table, filter rows where Product = 'Widget-A' and Line is either 'Line-1' or 'Line-2'.\n2. Group the filtered data by Line.\n3. For each group (Line-1 and Line-2), calculate the average Rework_Count.\n4. Subtract the average rework count of Line-2 from that of Line-1 to obtain the difference.\n```",
    "sql_response": "```sql\nSELECT\n    Line,\n    AVG(Rework_Count) AS Avg_Rework\nFROM quality_control\nWHERE Product = 'Widget-A'\n  AND Line IN ('Line-1', 'Line-2')\nGROUP BY Line;\n```",
    "table_selection_response": "{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Product\", \"Line\", \"Rework_Count\"]\n}",
    "methodology_score": 100,
    "sql_score": 98.10344827586206,
    "table_column_score": 100.0,
    "response_quality_score": 98.4081780910492,
    "overall_score": 99.17702470565663,
    "methodology_details": {
      "similarity_score": 100,
      "matching_concepts": [
        "Filtering quality_control data for Product = 'Widget-A'",
        "Grouping data by Line (Line-1 and Line-2)",
        "Calculating the average Rework_Count for each line",
        "Comparing the average Rework_Count between the two lines"
      ],
      "missing_concepts": [],
      "extra_concepts": [],
      "reasoning": "The LLM's steps perfectly match the expected steps. The wording is slightly different in step 4 (explicitly mentioning subtraction), but the core logic of comparing the averages is present and accurate."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 87.35632183908046,
      "overall_score": 98.10344827586206,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "line",
        "product",
        "rework_count"
      ],
      "generated_columns": [
        "line",
        "product",
        "rework_count"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "line",
        "rework_count"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "line",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "line",
        "rework_count"
      ]
    },
    "total_latency_ms": 1591.8219089508057,
    "total_tokens": 1294,
    "errors": [],
    "timestamp": "2025-11-30T13:36:40.944234"
  },
  {
    "question_id": "Easy_15",
    "question_text": "Which inspector had the highest number of reworked items?",
    "category": "Easy",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Use only the quality_control table – it is the only source that contains both the inspector name and the rework count per inspection batch.\n\n2. Filter out any rows where Rework_Count is NULL or negative (keep only valid numeric rework values).\n\n3. Group the data by Inspector_Name.\n\n4. Within each inspector group, compute the sum of Rework_Count – this gives the total number of items that had to be reworked under that inspector’s watch.\n\n5. Sort the aggregated result in descending order of total reworked items.\n\n6. Select the top row; the Inspector_Name in this row is the inspector with the highest number of reworked items.\n```",
    "sql_response": "```sql\nSELECT Inspector_Name,\n       SUM(Rework_Count) AS Total_Reworked\nFROM quality_control\nGROUP BY Inspector_Name\nORDER BY Total_Reworked DESC\nLIMIT 1;\n```",
    "table_selection_response": "{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Inspector_Name\", \"Rework_Count\"]\n}",
    "methodology_score": 100,
    "sql_score": 78.70782726045883,
    "table_column_score": 100.0,
    "response_quality_score": 97.86485815048218,
    "overall_score": 92.3342253562088,
    "methodology_details": {
      "similarity_score": 100,
      "matching_concepts": [
        "Use only the quality_control table",
        "Group quality_control by Inspector_Name",
        "Sum the Rework_Count for each inspector",
        "Order by the sum in descending order",
        "Select the first Inspector_Name"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Filter out any rows where Rework_Count is NULL or negative (keep only valid numeric rework values)."
      ],
      "reasoning": "The LLM's steps are logically equivalent to the ground truth. The extra step of filtering out NULL or negative values is a good practice and does not detract from the solution's correctness."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 50.0,
      "aggregate_match": 66.66666666666666,
      "clause_match": 100.0,
      "fuzzy_score": 85.82995951417004,
      "overall_score": 78.70782726045883,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "sum",
        "rework_count",
        "inspector_name"
      ],
      "generated_columns": [
        "total_reworked",
        "rework_count",
        "inspector_name"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "rework_count",
        "inspector_name"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "rework_count",
        "inspector_name"
      ],
      "generated_columns": [
        "rework_count",
        "inspector_name"
      ]
    },
    "total_latency_ms": 2135.1418495178223,
    "total_tokens": 1302,
    "errors": [],
    "timestamp": "2025-11-30T13:36:47.696433"
  },
  {
    "question_id": "Easy_4",
    "question_text": "What is the total number of components reworked in Line-2?",
    "category": "Easy",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Identify the relevant table: Use the quality_control table as it contains the Rework_Count field.\n\n2. Apply the filter: Filter the quality_control table to keep only rows where Line = 'Line-2'.\n\n3. Aggregate the rework counts: Sum the Rework_Count column for the filtered rows to obtain the total number of components reworked on Line-2.\n```",
    "sql_response": "```sql\nSELECT SUM(Rework_Count) AS Total_Reworked_Components\nFROM quality_control\nWHERE Line = 'Line-2';\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Rework_Count\", \"Line\"]\n}\n```",
    "methodology_score": 100,
    "sql_score": 90.52083333333333,
    "table_column_score": 100.0,
    "response_quality_score": 98.31886410713196,
    "overall_score": 96.51417807737985,
    "methodology_details": {
      "similarity_score": 100,
      "matching_concepts": [
        "Use the quality_control table",
        "Filter quality_control for Line = 'Line-2'",
        "Sum the Rework_Count column"
      ],
      "missing_concepts": [],
      "extra_concepts": [],
      "reasoning": "The LLM's steps perfectly match the expected steps. It correctly identifies the table, filtering condition, and aggregation function."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 66.66666666666666,
      "clause_match": 100.0,
      "fuzzy_score": 81.25,
      "overall_score": 90.52083333333333,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "line",
        "rework_count"
      ],
      "generated_columns": [
        "line",
        "rework_count"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "line",
        "rework_count"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "line",
        "rework_count"
      ],
      "generated_columns": [
        "line",
        "rework_count"
      ]
    },
    "total_latency_ms": 1681.135892868042,
    "total_tokens": 1245,
    "errors": [],
    "timestamp": "2025-11-30T13:36:53.764478"
  },
  {
    "question_id": "Complex_26",
    "question_text": "What is the impact of preventive maintenance on the subsequent breakdown frequency for Machine-M1?",
    "category": "Complex",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. From maintenance_logs, filter Machine = 'Machine-M1' and Maintenance_Type = 'Preventive' to create a list of all preventive-maintenance (PM) dates.\n\n2. For each PM date, create a 30-day “post-PM window” (PM_Date to PM_Date + 30 days) and a 30-day “pre-PM window” (PM_Date – 30 days to PM_Date – 1 day).\n\n3. Still in maintenance_logs, filter Machine = 'Machine-M1' and Maintenance_Type = 'Breakdown' to obtain every breakdown record; keep Breakdown_Date and Downtime_Hours.\n\n4. Join the breakdown list to the PM list so every breakdown is tagged with the nearest preceding PM date (i.e., the PM that “covers” it).  Breakdowns falling inside the post-PM window are flagged as post-PM; those inside the pre-PM window are flagged as pre-PM.\n\n5. Aggregate to count:\n   - Pre_Breakdown_Count = number of breakdowns in the pre-PM window  \n   - Post_Breakdown_Count = number of breakdowns in the post-PM window  \n   for each PM event.\n\n6. Compute the breakdown frequency:\n   - Pre_Frequency = Pre_Breakdown_Count / 30 days  \n   - Post_Frequency = Post_Breakdown_Count / 30 days\n\n7. Average Pre_Frequency and Post_Frequency across all PM events to get overall pre-PM and post-PM breakdown frequencies.\n\n8. Calculate the impact:  \n   Impact = (Pre_Frequency – Post_Frequency) / Pre_Frequency × 100%.  \n   A positive percentage indicates preventive maintenance reduced breakdown frequency for Machine-M1.\n```",
    "sql_response": "```sql\n-- Compare breakdown frequency in the 30 days AFTER a preventive-maintenance event\n-- with the 30 days BEFORE that same event for Machine-M1.\n-- A lower ratio (<1) indicates that preventive maintenance reduces breakdowns.\n\nWITH pm_events AS (\n    SELECT Maintenance_Date AS pm_date\n    FROM maintenance_logs\n    WHERE Machine = 'Machine-M1'\n      AND Maintenance_Type = 'Preventive'\n),\nbefore_window AS (\n    SELECT\n        p.pm_date,\n        COUNT(*) AS breakdowns_before\n    FROM pm_events p\n    JOIN maintenance_logs m\n      ON m.Machine = 'Machine-M1'\n     AND m.Maintenance_Type = 'Breakdown'\n     AND m.Breakdown_Date BETWEEN DATE(p.pm_date, '-30 days') AND DATE(p.pm_date, '-1 day')\n    GROUP BY p.pm_date\n),\nafter_window AS (\n    SELECT\n        p.pm_date,\n        COUNT(*) AS breakdowns_after\n    FROM pm_events p\n    JOIN maintenance_logs m\n      ON m.Machine = 'Machine-M1'\n     AND m.Maintenance_Type = 'Breakdown'\n     AND m.Breakdown_Date BETWEEN DATE(p.pm_date, '+1 day') AND DATE(p.pm_date, '+30 days')\n    GROUP BY p.pm_date\n)\nSELECT\n    AVG(CASE WHEN b.breaks > 0 THEN a.breaks * 1.0 / b.breaks ELSE NULL END) AS avg_breakdown_ratio,\n    COUNT(*) AS pm_events_considered\nFROM (\n    SELECT\n        COALESCE(bw.pm_date, aw.pm_date) AS pm_date,\n        COALESCE(bw.breakdowns_before, 0) AS breaks,\n        COALESCE(aw.breakdowns_after, 0) AS breaks\n    FROM before_window bw\n    FULL OUTER JOIN after_window aw ON bw.pm_date = aw.pm_date\n) AS combined;\n```",
    "table_selection_response": "{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Machine\", \"Maintenance_Type\", \"Breakdown_Date\", \"Maintenance_Date\"]\n}",
    "methodology_score": 75.0,
    "sql_score": 59.52761704366123,
    "table_column_score": 100.0,
    "response_quality_score": 96.41529679298401,
    "overall_score": 77.97619564457983,
    "methodology_details": {
      "similarity_score": 75.0,
      "matching_concepts": [
        "Identify dates of preventive maintenance for Machine-M1",
        "Identify dates of breakdown events for Machine-M1",
        "Calculate the impact of preventive maintenance on breakdown frequency",
        "Filtering data based on machine and maintenance type"
      ],
      "missing_concepts": [
        "Calculate the time (in days) between each preventive maintenance and the next breakdown",
        "Calculate the average time to breakdown after preventive maintenance"
      ],
      "extra_concepts": [
        "Creating pre and post PM windows",
        "Calculating breakdown counts within the windows",
        "Calculating breakdown frequency within the windows"
      ],
      "reasoning": "The LLM provides a reasonable approach using pre and post-PM windows to determine breakdown frequency changes, but the method in the Expected steps calculates the average time to breakdown, which is a different, and arguably better, measure. LLM uses breakdown counts and frequencies within fixed windows, while the ground truth uses actual time until the next breakdown. LLM steps are logical and could provide an answer, but misses some of the intended logic. It also introduces the unnecessary complexity of Pre_Breakdown_Count and Pre_Frequency."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 20.0,
      "aggregate_match": 50.0,
      "clause_match": 80.0,
      "fuzzy_score": 50.1841136244082,
      "overall_score": 59.52761704366123,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "breakdown_date",
        "maintenance_date",
        "daystobreakdown",
        "machine"
      ],
      "generated_columns": [
        "breakdowns_before",
        "maintenance_date",
        "p",
        "0",
        "breakdowns_after",
        "machine",
        "pm_date",
        "breaks else null end"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "breakdown_date",
        "maintenance_date",
        "maintenance_type",
        "machine"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "breakdown_date",
        "maintenance_date",
        "maintenance_type",
        "machine"
      ],
      "generated_columns": [
        "breakdown_date",
        "maintenance_date",
        "maintenance_type",
        "machine"
      ]
    },
    "total_latency_ms": 3584.703207015991,
    "total_tokens": 1897,
    "errors": [],
    "timestamp": "2025-11-30T13:37:02.821904"
  },
  {
    "question_id": "Medium_2",
    "question_text": "What is the total cost of maintenance for Machine-M2 in 2026?",
    "category": "Medium",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Open the maintenance_logs table and filter the Machine column to keep only rows where Machine = 'Machine-M2'.\n\n2. Within the filtered data, keep only rows where the Maintenance_Date falls in calendar year 2026 (i.e., Maintenance_Date between 2026-01-01 and 2026-12-31).\n\n3. Sum the Cost_Rupees column of the remaining rows to obtain the total maintenance cost for Machine-M2 in 2026.\n```",
    "sql_response": "```sql\nSELECT SUM(Cost_Rupees) AS Total_Maintenance_Cost\nFROM maintenance_logs\nWHERE Machine = 'Machine-M2'\n  AND Maintenance_Date BETWEEN '2026-01-01' AND '2026-12-31';\n```",
    "table_selection_response": "{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Machine\", \"Maintenance_Date\", \"Cost_Rupees\"]\n}",
    "methodology_score": 100,
    "sql_score": 86.08208955223881,
    "table_column_score": 100.0,
    "response_quality_score": 98.34061551094055,
    "overall_score": 94.96279289437764,
    "methodology_details": {
      "similarity_score": 100,
      "matching_concepts": [
        "Filter maintenance_logs for Machine = 'Machine-M2'",
        "Filter for dates in 2026",
        "Sum the Cost_Rupees column."
      ],
      "missing_concepts": [],
      "extra_concepts": [],
      "reasoning": "The LLM generated steps perfectly match the ground truth steps. All filtering and aggregation operations are identical."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 73.88059701492537,
      "overall_score": 86.08208955223881,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "machine"
      ],
      "generated_columns": [
        "cost_rupees",
        "machine"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "cost_rupees",
        "maintenance_date",
        "machine"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "maintenance_date",
        "machine"
      ],
      "generated_columns": [
        "cost_rupees",
        "maintenance_date",
        "machine"
      ]
    },
    "total_latency_ms": 1659.3844890594482,
    "total_tokens": 1297,
    "errors": [],
    "timestamp": "2025-11-30T13:37:08.868898"
  },
  {
    "question_id": "Easy_32",
    "question_text": "What is the percentage of failed quantity for each product?",
    "category": "Easy",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Identify the required table: Use only the **quality_control** table, as it contains the inspected, passed, and failed quantities by product.\n\n2. Aggregate the data: Group the records by **Product** and compute the sums of **Inspected_Qty** and **Failed_Qty** for each product.\n\n3. Calculate the failure percentage: For each product, divide the total **Failed_Qty** by the total **Inspected_Qty** and multiply by 100 to obtain the percentage of failed quantity.\n\n4. (Optional) Filter out null or zero inspected quantities: Exclude any product rows where **Inspected_Qty** is zero or null to avoid division-by-zero errors.\n```",
    "sql_response": "```sql\nSELECT\n    Product,\n    SUM(Inspected_Qty) AS Total_Inspected,\n    SUM(Failed_Qty) AS Total_Failed,\n    ROUND(100.0 * SUM(Failed_Qty) / NULLIF(SUM(Inspected_Qty), 0), 2) AS Fail_Percentage\nFROM quality_control\nGROUP BY Product\nORDER BY Fail_Percentage DESC;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Product\", \"Failed_Qty\", \"Inspected_Qty\"]\n}\n```",
    "methodology_score": 100,
    "sql_score": 62.056786703601105,
    "table_column_score": 100.0,
    "response_quality_score": 98.00325322151184,
    "overall_score": 86.52020066841156,
    "methodology_details": {
      "similarity_score": 100,
      "matching_concepts": [
        "Use quality_control table",
        "Group by Product",
        "Calculate the sum of Failed_Qty for each product",
        "Calculate the sum of Inspected_Qty for each product",
        "Divide the sum of Failed_Qty by the sum of Inspected_Qty, and multiply by 100"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Filter out null or zero inspected quantities"
      ],
      "reasoning": "The LLM accurately identifies the table and performs the required grouping and aggregation steps. The logic for calculating the failure percentage is also correct. The optional filtering step is a good practice."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 25.0,
      "aggregate_match": 50.0,
      "clause_match": 75.0,
      "fuzzy_score": 63.711911357340725,
      "overall_score": 62.056786703601105,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "0 / inspected_qty",
        "failed_qty"
      ],
      "generated_columns": [
        "inspected_qty",
        "product",
        "0",
        "fail_percentage",
        "failed_qty",
        "2",
        "0 * failed_qty / nullifinspected_qty"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "inspected_qty",
        "failed_qty"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "inspected_qty",
        "failed_qty"
      ],
      "generated_columns": [
        "product",
        "inspected_qty",
        "failed_qty"
      ]
    },
    "total_latency_ms": 1996.7467784881592,
    "total_tokens": 1364,
    "errors": [],
    "timestamp": "2025-11-30T13:37:15.651586"
  },
  {
    "question_id": "Easy_29",
    "question_text": "What is the maximum downtime hours recorded for any breakdown?",
    "category": "Easy",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Identify the relevant table: Use the maintenance_logs table as it contains the Downtime_Hours column that records breakdown durations.\n\n2. Filter for breakdown events: Apply a filter to include only rows where Maintenance_Type = 'Breakdown' (assuming this value exists; if not, use all rows since breakdowns are inherently captured in this table).\n\n3. Extract the Downtime_Hours column: Select only the Downtime_Hours values from the filtered dataset.\n\n4. Calculate the maximum value: Use the MAX() aggregation function on the Downtime_Hours column to find the highest recorded downtime hours for any breakdown event.\n\n5. Return the result: The output will be a single numerical value representing the maximum downtime hours recorded for any breakdown.\n```",
    "sql_response": "```sql\nSELECT MAX(Downtime_Hours) AS Max_Downtime_Hours\nFROM maintenance_logs\nWHERE Maintenance_Type = 'Breakdown';\n```",
    "table_selection_response": "{\"tables\": [\"maintenance_logs\"], \"columns\": [\"Downtime_Hours\"]}",
    "methodology_score": 100,
    "sql_score": 98.17460317460318,
    "table_column_score": 80.0,
    "response_quality_score": 98.24360918998718,
    "overall_score": 94.18547203010984,
    "methodology_details": {
      "similarity_score": 100,
      "matching_concepts": [
        "Filtering maintenance_logs for Maintenance_Type = 'Breakdown'",
        "Finding the maximum value in the Downtime_Hours column",
        "Using the maintenance_logs table"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Identifying the relevant table",
        "Extracting the Downtime_Hours column",
        "Applying MAX() aggregation function",
        "Returning the result"
      ],
      "reasoning": "The LLM's steps are logically equivalent to the ground truth. The extra steps are simply more verbose descriptions of the underlying operations."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 87.83068783068784,
      "overall_score": 98.17460317460318,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "downtime_hours",
        "maintenance_type"
      ],
      "generated_columns": [
        "downtime_hours",
        "maintenance_type"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 80.0,
      "table_score": 100.0,
      "column_score": 66.66666666666666,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 50.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "downtime_hours"
      ],
      "columns_missing": [
        "maintenance_type"
      ],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "downtime_hours",
        "maintenance_type"
      ],
      "generated_columns": [
        "downtime_hours"
      ]
    },
    "total_latency_ms": 1756.3908100128174,
    "total_tokens": 1309,
    "errors": [],
    "timestamp": "2025-11-30T13:37:21.947332"
  },
  {
    "question_id": "Easy_18",
    "question_text": "How many preventive maintenance activities were performed on Machine-M2 this year?",
    "category": "Easy",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Open the maintenance_logs table – it is the only table that contains maintenance activity records.\n2. Filter the column Machine to keep only rows where Machine = 'Machine-M2'.\n3. Filter the column Maintenance_Date to keep only rows whose year equals the current year (e.g., 2023 if today is 2023).\n4. Filter the column Maintenance_Type to keep only rows where Maintenance_Type = 'Preventive'.\n5. Count the number of rows that remain after the above three filters – this count is the number of preventive maintenance activities performed on Machine-M2 this year.\n```",
    "sql_response": "```sql\nSELECT COUNT(*) AS Preventive_Maintenance_Count\nFROM maintenance_logs\nWHERE Machine = 'Machine-M2'\n  AND Maintenance_Type = 'Preventive'\n  AND EXTRACT(YEAR FROM Maintenance_Date) = EXTRACT(YEAR FROM CURRENT_DATE);\n```",
    "table_selection_response": "{\"tables\": [\"maintenance_logs\"], \"columns\": [\"Machine\", \"Maintenance_Type\"]}",
    "methodology_score": 100,
    "sql_score": 95.68062827225131,
    "table_column_score": 88.0,
    "response_quality_score": 98.18256616592407,
    "overall_score": 95.30647651188036,
    "methodology_details": {
      "similarity_score": 100,
      "matching_concepts": [
        "Filter maintenance_logs for Machine = 'Machine-M2'",
        "Filter for Maintenance_Type = 'Preventive'",
        "Filter for dates this year",
        "Count the number of rows"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Open the maintenance_logs table"
      ],
      "reasoning": "The LLM's steps perfectly match the ground truth steps, with only an extra step of opening the table, which is implicit in the ground truth. The filtering conditions and the final aggregation are identical."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 71.20418848167539,
      "overall_score": 95.68062827225131,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "machine"
      ],
      "generated_columns": [
        "machine"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 88.0,
      "table_score": 100.0,
      "column_score": 80.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 66.66666666666666,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "maintenance_type",
        "machine"
      ],
      "columns_missing": [
        "maintenance_date"
      ],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "maintenance_date",
        "maintenance_type",
        "machine"
      ],
      "generated_columns": [
        "maintenance_type",
        "machine"
      ]
    },
    "total_latency_ms": 1817.4338340759277,
    "total_tokens": 1306,
    "errors": [],
    "timestamp": "2025-11-30T13:37:28.270045"
  },
  {
    "question_id": "Complex_32",
    "question_text": "Which machine has the highest average downtime due to breakdowns in the last quarter, and what is the total cost associated with those breakdowns?",
    "category": "Complex",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Filter the maintenance_logs table to keep only records where Maintenance_Type = 'Breakdown' and Breakdown_Date falls within the last quarter (use DATEADD(QUARTER, -1, GETDATE()) or equivalent in your SQL dialect).\n\n2. From the filtered maintenance_logs, group by Machine and calculate:\n   - Average Downtime_Hours = AVG(Downtime_Hours)\n   - Total_Cost = SUM(Cost_Rupees)\n\n3. Order the grouped results by Average Downtime_Hours in descending order and pick the top 1 Machine; this gives the machine with the highest average downtime due to breakdowns.\n\n4. Return the Machine name, its Average Downtime_Hours, and the corresponding Total_Cost for that machine.\n```",
    "sql_response": "```sql\nSELECT \n    Machine,\n    AVG(Downtime_Hours) AS Avg_Downtime_Hours,\n    SUM(Cost_Rupees) AS Total_Breakdown_Cost\nFROM maintenance_logs\nWHERE Maintenance_Type = 'Breakdown'\n  AND Maintenance_Date >= DATE_SUB(CURDATE(), INTERVAL 3 MONTH)\nGROUP BY Machine\nORDER BY Avg_Downtime_Hours DESC\nLIMIT 1;\n```",
    "table_selection_response": "{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Machine\", \"Breakdown_Date\", \"Downtime_Hours\", \"Cost_Rupees\"]\n}",
    "methodology_score": 90.0,
    "sql_score": 85.59006211180123,
    "table_column_score": 80.0,
    "response_quality_score": 97.95240998268127,
    "overall_score": 86.75176273739856,
    "methodology_details": {
      "similarity_score": 90.0,
      "matching_concepts": [
        "Filter maintenance_logs for breakdowns",
        "Filter records within the last quarter",
        "Group records by 'Machine'",
        "Calculate average Downtime_Hours for each machine",
        "Calculate sum of Cost_Rupees for each machine",
        "Order by average Downtime_Hours in descending order",
        "Select the machine with the highest average downtime and its total cost"
      ],
      "missing_concepts": [],
      "extra_concepts": [],
      "reasoning": "The LLM's steps closely mirror the expected steps. It correctly identifies the table, filters, groupings, aggregations, ordering, and selection criteria. The use of SQL-specific terminology like DATEADD and GETDATE is not penalized, as it is a possible implementation detail. No steps are missing and no incorrect steps are present."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 57.14285714285714,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 75.36231884057972,
      "overall_score": 85.59006211180123,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "totalcost",
        "avgdowntime",
        "maintenance_type",
        "machine",
        "cost_rupees",
        "downtime_hours"
      ],
      "generated_columns": [
        "cost_rupees",
        "maintenance_type",
        "machine",
        "avg_downtime_hours",
        "downtime_hours"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 80.0,
      "table_score": 100.0,
      "column_score": 66.66666666666666,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 75.0,
      "column_recall": 60.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "cost_rupees",
        "downtime_hours",
        "machine"
      ],
      "columns_missing": [
        "maintenance_date",
        "maintenance_type"
      ],
      "columns_extra": [
        "breakdown_date"
      ],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "maintenance_date",
        "maintenance_type",
        "machine",
        "cost_rupees",
        "downtime_hours"
      ],
      "generated_columns": [
        "breakdown_date",
        "cost_rupees",
        "downtime_hours",
        "machine"
      ]
    },
    "total_latency_ms": 2047.5900173187256,
    "total_tokens": 1438,
    "errors": [],
    "timestamp": "2025-11-30T13:37:35.309390"
  },
  {
    "question_id": "Easy_14",
    "question_text": "What is the total wastage in kilograms for 'Steel Sheet 101' this year?",
    "category": "Easy",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Open the inventory_logs table and filter the Material_Name column to keep only rows where Material_Name = 'Steel Sheet 101'.\n\n2. Within the filtered data, keep only the rows whose Date falls in the current calendar year (e.g., for 2024 keep rows with Date between 2024-01-01 and 2024-12-31).\n\n3. Sum the Wastage_Kg column of the remaining rows to obtain the total wastage for 'Steel Sheet 101' this year.\n```",
    "sql_response": "```sql\nSELECT SUM(Wastage_Kg) AS Total_Wastage_Kg\nFROM inventory_logs\nWHERE Material_Name = 'Steel Sheet 101'\n  AND Date >= DATE_TRUNC('year', CURRENT_DATE)\n  AND Date < DATE_TRUNC('year', CURRENT_DATE) + INTERVAL '1 year';\n```",
    "table_selection_response": "{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Material_Name\", \"Wastage_Kg\"]\n}",
    "methodology_score": 100,
    "sql_score": 72.1067415730337,
    "table_column_score": 88.0,
    "response_quality_score": 98.19662594795227,
    "overall_score": 87.05702214535702,
    "methodology_details": {
      "similarity_score": 100,
      "matching_concepts": [
        "Filter inventory_logs for Material_Name containing 'Steel Sheet 101'",
        "Filter for dates this year",
        "Sum the Wastage_Kg column"
      ],
      "missing_concepts": [],
      "extra_concepts": [],
      "reasoning": "The LLM's steps perfectly match the expected steps. It correctly identifies the table to use, the filtering conditions, and the aggregation function."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 50.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 64.04494382022472,
      "overall_score": 72.1067415730337,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg"
      ],
      "generated_columns": [
        "wastage_kg",
        "material_name"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 88.0,
      "table_score": 100.0,
      "column_score": 80.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 66.66666666666666,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "wastage_kg",
        "material_name"
      ],
      "columns_missing": [
        "date"
      ],
      "columns_extra": [],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "material_name",
        "date"
      ],
      "generated_columns": [
        "wastage_kg",
        "material_name"
      ]
    },
    "total_latency_ms": 1803.3740520477295,
    "total_tokens": 1324,
    "errors": [],
    "timestamp": "2025-11-30T13:37:41.481774"
  },
  {
    "question_id": "Complex_18",
    "question_text": "What is the average unit cost of materials purchased from each supplier, and which supplier has the lowest average cost?",
    "category": "Complex",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Use only the inventory_logs table; no joins are required.\n2. Filter out any rows where Received_Kg = 0 (or NULL) to keep only actual purchases.\n3. For every supplier, aggregate:\n   a. Total quantity received: SUM(Received_Kg)  \n   b. Total purchase value: SUM(Received_Kg × Unit_Cost_Rupees)\n4. Compute the average unit cost per supplier:  \n   Average_Unit_Cost = Total purchase value ÷ Total quantity received.\n5. Rank suppliers by Average_Unit_Cost in ascending order; the first-ranked supplier is the one with the lowest average cost.\n```",
    "sql_response": "```sql\nSELECT \n    Supplier,\n    ROUND(AVG(Unit_Cost_Rupees), 2) AS Avg_Unit_Cost_Rupees\nFROM inventory_logs\nGROUP BY Supplier\nORDER BY Avg_Unit_Cost_Rupees ASC\nLIMIT 1;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Supplier\", \"Unit_Cost_Rupees\"]\n}\n```",
    "methodology_score": 70.0,
    "sql_score": 80.4632972322503,
    "table_column_score": 100.0,
    "response_quality_score": 97.91488099098206,
    "overall_score": 83.9536421303858,
    "methodology_details": {
      "similarity_score": 70.0,
      "matching_concepts": [
        "Use only the inventory_logs table",
        "Grouping by Supplier",
        "Calculate average unit cost",
        "Identify supplier with the lowest average cost"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Filtering out Received_Kg = 0",
        "Calculating Total quantity received and Total purchase value before calculating the average"
      ],
      "reasoning": "The LLM's steps are mostly correct. It correctly identifies the table to use, the grouping operation, and the calculation of average unit cost. The LLM adds extra steps like filtering out rows where Received_Kg = 0 which isn't strictly necessary to get the average cost, but is good practice for data cleaning and ensuring accurate calculations. The way of calculating the average, by first calculating the totals, is equivalent but more detailed than the ground truth. The final step of ordering the results to find the lowest cost supplier is also correct. Overall, the approach is slightly different but achieves the same goal."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 33.33333333333333,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 80.86642599277978,
      "overall_score": 80.4632972322503,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "unit_cost_rupees",
        "supplier",
        "avgunitcost"
      ],
      "generated_columns": [
        "avg_unit_cost_rupees",
        "unit_cost_rupees",
        "2",
        "supplier",
        "roundunit_cost_rupees"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "unit_cost_rupees",
        "supplier"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "unit_cost_rupees",
        "supplier"
      ],
      "generated_columns": [
        "unit_cost_rupees",
        "supplier"
      ]
    },
    "total_latency_ms": 2085.1190090179443,
    "total_tokens": 1359,
    "errors": [],
    "timestamp": "2025-11-30T13:37:48.930170"
  },
  {
    "question_id": "Complex_1",
    "question_text": "What is the average downtime (in minutes) per shift for each machine in the last quarter?",
    "category": "Complex",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Identify the date range for “last quarter” (e.g., if today is 2024-04-15, the last quarter is 2024-01-01 to 2024-03-31).\n\n2. From production_logs, filter rows where Date falls within the last-quarter date range.\n\n3. Group the filtered data by the composite key: Line_Machine, Shift, Date (because the same machine can run multiple shifts per day).\n\n4. Within each group, sum Downtime_Minutes to get total downtime per shift per machine per day.\n\n5. Aggregate again by Line_Machine and Shift to compute the average of the summed downtime values across all days in the quarter.\n\n6. Round the resulting average downtime to the desired precision (e.g., two decimals) and present the list with columns: Line_Machine, Shift, Avg_Downtime_Minutes_Last_Quarter.\n```",
    "sql_response": "```sql\nSELECT\n    Line_Machine,\n    Shift,\n    ROUND(AVG(Downtime_Minutes), 2) AS Avg_Downtime_Minutes\nFROM production_logs\nWHERE Date >= DATE_SUB(CURDATE(), INTERVAL 3 MONTH)\nGROUP BY Line_Machine, Shift\nORDER BY Line_Machine, Shift;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"production_logs\"],\n  \"columns\": [\"Date\", \"Shift\", \"Line_Machine\", \"Downtime_Minutes\"]\n}\n```",
    "methodology_score": 80.0,
    "sql_score": 76.71428571428572,
    "table_column_score": 100.0,
    "response_quality_score": 97.82674932479858,
    "overall_score": 85.63267493247986,
    "methodology_details": {
      "similarity_score": 80.0,
      "matching_concepts": [
        "Filter production_logs for the last 3 months.",
        "Group the data by Line_Machine and Shift.",
        "Calculate the average Downtime_Minutes for each group."
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Identify the date range for “last quarter”.",
        "Group the filtered data by Date (because the same machine can run multiple shifts per day).",
        "Sum Downtime_Minutes to get total downtime per shift per machine per day.",
        "Aggregate again by Line_Machine and Shift to compute the average of the summed downtime values across all days in the quarter.",
        "Round the resulting average downtime to the desired precision (e.g., two decimals) and present the list with columns: Line_Machine, Shift, Avg_Downtime_Minutes_Last_Quarter."
      ],
      "reasoning": "The LLM correctly identifies the data source and required filtering and grouping. The difference lies in the detailed approach to calculating the average. LLM initially groups by Date, then sums, and then averages again. This is a more complex but valid approach. The expected steps directly calculates the average after grouping by Line_Machine and Shift. Both accomplish the same result. Adding date in the first grouping step is unnecessary and adds complexity but is not inherently wrong. The last two steps on aggregating and rounding are implicit in ground truth."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 42.857142857142854,
      "aggregate_match": 100.0,
      "clause_match": 80.0,
      "fuzzy_score": 60.0,
      "overall_score": 76.71428571428572,
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "pl",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "rounddowntime_minutes",
        "2",
        "shift"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ]
    },
    "total_latency_ms": 2173.250675201416,
    "total_tokens": 1421,
    "errors": [],
    "timestamp": "2025-11-30T13:37:56.715524"
  },
  {
    "question_id": "Easy_12",
    "question_text": "How many breakdown maintenance events occurred in the last quarter?",
    "category": "Easy",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Identify the relevant table: Use only the maintenance_logs table since the question is specifically about breakdown maintenance events.\n\n2. Define the last quarter date range: Determine the start and end dates for the last quarter (e.g., if today is 2024-04-15, the last quarter would be 2024-01-01 to 2024-03-31).\n\n3. Filter for breakdown maintenance: Apply a filter to select only rows where Maintenance_Type = 'Breakdown' (assuming this is the value used to identify breakdown maintenance events).\n\n4. Filter for last quarter: Apply a date filter to select only rows where Maintenance_Date falls within the last quarter date range.\n\n5. Count the events: Count the number of rows that meet both the Maintenance_Type and date criteria to get the total number of breakdown maintenance events in the last quarter.\n```",
    "sql_response": "```sql\nSELECT COUNT(*) AS Breakdown_Events\nFROM maintenance_logs\nWHERE Maintenance_Type = 'Breakdown'\n  AND Maintenance_Date >= DATE_SUB(CURDATE(), INTERVAL 3 MONTH);\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Maintenance_Date\", \"Maintenance_Type\"]\n}\n```",
    "methodology_score": 100,
    "sql_score": 96.88888888888889,
    "table_column_score": 70.0,
    "response_quality_score": 97.93063116073608,
    "overall_score": 91.20417422718471,
    "methodology_details": {
      "similarity_score": 100,
      "matching_concepts": [
        "Use maintenance_logs table",
        "Filter Maintenance_Type = 'Breakdown'",
        "Filter for dates in the last quarter/3 months",
        "Count the number of rows/events"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Define the last quarter date range"
      ],
      "reasoning": "The LLM's steps are logically equivalent to the expected steps. The LLM's addition of 'Define the last quarter date range' is just a more explicit way of explaining the date filtering step, which is acceptable. All core steps are present and correct."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 79.25925925925927,
      "overall_score": 96.88888888888889,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "maintenance_type"
      ],
      "generated_columns": [
        "maintenance_type"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 70.0,
      "table_score": 100.0,
      "column_score": 50.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 50.0,
      "column_recall": 50.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "maintenance_type"
      ],
      "columns_missing": [
        "breakdown_date"
      ],
      "columns_extra": [
        "maintenance_date"
      ],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "breakdown_date",
        "maintenance_type"
      ],
      "generated_columns": [
        "maintenance_date",
        "maintenance_type"
      ]
    },
    "total_latency_ms": 2069.368839263916,
    "total_tokens": 1347,
    "errors": [],
    "timestamp": "2025-11-30T13:38:03.432519"
  },
  {
    "question_id": "Complex_7",
    "question_text": "What are the top 3 defect types that lead to rework for Widget-B, and what is their total rework count?",
    "category": "Complex",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Filter the quality_control table to keep only rows where Product = 'Widget-B'.\n2. Within this filtered set, keep only rows where Rework_Count > 0 (i.e., defects that actually triggered rework).\n3. Group the data by Defect_Type.\n4. Sum the Rework_Count for each Defect_Type to get the total rework incidents per defect type.\n5. Sort the resulting summary in descending order of total rework count.\n6. Select the top 3 rows (highest rework counts) and list their Defect_Type along with the corresponding total rework count.\n```",
    "sql_response": "```sql\nSELECT \n    Defect_Type,\n    SUM(Rework_Count) AS Total_Rework\nFROM quality_control\nWHERE Product = 'Widget-B'\nGROUP BY Defect_Type\nORDER BY Total_Rework DESC\nLIMIT 3;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Product\", \"Defect_Type\", \"Rework_Count\"]\n}\n```",
    "methodology_score": 90.0,
    "sql_score": 89.39628482972137,
    "table_column_score": 100.0,
    "response_quality_score": 98.13512778282166,
    "overall_score": 93.10221246868466,
    "methodology_details": {
      "similarity_score": 90.0,
      "matching_concepts": [
        "Filter quality_control for Product = 'Widget-B'",
        "Group the data by Defect_Type",
        "Calculate the sum of Rework_Count for each defect type",
        "Order the results in descending order of total rework count",
        "Select the top 3 defect types and their corresponding total rework counts"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Keep only rows where Rework_Count > 0"
      ],
      "reasoning": "The LLM-generated steps are mostly correct and align with the expected steps. The only difference is the addition of a filter for `Rework_Count > 0`. This filter is redundant, as the aggregation inherently only sums the Rework_Counts that are greater than zero. However, it doesn't introduce any errors, just an unnecessary step."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 60.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 95.97523219814241,
      "overall_score": 89.39628482972137,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "defect_type",
        "total_rework_count",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "defect_type",
        "total_rework",
        "rework_count"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "defect_type",
        "rework_count"
      ]
    },
    "total_latency_ms": 1864.8722171783447,
    "total_tokens": 1344,
    "errors": [],
    "timestamp": "2025-11-30T13:38:10.329991"
  },
  {
    "question_id": "Medium_21",
    "question_text": "What is the average downtime per shift for Machine-M1 in October 2026?",
    "category": "Medium",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Open the production_logs table and keep only rows where Line_Machine = 'Machine-M1'.\n2. Filter the remaining rows to the date range 1-Oct-2026 to 31-Oct-2026 (inclusive).\n3. Group the filtered data by Shift (Day, Night, or whatever values exist).\n4. Within each shift group, sum the Downtime_Minutes column to get total downtime for that shift.\n5. Count the number of production records in each shift to obtain the number of times that shift occurred in October 2026.\n6. For each shift, divide the total downtime (from step 4) by the count of records (from step 5) to compute average downtime per occurrence of that shift.\n7. Present the result: Shift name and its corresponding average downtime in minutes.\n```",
    "sql_response": "```sql\nSELECT\n    Shift,\n    ROUND(AVG(Downtime_Minutes), 2) AS Avg_Downtime_Minutes\nFROM production_logs\nWHERE Line_Machine = 'Line-1/Machine-M1'\n  AND Date BETWEEN '2026-10-01' AND '2026-10-31'\nGROUP BY Shift\nORDER BY Shift;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"production_logs\"],\n  \"columns\": [\"Date\", \"Shift\", \"Line_Machine\", \"Downtime_Minutes\"]\n}\n```",
    "methodology_score": 100,
    "sql_score": 77.20172910662824,
    "table_column_score": 100.0,
    "response_quality_score": 97.80901408195496,
    "overall_score": 91.80150659551538,
    "methodology_details": {
      "similarity_score": 100,
      "matching_concepts": [
        "Filter production_logs for Line_Machine containing 'Machine-M1'",
        "Filter for dates in October 2026",
        "Group by Shift",
        "Calculate the average of Downtime_Minutes for each shift"
      ],
      "missing_concepts": [],
      "extra_concepts": [],
      "reasoning": "The LLM's steps are logically equivalent to the expected steps. Although the LLM breaks down the calculation of the average downtime into summing the downtime and dividing by the number of occurrences of that shift, it arrives at the same result as directly calculating the average. Therefore, the steps are all correct and logically aligned."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 40.0,
      "aggregate_match": 100.0,
      "clause_match": 80.0,
      "fuzzy_score": 68.0115273775216,
      "overall_score": 77.20172910662824,
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "downtime_minutes",
        "rounddowntime_minutes",
        "2",
        "shift"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ]
    },
    "total_latency_ms": 2190.985918045044,
    "total_tokens": 1405,
    "errors": [],
    "timestamp": "2025-11-30T13:38:17.307310"
  },
  {
    "question_id": "Easy_5",
    "question_text": "How many breakdown maintenances were performed on Machine-M2 in Q3 2026?",
    "category": "Easy",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Use only the maintenance_logs table – no joins are required.\n2. Filter the table to Machine = 'Machine-M2'.\n3. Filter Maintenance_Type = 'Breakdown' (only breakdown events are counted, not preventive or scheduled).\n4. Extract the quarter and year from Maintenance_Date: keep rows where the date falls between 2026-07-01 and 2026-09-30 (Q3 2026).\n5. Count the number of rows that remain after the above filters; this count is the number of breakdown maintenances performed on Machine-M2 in Q3 2026.\n```",
    "sql_response": "```sql\nSELECT COUNT(*) AS Breakdown_Count\nFROM maintenance_logs\nWHERE Machine = 'Machine-M2'\n  AND Maintenance_Type = 'Breakdown'\n  AND Maintenance_Date >= '2026-07-01'\n  AND Maintenance_Date <= '2026-09-30';\n```",
    "table_selection_response": "{\"tables\": [\"maintenance_logs\"], \"columns\": [\"Machine\", \"Maintenance_Type\", \"Breakdown_Date\"]}",
    "methodology_score": 100,
    "sql_score": 97.42744063324538,
    "table_column_score": 80.0,
    "response_quality_score": 98.12465643882751,
    "overall_score": 93.91206986551862,
    "methodology_details": {
      "similarity_score": 100,
      "matching_concepts": [
        "Filtering maintenance_logs for Machine = 'Machine-M2'",
        "Filtering for Maintenance_Type = 'Breakdown'",
        "Filtering for dates between July 1, 2026 and September 30, 2026",
        "Counting the number of rows"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Use only the maintenance_logs table – no joins are required.",
        "Extract the quarter and year from Maintenance_Date: keep rows where the date falls between 2026-07-01 and 2026-09-30 (Q3 2026)."
      ],
      "reasoning": "The LLM's steps are logically equivalent to the ground truth steps. The extra steps it includes are helpful context and do not detract from the solution."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 82.84960422163589,
      "overall_score": 97.42744063324538,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "machine"
      ],
      "generated_columns": [
        "machine"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 80.0,
      "table_score": 100.0,
      "column_score": 66.66666666666666,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 66.66666666666666,
      "column_recall": 66.66666666666666,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "maintenance_type",
        "machine"
      ],
      "columns_missing": [
        "maintenance_date"
      ],
      "columns_extra": [
        "breakdown_date"
      ],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "maintenance_date",
        "maintenance_type",
        "machine"
      ],
      "generated_columns": [
        "breakdown_date",
        "maintenance_type",
        "machine"
      ]
    },
    "total_latency_ms": 1875.3435611724854,
    "total_tokens": 1337,
    "errors": [],
    "timestamp": "2025-11-30T13:38:24.052222"
  },
  {
    "question_id": "Complex_33",
    "question_text": "What are the top 3 most frequently replaced parts across all machines, and what is the total cost associated with replacing these parts?",
    "category": "Complex",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. From maintenance_logs, filter the dataset to include only rows where Parts_Replaced is NOT NULL and Parts_Replaced ≠ '' (i.e., at least one part was replaced).\n\n2. Split the Parts_Replaced field (which may contain comma-separated values) into individual part names; create one row per part per maintenance record.\n\n3. Group by the extracted part name and aggregate:\n   - Count of replacement occurrences (frequency)\n   - Sum of Cost_Rupees (total cost for all replacements of that part)\n\n4. Sort the aggregated results in descending order by frequency (count of replacements).\n\n5. Select the top 3 rows (highest frequency) to identify the most frequently replaced parts and their associated total costs.\n```",
    "sql_response": "```sql\nSELECT \n    Parts_Replaced AS Part,\n    COUNT(*) AS Replacement_Count,\n    SUM(Cost_Rupees) AS Total_Cost_Rupees\nFROM maintenance_logs\nWHERE Parts_Replaced IS NOT NULL\n  AND TRIM(Parts_Replaced) <> ''\nGROUP BY Parts_Replaced\nORDER BY Replacement_Count DESC\nLIMIT 3;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"maintenance_logs\"],\n  \"columns\": [\"Parts_Replaced\", \"Cost_Rupees\"]\n}\n```",
    "methodology_score": 90.0,
    "sql_score": 82.07228915662651,
    "table_column_score": 100.0,
    "response_quality_score": 97.81918621063232,
    "overall_score": 90.50721982588252,
    "methodology_details": {
      "similarity_score": 90.0,
      "matching_concepts": [
        "Group maintenance_logs data by Parts_Replaced",
        "Count the occurrences of each Parts_Replaced (ReplacementCount)",
        "Calculate the total cost (SUM(Cost_Rupees)) for each Parts_Replaced group",
        "Order the results by ReplacementCount in descending order",
        "Select the top 3 Parts_Replaced entries with their counts and total costs."
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "filter the dataset to include only rows where Parts_Replaced is NOT NULL and Parts_Replaced ≠ ''",
        "Split the Parts_Replaced field (which may contain comma-separated values) into individual part names; create one row per part per maintenance record."
      ],
      "reasoning": "The LLM's steps closely align with the expected steps. The core logic of grouping, counting, calculating cost, sorting, and selecting the top 3 is correctly implemented. The LLM adds steps to handle comma-separated values in 'Parts_Replaced', which is a good consideration and doesn't deviate from the goal. The filtering step is also sensible, though it could be incorporated directly into the grouping/counting if the source data is clean."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 50.0,
      "aggregate_match": 100.0,
      "clause_match": 83.33333333333334,
      "fuzzy_score": 80.48192771084337,
      "overall_score": 82.07228915662651,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "replacementcount",
        "parts_replaced"
      ],
      "generated_columns": [
        "cost_rupees",
        "parts_replaced",
        "replacement_count"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "cost_rupees",
        "parts_replaced"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "parts_replaced"
      ],
      "generated_columns": [
        "cost_rupees",
        "parts_replaced"
      ]
    },
    "total_latency_ms": 2180.813789367676,
    "total_tokens": 1402,
    "errors": [],
    "timestamp": "2025-11-30T13:38:31.515387"
  },
  {
    "question_id": "Complex_23",
    "question_text": "What is the correlation between Downtime_Minutes and Failed_Qty on a daily basis?",
    "category": "Complex",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. From production_logs, aggregate Downtime_Minutes by Date:  \n   SELECT Date, SUM(Downtime_Minutes) AS Total_Downtime  \n   FROM production_logs  \n   GROUP BY Date;\n\n2. From quality_control, aggregate Failed_Qty by Date:  \n   SELECT Inspection_Date AS Date, SUM(Failed_Qty) AS Total_Failed  \n   FROM quality_control  \n   GROUP BY Inspection_Date;\n\n3. Join the two daily aggregates on Date to create a single daily table with columns Date, Total_Downtime, Total_Failed.\n\n4. Filter out any dates where either Total_Downtime or Total_Failed is NULL or zero to avoid spurious correlations.\n\n5. Calculate the Pearson correlation coefficient between the series Total_Downtime and Total_Failed across the remaining dates (in Excel: =CORREL(...), in SQL use a window function or export to a stats tool).\n\n6. (Optional) Plot Total_Downtime vs Total_Failed to visually inspect the linear relationship and validate the correlation value.\n```",
    "sql_response": "```sql\nSELECT\n    p.Date,\n    SUM(p.Downtime_Minutes) AS Total_Downtime_Minutes,\n    SUM(COALESCE(q.Failed_Qty,0)) AS Total_Failed_Qty,\n    CORR(SUM(p.Downtime_Minutes), SUM(COALESCE(q.Failed_Qty,0))) OVER () AS Correlation_Coeff\nFROM production_logs AS p\nLEFT JOIN quality_control AS q\n    ON p.Date = q.Inspection_Date\n    AND SUBSTRING_INDEX(p.Line_Machine,'/',1) = q.Line\nGROUP BY p.Date\nORDER BY p.Date;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"production_logs\", \"quality_control\"],\n  \"columns\": [\"Date\", \"Downtime_Minutes\", \"Failed_Qty\"]\n}\n```",
    "methodology_score": 90.0,
    "sql_score": 72.87878787878788,
    "table_column_score": 91.42857142857143,
    "response_quality_score": 97.60581874847412,
    "overall_score": 85.12530048956603,
    "methodology_details": {
      "similarity_score": 90.0,
      "matching_concepts": [
        "Join production_logs and quality_control on Date",
        "Group by Date",
        "Calculate the sum of Downtime_Minutes and Failed_Qty for each date",
        "Calculate the Pearson correlation coefficient between TotalDowntime and TotalFailures"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Filter out any dates where either Total_Downtime or Total_Failed is NULL or zero to avoid spurious correlations.",
        "(Optional) Plot Total_Downtime vs Total_Failed to visually inspect the linear relationship and validate the correlation value."
      ],
      "reasoning": "The LLM successfully identifies all the necessary steps, including joining the tables, grouping by date, summing the relevant columns, and calculating the Pearson correlation. The LLM also provides additional filtering and validation steps that aren't strictly required but are good practice, improving the overall quality of the solution. The only difference is that the LLM provides additional good-practice steps."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 33.33333333333333,
      "aggregate_match": 100.0,
      "clause_match": 66.66666666666666,
      "fuzzy_score": 63.63636363636363,
      "overall_score": 72.87878787878788,
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "pl",
        "corrtotaldowntime",
        "date",
        "downtime_minutes",
        "failed_qty",
        "totalfailures"
      ],
      "generated_columns": [
        "p",
        "0",
        "date",
        "downtime_minutes",
        "0 over",
        "failed_qty"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 91.42857142857143,
      "table_score": 100.0,
      "column_score": 85.71428571428571,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 75.0,
      "tables_correct": [
        "production_logs",
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "date",
        "downtime_minutes",
        "failed_qty"
      ],
      "columns_missing": [
        "inspection_date"
      ],
      "columns_extra": [],
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "inspection_date",
        "date",
        "downtime_minutes",
        "failed_qty"
      ],
      "generated_columns": [
        "date",
        "downtime_minutes",
        "failed_qty"
      ]
    },
    "total_latency_ms": 2394.181251525879,
    "total_tokens": 1505,
    "errors": [],
    "timestamp": "2025-11-30T13:38:39.063503"
  },
  {
    "question_id": "Easy_28",
    "question_text": "What is the total cost of parts replaced for Machine-M3?",
    "category": "Easy",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Open the maintenance_logs table and filter the column Machine to keep only rows where Machine = \"Machine-M3\".\n2. From the filtered result, select only the column Parts_Replaced and Cost_Rupees.\n3. For every row, parse the Parts_Replaced field to identify each individual part (comma-separated or listed) and note the corresponding Cost_Rupees for that maintenance record.\n4. Sum the Cost_Rupees values across all rows that have any parts listed (ignore rows with blank or “NA” in Parts_Replaced).\n5. Report the resulting sum as the total cost of parts replaced for Machine-M3.\n```",
    "sql_response": "```sql\nSELECT SUM(Cost_Rupees) AS Total_Parts_Cost\nFROM maintenance_logs\nWHERE Machine = 'Machine-M3'\n  AND Parts_Replaced IS NOT NULL\n  AND Parts_Replaced <> '';\n```",
    "table_selection_response": "{\"tables\": [\"maintenance_logs\"], \"columns\": [\"Machine\", \"Parts_Replaced\", \"Cost_Rupees\"]}",
    "methodology_score": 70.0,
    "sql_score": 84.86301369863014,
    "table_column_score": 88.0,
    "response_quality_score": 98.07426309585571,
    "overall_score": 82.50948110410613,
    "methodology_details": {
      "similarity_score": 70.0,
      "matching_concepts": [
        "Filter maintenance_logs for Machine = 'Machine-M3'",
        "Sum the Cost_Rupees column"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "select only the column Parts_Replaced and Cost_Rupees",
        "parse the Parts_Replaced field to identify each individual part",
        "ignore rows with blank or “NA” in Parts_Replaced"
      ],
      "reasoning": "The LLM correctly identifies the table and filtering criteria. It also correctly identifies that the Cost_Rupees needs to be summed. However, the LLM introduces unnecessary steps involving parsing the Parts_Replaced field, which is not required to calculate the total cost. The question asks for total cost of parts replaced, not cost of individual parts. This reduces the overall similarity."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 65.75342465753424,
      "overall_score": 84.86301369863014,
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "machine"
      ],
      "generated_columns": [
        "cost_rupees",
        "machine"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 88.0,
      "table_score": 100.0,
      "column_score": 80.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 66.66666666666666,
      "column_recall": 100.0,
      "tables_correct": [
        "maintenance_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "cost_rupees",
        "machine"
      ],
      "columns_missing": [],
      "columns_extra": [
        "parts_replaced"
      ],
      "expected_tables": [
        "maintenance_logs"
      ],
      "generated_tables": [
        "maintenance_logs"
      ],
      "expected_columns": [
        "cost_rupees",
        "machine"
      ],
      "generated_columns": [
        "cost_rupees",
        "parts_replaced",
        "machine"
      ]
    },
    "total_latency_ms": 1925.736904144287,
    "total_tokens": 1313,
    "errors": [],
    "timestamp": "2025-11-30T13:38:46.135260"
  },
  {
    "question_id": "Easy_30",
    "question_text": "How much total waste was recorded in 2025?",
    "category": "Easy",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Identify the relevant table: Use only the **inventory_logs** table, as it contains the field **Wastage_Kg** which directly records waste.\n\n2. Filter the data: Apply a date filter to keep only records where **Date** falls within the year 2025 (i.e., Date between 2025-01-01 and 2025-12-31).\n\n3. Aggregate the waste: Sum the **Wastage_Kg** column across all filtered records to obtain the total waste recorded in 2025.\n```",
    "sql_response": "```sql\nSELECT SUM(Wastage_Kg) AS Total_Waste_Kg\nFROM inventory_logs\nWHERE YEAR(Date) = 2025;\n```",
    "table_selection_response": "{\"tables\": [\"inventory_logs\"], \"columns\": [\"Wastage_Kg\"]}",
    "methodology_score": 100,
    "sql_score": 84.76744186046511,
    "table_column_score": 80.0,
    "response_quality_score": 98.19097852706909,
    "overall_score": 89.48770250386968,
    "methodology_details": {
      "similarity_score": 100,
      "matching_concepts": [
        "Use inventory_logs table",
        "Filter for the year 2025",
        "Sum the Wastage_Kg column"
      ],
      "missing_concepts": [],
      "extra_concepts": [],
      "reasoning": "The LLM generated steps perfectly match the expected steps, including the table used, the filtering criteria, and the aggregation operation. The wording is slightly different but the logic is identical."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 65.11627906976744,
      "overall_score": 84.76744186046511,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg"
      ],
      "generated_columns": [
        "wastage_kg"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 80.0,
      "table_score": 100.0,
      "column_score": 66.66666666666666,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 50.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "wastage_kg"
      ],
      "columns_missing": [
        "date"
      ],
      "columns_extra": [],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "date"
      ],
      "generated_columns": [
        "wastage_kg"
      ]
    },
    "total_latency_ms": 1809.0214729309082,
    "total_tokens": 1266,
    "errors": [],
    "timestamp": "2025-11-30T13:38:52.422209"
  },
  {
    "question_id": "Medium_31",
    "question_text": "What is the average downtime (in minutes) per shift for Machine-M1 on Line-1 during the last quarter?",
    "category": "Medium",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Identify the relevant table: Use only the production_logs table, as it contains both the downtime and the shift information needed.\n\n2. Filter the data:\n   - Filter for Machine = 'Machine-M1'\n   - Filter for Line_Machine = 'Line-1'\n   - Filter for Date within the last quarter (e.g., if today is 2024-04-01, use dates from 2024-01-01 to 2024-03-31)\n\n3. Group the filtered data by Shift (e.g., Shift-A, Shift-B, Shift-C) to prepare for per-shift calculations.\n\n4. For each shift, sum the Downtime_Minutes to get the total downtime per shift.\n\n5. Count the number of records (production days) for each shift to determine how many times each shift occurred in the quarter.\n\n6. Calculate the average downtime per shift by dividing the total downtime for each shift by the number of times that shift occurred.\n\n7. Optionally, compute an overall average across all shifts by summing all Downtime_Minutes and dividing by the total number of records (days) in the filtered dataset.\n```",
    "sql_response": "```sql\nSELECT\n    Shift,\n    AVG(Downtime_Minutes) AS Avg_Downtime_Minutes\nFROM production_logs\nWHERE Line_Machine = 'Line-1/Machine-M1'\n  AND Date >= DATE_TRUNC('quarter', CURRENT_DATE) - INTERVAL '3 months'\n  AND Date < DATE_TRUNC('quarter', CURRENT_DATE)\nGROUP BY Shift\nORDER BY Shift;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"production_logs\"],\n  \"columns\": [\"Date\", \"Shift\", \"Line_Machine\", \"Downtime_Minutes\"]\n}\n```",
    "methodology_score": 85.0,
    "sql_score": 92.70921985815603,
    "table_column_score": 100.0,
    "response_quality_score": 97.78111481666565,
    "overall_score": 92.72633843202118,
    "methodology_details": {
      "similarity_score": 85.0,
      "matching_concepts": [
        "Use the production_logs table",
        "Filter for Line_Machine = 'Line-1/Machine-M1'",
        "Filter for dates in the last 3 months (approximately a quarter)",
        "Group by Shift",
        "Calculate the average Downtime_Minutes for each shift"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Sum the Downtime_Minutes to get the total downtime per shift.",
        "Count the number of records (production days) for each shift to determine how many times each shift occurred in the quarter.",
        "Optionally, compute an overall average across all shifts by summing all Downtime_Minutes and dividing by the total number of records (days) in the filtered dataset."
      ],
      "reasoning": "The LLM correctly identifies the table and filtering criteria. It also correctly identifies the need to group by shift and calculate an average downtime. The steps it outlines to calculate average downtime are valid and are just a more granular breakdown of step 4 in the ground truth. The LLM is penalized slightly for the extra steps, even though they are technically correct. Step 2 in the LLM response has a minor mistake where the conditions for machine and line are separated into 2 different filters. These are logically equivalent, but the Ground Truth uses a Line_Machine filter."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 80.0,
      "fuzzy_score": 71.39479905437352,
      "overall_score": 92.70921985815603,
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "downtime_minutes",
        "shift"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ],
      "generated_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "shift"
      ]
    },
    "total_latency_ms": 2218.8851833343506,
    "total_tokens": 1500,
    "errors": [],
    "timestamp": "2025-11-30T13:39:00.300805"
  },
  {
    "question_id": "Complex_9",
    "question_text": "Which supplier provides the material with the lowest unit cost, considering only materials consumed after 2026-01-01?",
    "category": "Complex",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. From inventory_logs, filter rows where Date > '2026-01-01' to keep only consumption records after 1-Jan-2026.\n2. Filter the same result set to keep only rows where Consumption_Kg > 0, so we consider only materials that were actually consumed.\n3. Group the filtered data by Material_Code, Material_Name, Supplier, and Unit_Cost_Rupees.\n4. Within each group, sum Consumption_Kg to get total consumption per (Material, Supplier, Unit-Cost) combination.\n5. Sort the grouped result ascending on Unit_Cost_Rupees.\n6. Select the first row of the sorted list; its Supplier column gives the supplier with the lowest unit cost among materials consumed after 2026-01-01.\n```",
    "sql_response": "```sql\nSELECT Supplier\nFROM inventory_logs\nWHERE Date > '2026-01-01'\n  AND Consumption_Kg > 0\nORDER BY Unit_Cost_Rupees ASC\nLIMIT 1;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Material_Code\", \"Material_Name\", \"Consumption_Kg\", \"Supplier\", \"Unit_Cost_Rupees\", \"Date\"]\n}\n```",
    "methodology_score": 70.0,
    "sql_score": 61.47959183673469,
    "table_column_score": 88.0,
    "response_quality_score": 97.91982507705688,
    "overall_score": 74.30983965056282,
    "methodology_details": {
      "similarity_score": 70.0,
      "matching_concepts": [
        "Filter for materials consumed after 2026-01-01",
        "Group the data by supplier and material name.",
        "Order the data by minimum unit cost and take the first row"
      ],
      "missing_concepts": [
        "Identify the minimum unit cost for each supplier and material"
      ],
      "extra_concepts": [
        "Filter the same result set to keep only rows where Consumption_Kg > 0",
        "Within each group, sum Consumption_Kg to get total consumption per (Material, Supplier, Unit-Cost) combination.",
        "Grouping by Material_Code"
      ],
      "reasoning": "The LLM correctly filters by date and groups by supplier and material. However, it attempts to sum consumption and filter by it, which isn't strictly necessary to identify the minimum unit cost. More importantly, the LLM doesn't explicitly calculate or isolate the minimum unit cost before ordering. It also includes Material_Code in the group by, which is redundant. Summing consumption may be useful in some contexts, but is not specified in the expected steps and is more complex than needed. The approach of ordering by Unit_Cost_Rupees and selecting the first row might give the wrong answer in the presence of different unit costs for the same material from the same supplier."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 60.0,
      "aggregate_match": 0.0,
      "clause_match": 83.33333333333334,
      "fuzzy_score": 59.863945578231295,
      "overall_score": 61.47959183673469,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_name",
        "min",
        "date",
        "unit_cost_rupees",
        "supplier"
      ],
      "generated_columns": [
        "unit_cost_rupees",
        "supplier",
        "date"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 88.0,
      "table_score": 100.0,
      "column_score": 80.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 66.66666666666666,
      "column_recall": 100.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "material_name",
        "supplier",
        "date",
        "unit_cost_rupees"
      ],
      "columns_missing": [],
      "columns_extra": [
        "material_code",
        "consumption_kg"
      ],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_name",
        "supplier",
        "date",
        "unit_cost_rupees"
      ],
      "generated_columns": [
        "material_code",
        "material_name",
        "date",
        "unit_cost_rupees",
        "consumption_kg",
        "supplier"
      ]
    },
    "total_latency_ms": 2080.1749229431152,
    "total_tokens": 1396,
    "errors": [],
    "timestamp": "2025-11-30T13:39:07.880841"
  },
  {
    "question_id": "Complex_19",
    "question_text": "How does the 'Downtime_Minutes' in production relate to the 'Downtime_Hours' in maintenance logs when the 'Issue_Description' involves 'Motor overheating and failure to start'?",
    "category": "Complex",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Filter the maintenance_logs table to keep only rows where Issue_Description contains the phrase “Motor overheating and failure to start” (case-insensitive).  \n   – Keep columns: Maintenance_Date, Machine, Downtime_Hours.\n\n2. Create a daily-machine key by casting Maintenance_Date to DATE and trimming Machine to remove extra spaces.\n\n3. Aggregate the filtered maintenance data by this daily-machine key:  \n   – Sum(Downtime_Hours) → Total_Maintenance_Downtime_Hours.\n\n4. In production_logs, create the same daily-machine key by casting Date to DATE and trimming Line_Machine.\n\n5. Aggregate production_logs by this daily-machine key:  \n   – Sum(Downtime_Minutes) → Total_Production_Downtime_Minutes.\n\n6. Inner-join the two aggregated sets on the daily-machine key, keeping only days/machines that appear in both sets.\n\n7. Convert Total_Maintenance_Downtime_Hours to minutes (multiply by 60) to obtain Total_Maintenance_Downtime_Minutes.\n\n8. Compute the Pearson correlation coefficient between Total_Production_Downtime_Minutes and Total_Maintenance_Downtime_Minutes across the matched daily-machine records.\n\n9. Optionally, also compute the simple difference (Production_Downtime_Minutes – Maintenance_Downtime_Minutes) per record to inspect the residual downtime not explained by the motor issue.\n```",
    "sql_response": "```sql\nSELECT\n    p.Date,\n    p.Line_Machine,\n    p.Downtime_Minutes AS Production_Downtime_Min,\n    m.Downtime_Hours AS Maintenance_Downtime_Hrs,\n    ROUND(m.Downtime_Hours * 60, 2) AS Maintenance_Downtime_Min,\n    (p.Downtime_Minutes - ROUND(m.Downtime_Hours * 60, 2)) AS Downtime_Difference_Min\nFROM production_logs AS p\nJOIN maintenance_logs AS m\n    ON p.Date = m.Maintenance_Date\n    AND SUBSTRING_INDEX(p.Line_Machine, '/', -1) = m.Machine\nWHERE m.Issue_Description LIKE '%Motor overheating and failure to start%';\n```",
    "table_selection_response": "{\n  \"tables\": [\"production_logs\", \"maintenance_logs\"],\n  \"columns\": [\"production_logs.Date\", \"production_logs.Line_Machine\", \"production_logs.Downtime_Minutes\", \"maintenance_logs.Machine\", \"maintenance_logs.Downtime_Hours\", \"maintenance_logs.Issue_Description\"]\n}",
    "methodology_score": 60.0,
    "sql_score": 64.85315712187959,
    "table_column_score": 40.0,
    "response_quality_score": 96.92904782295227,
    "overall_score": 60.391509774953086,
    "methodology_details": {
      "similarity_score": 60.0,
      "matching_concepts": [
        "Filtering for Issue_Description = 'Motor overheating and failure to start'",
        "Using production_logs and maintenance_logs",
        "Aggregating Downtime_Minutes and Downtime_Hours"
      ],
      "missing_concepts": [
        "Joining production_logs and maintenance_logs on Date/Breakdown_Date",
        "Calculating the average Downtime_Minutes from production_logs",
        "Calculating the average Downtime_Hours from maintenance_logs",
        "Comparing the average Downtime_Minutes and Downtime_Hours"
      ],
      "extra_concepts": [
        "Creating a daily-machine key by casting Maintenance_Date/Date to DATE and trimming Machine/Line_Machine",
        "Inner-joining the two aggregated sets on the daily-machine key",
        "Converting Total_Maintenance_Downtime_Hours to minutes",
        "Computing the Pearson correlation coefficient",
        "Computing the simple difference (Production_Downtime_Minutes – Maintenance_Downtime_Minutes)"
      ],
      "reasoning": "The LLM's steps address the question but approach it differently. Instead of a direct comparison of averages after a simple join, the LLM introduces a daily-machine key, aggregates downtime based on this key, joins on the key, and calculates correlation. While this approach could provide more granular insights, it's significantly more complex than the expected steps, which focus on a simpler comparison of average downtime. The LLM misses calculating straightforward averages as requested by the original intent."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 33.33333333333333,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 43.46549192364171,
      "overall_score": 64.85315712187959,
      "expected_tables": [
        "maintenance_logs",
        "production_logs"
      ],
      "generated_tables": [
        "maintenance_logs",
        "production_logs"
      ],
      "expected_columns": [
        "downtime_hours",
        "downtime_minutes"
      ],
      "generated_columns": [
        "line_machine",
        "date",
        "downtime_minutes",
        "downtime_hours * 60",
        "2",
        "downtime_hours"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 40.0,
      "table_score": 100.0,
      "column_score": 0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 0.0,
      "column_recall": 0.0,
      "tables_correct": [
        "maintenance_logs",
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [],
      "columns_missing": [
        "date",
        "downtime_minutes",
        "breakdown_date",
        "issue_description",
        "downtime_hours"
      ],
      "columns_extra": [
        "maintenance_logs.downtime_hours",
        "production_logs.date",
        "production_logs.line_machine",
        "maintenance_logs.machine",
        "maintenance_logs.issue_description",
        "production_logs.downtime_minutes"
      ],
      "expected_tables": [
        "maintenance_logs",
        "production_logs"
      ],
      "generated_tables": [
        "maintenance_logs",
        "production_logs"
      ],
      "expected_columns": [
        "date",
        "downtime_minutes",
        "breakdown_date",
        "issue_description",
        "downtime_hours"
      ],
      "generated_columns": [
        "maintenance_logs.downtime_hours",
        "production_logs.line_machine",
        "production_logs.date",
        "maintenance_logs.machine",
        "maintenance_logs.issue_description",
        "production_logs.downtime_minutes"
      ]
    },
    "total_latency_ms": 3070.9521770477295,
    "total_tokens": 1704,
    "errors": [],
    "timestamp": "2025-11-30T13:39:16.797701"
  },
  {
    "question_id": "Complex_3",
    "question_text": "What is the correlation between downtime minutes and failed quantity per line machine and product?",
    "category": "Complex",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. From production_logs, aggregate downtime by Date, Line_Machine, Product, and Shift:  \n   `SUM(Downtime_Minutes) AS Total_Downtime` grouped by Date, Line_Machine, Product, Shift.\n\n2. From quality_control, aggregate failed quantity by the same keys:  \n   `SUM(Failed_Qty) AS Total_Failed_Qty` grouped by Inspection_Date (treat as Date), Line (map to Line_Machine), Product.\n\n3. Create a common key (Date, Line_Machine, Product) and join the two aggregated results on this key; discard rows where either Total_Downtime or Total_Failed_Qty is NULL or zero.\n\n4. Compute the Pearson correlation coefficient between Total_Downtime and Total_Failed_Qty across the joined dataset; optionally break the correlation by Line_Machine or by Product to see segment-level relationships.\n\n5. (Optional) Bin Total_Downtime into low/medium/high bands and calculate average Total_Failed_Qty per band to visualize the relationship.\n```",
    "sql_response": "```sql\nSELECT\n    pl.Line_Machine,\n    pl.Product,\n    SUM(pl.Downtime_Minutes) AS Total_Downtime_Minutes,\n    SUM(qc.Failed_Qty) AS Total_Failed_Qty,\n    CASE\n        WHEN SUM(pl.Downtime_Minutes) > 0 AND SUM(qc.Failed_Qty) > 0\n        THEN (SUM(pl.Downtime_Minutes) * 1.0 * SUM(qc.Failed_Qty)) / \n             (SQRT(SUM(pl.Downtime_Minutes) * SUM(pl.Downtime_Minutes)) * SQRT(SUM(qc.Failed_Qty) * SUM(qc.Failed_Qty)))\n        ELSE NULL\n    END AS Correlation_Coefficient\nFROM production_logs pl\nJOIN quality_control qc\n    ON pl.Date = qc.Inspection_Date\n    AND pl.Product = qc.Product\n    AND pl.Line_Machine LIKE CONCAT(qc.Line, '/%')\nGROUP BY pl.Line_Machine, pl.Product\nORDER BY pl.Line_Machine, pl.Product;\n```",
    "table_selection_response": "{\n  \"tables\": [\"production_logs\", \"quality_control\"],\n  \"columns\": [\"Line_Machine\", \"Product\", \"Downtime_Minutes\", \"Failed_Qty\"]\n}",
    "methodology_score": 75.0,
    "sql_score": 55.32302405498281,
    "table_column_score": 85.0,
    "response_quality_score": 97.26443290710449,
    "overall_score": 72.83950170995443,
    "methodology_details": {
      "similarity_score": 75.0,
      "matching_concepts": [
        "production_logs table",
        "quality_control table",
        "Aggregate downtime",
        "Aggregate failed quantity",
        "Join tables",
        "Calculate correlation"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Grouping by Date, Line_Machine, Product, and Shift",
        "Discard rows where Total_Downtime or Total_Failed_Qty is NULL or zero",
        "Bin Total_Downtime into low/medium/high bands and calculate average Total_Failed_Qty per band"
      ],
      "reasoning": "The LLM response is mostly aligned with the expected steps. It correctly identifies the tables involved and the core operations of aggregation, joining, and correlation calculation. However, the LLM introduces unnecessary aggregation by 'Shift' which is not specified in the original prompt. It also suggests additional filtering and binning which, while potentially helpful for further analysis, are not part of the core requirement stated in the 'Expected Steps'."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 33.33333333333333,
      "aggregate_match": 33.33333333333333,
      "clause_match": 60.0,
      "fuzzy_score": 42.153493699885445,
      "overall_score": 55.32302405498281,
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "downtime_minutes",
        "failed_qty"
      ],
      "generated_columns": [
        "pl",
        "line_machine",
        "product",
        "failed_qty else null end",
        "downtime_minutes",
        "failed_qty"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 85.0,
      "table_score": 100.0,
      "column_score": 75.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 75.0,
      "column_recall": 75.0,
      "tables_correct": [
        "production_logs",
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "downtime_minutes",
        "failed_qty"
      ],
      "columns_missing": [
        "date"
      ],
      "columns_extra": [
        "line_machine"
      ],
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "date",
        "downtime_minutes",
        "failed_qty"
      ],
      "generated_columns": [
        "line_machine",
        "product",
        "downtime_minutes",
        "failed_qty"
      ]
    },
    "total_latency_ms": 2735.567092895508,
    "total_tokens": 1596,
    "errors": [],
    "timestamp": "2025-11-30T13:39:25.082167"
  },
  {
    "question_id": "Easy_26",
    "question_text": "What is the average target quantity for all products?",
    "category": "Easy",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Identify the relevant table: Use only the production_logs table, as it contains the Target_Qty column needed for the calculation.\n\n2. Select the required column: Extract the Target_Qty column from the production_logs table.\n\n3. Check for data quality: Ensure there are no NULL or non-numeric values in the Target_Qty column. If found, exclude or handle them appropriately.\n\n4. Calculate the average: Compute the mean (average) of all values in the Target_Qty column. This is done by summing all Target_Qty values and dividing by the total number of records.\n\n5. Round the result (optional): Round the calculated average to a reasonable number of decimal places (e.g., 2 decimal places) for clarity.\n\n6. Return the result: The final output is the average target quantity across all products and all production records.\n```",
    "sql_response": "```sql\nSELECT AVG(Target_Qty) AS Average_Target_Qty\nFROM production_logs;\n```",
    "table_selection_response": "{\"tables\": [\"production_logs\"], \"columns\": [\"Target_Qty\"]}",
    "methodology_score": 100,
    "sql_score": 96.8348623853211,
    "table_column_score": 100.0,
    "response_quality_score": 97.99535012245178,
    "overall_score": 98.69173684710756,
    "methodology_details": {
      "similarity_score": 100,
      "matching_concepts": [
        "Calculate the average of the Target_Qty column in production_logs"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Identify the relevant table: Use only the production_logs table, as it contains the Target_Qty column needed for the calculation.",
        "Select the required column: Extract the Target_Qty column from the production_logs table.",
        "Check for data quality: Ensure there are no NULL or non-numeric values in the Target_Qty column. If found, exclude or handle them appropriately.",
        "Compute the mean (average) of all values in the Target_Qty column. This is done by summing all Target_Qty values and dividing by the total number of records.",
        "Round the result (optional): Round the calculated average to a reasonable number of decimal places (e.g., 2 decimal places) for clarity.",
        "Return the result: The final output is the average target quantity across all products and all production records."
      ],
      "reasoning": "The LLM's steps are a more detailed breakdown of the ground truth step. The core concept of averaging the Target_Qty column from the production_logs table is maintained."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 78.89908256880733,
      "overall_score": 96.8348623853211,
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "target_qty"
      ],
      "generated_columns": [
        "target_qty"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "production_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "target_qty"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "production_logs"
      ],
      "generated_tables": [
        "production_logs"
      ],
      "expected_columns": [
        "target_qty"
      ],
      "generated_columns": [
        "target_qty"
      ]
    },
    "total_latency_ms": 2004.6498775482178,
    "total_tokens": 1312,
    "errors": [],
    "timestamp": "2025-11-30T13:39:32.762255"
  },
  {
    "question_id": "Complex_24",
    "question_text": "For Material Steel-101, what is the trend of wastage (increase or decrease) over the past 6 months?",
    "category": "Complex",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. From inventory_logs, filter rows where Material_Code = 'Steel-101' and Date is within the last 6 months from the latest date in the table.\n2. Sort the filtered data by Date ascending to create a chronological series.\n3. Calculate the monthly wastage for each month in the 6-month window: for every distinct month, sum Wastage_Kg.\n4. Compute the month-over-month change: for each month after the first, subtract the previous month’s total wastage from the current month’s total wastage.\n5. Count the number of months with positive change (increase) and negative change (decrease).\n6. If positive changes outnumber negative changes, conclude “wastage is increasing”; otherwise conclude “wastage is decreasing”.\n```",
    "sql_response": "```sql\nSELECT\n    DATE_TRUNC('month', Date) AS Month,\n    SUM(Wastage_Kg) AS Total_Wastage_Kg\nFROM inventory_logs\nWHERE Material_Code = 'Steel-101'\n  AND Date >= CURRENT_DATE - INTERVAL '6 months'\nGROUP BY DATE_TRUNC('month', Date)\nORDER BY Month;\n```",
    "table_selection_response": "{\"tables\": [\"inventory_logs\"], \"columns\": [\"Date\", \"Material_Code\", \"Wastage_Kg\"]}",
    "methodology_score": 85.0,
    "sql_score": 75.79787234042553,
    "table_column_score": 100.0,
    "response_quality_score": 97.81535172462463,
    "overall_score": 86.8107904916114,
    "methodology_details": {
      "similarity_score": 85.0,
      "matching_concepts": [
        "Filtering inventory_logs for Material_Code = 'Steel-101'",
        "Filtering for dates within the last 6 months",
        "Grouping by month",
        "Calculating the sum of Wastage_Kg for each month",
        "Determining an increasing or decreasing trend"
      ],
      "missing_concepts": [
        "Comparing the wastage in the first month to the wastage in the last month"
      ],
      "extra_concepts": [
        "Sorting data by Date ascending",
        "Computing month-over-month change",
        "Counting the number of months with positive/negative change"
      ],
      "reasoning": "The LLM solution correctly identifies the filtering, grouping, and aggregation steps. However, instead of directly comparing the first and last month's wastage, it calculates month-over-month changes and counts positive/negative changes. While this approach can also lead to the correct answer, it's more complex than necessary and not as directly aligned with the ground truth which asked for comparing first vs last month. Also, it explicitly sorts by date which is not a strict requirement given the grouping by month operation. The sorting step is beneficial, but not strictly necessary given the group by month operation."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 40.0,
      "aggregate_match": 100.0,
      "clause_match": 83.33333333333334,
      "fuzzy_score": 55.319148936170215,
      "overall_score": 75.79787234042553,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "material_code",
        "strftime",
        "date",
        "strftime%y-%m",
        "totalwastage",
        "case when select totalwastage",
        "month"
      ],
      "generated_columns": [
        "wastage_kg",
        "material_code",
        "date",
        "date_truncmonth",
        "date_trunc",
        "month"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "material_code",
        "wastage_kg",
        "date"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_code",
        "wastage_kg",
        "date"
      ],
      "generated_columns": [
        "material_code",
        "wastage_kg",
        "date"
      ]
    },
    "total_latency_ms": 2184.648275375366,
    "total_tokens": 1406,
    "errors": [],
    "timestamp": "2025-11-30T13:39:40.538752"
  },
  {
    "question_id": "Medium_20",
    "question_text": "Which supplier had the lowest average unit cost for 'Rubber-RB' in the last 6 months?",
    "category": "Medium",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Filter the inventory_logs table to keep only rows where Material_Name = 'Rubber-RB' and Date is within the last 6 months from today (use TODAY() – 180 days or equivalent date function).\n\n2. Group the filtered data by Supplier.\n\n3. For each Supplier, compute the weighted-average unit cost:  \n   SUM(Unit_Cost_Rupees × Consumption_Kg) ÷ SUM(Consumption_Kg).  \n   (This ensures the average reflects the actual mix of purchases, not just a simple mean of unit-cost rows.)\n\n4. Sort the resulting suppliers by the weighted-average unit cost in ascending order.\n\n5. Select the top supplier (lowest weighted-average unit cost) as the answer.\n```",
    "sql_response": "```sql\nSELECT Supplier\nFROM inventory_logs\nWHERE Material_Code = 'Rubber-RB'\n  AND Date >= CURRENT_DATE - INTERVAL '6 months'\nGROUP BY Supplier\nORDER BY AVG(Unit_Cost_Rupees) ASC\nLIMIT 1;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Material_Name\", \"Supplier\", \"Unit_Cost_Rupees\", \"Date\"]\n}\n```",
    "methodology_score": 90.0,
    "sql_score": 88.01408450704226,
    "table_column_score": 85.0,
    "response_quality_score": 97.73654818534851,
    "overall_score": 88.82858439599964,
    "methodology_details": {
      "similarity_score": 90.0,
      "matching_concepts": [
        "Filter for 'Rubber-RB'",
        "Filter dates in last 6 months",
        "Group by supplier",
        "Calculate average cost",
        "Order by average cost",
        "Select supplier with lowest cost"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Weighted average unit cost calculation using SUM(Unit_Cost_Rupees × Consumption_Kg) ÷ SUM(Consumption_Kg)"
      ],
      "reasoning": "The LLM's steps are very close to the expected steps. The main difference is that the LLM calculates a weighted average unit cost instead of a simple average. While a weighted average is arguably more accurate, it's not explicitly requested in the question. Also, the LLM refers to `Material_Name` instead of `Material_Code`, but this is a minor issue. The rest of the steps (filtering, grouping, ordering, and selecting) are all correct and in the right order."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 60.0,
      "aggregate_match": 100.0,
      "clause_match": 100.0,
      "fuzzy_score": 86.76056338028168,
      "overall_score": 88.01408450704226,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_code",
        "unit_cost_rupees",
        "supplier",
        "avgcost"
      ],
      "generated_columns": [
        "material_code",
        "unit_cost_rupees",
        "supplier",
        "avg"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 85.0,
      "table_score": 100.0,
      "column_score": 75.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 75.0,
      "column_recall": 75.0,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "unit_cost_rupees",
        "supplier",
        "date"
      ],
      "columns_missing": [
        "material_code"
      ],
      "columns_extra": [
        "material_name"
      ],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "material_code",
        "unit_cost_rupees",
        "supplier",
        "date"
      ],
      "generated_columns": [
        "material_name",
        "supplier",
        "date",
        "unit_cost_rupees"
      ]
    },
    "total_latency_ms": 2263.4518146514893,
    "total_tokens": 1376,
    "errors": [],
    "timestamp": "2025-11-30T13:39:48.022491"
  },
  {
    "question_id": "Complex_29",
    "question_text": "How does the rework count vary by defect type and product?",
    "category": "Complex",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Load the quality_control table (it is the only table that contains both Rework_Count and Defect_Type).\n\n2. Filter out any rows where Rework_Count is NULL or negative to ensure only valid rework records are analysed.\n\n3. Group the data by Product and Defect_Type.\n\n4. Within each (Product, Defect_Type) group, compute two aggregations:\n   a. Total_Rework = SUM(Rework_Count)  \n   b. Total_Incidents = COUNT(*) (i.e., number of quality-control records for that combination).\n\n5. (Optional) Calculate the average rework per incident: Avg_Rework = Total_Rework ÷ Total_Incidents.\n\n6. Sort the resulting summary table first by Product, then by Total_Rework (descending) to highlight which defect type for each product drives the most rework.\n\n7. Present the final table with columns: Product, Defect_Type, Total_Rework, Total_Incidents, Avg_Rework.\n```",
    "sql_response": "```sql\nSELECT\n    Product,\n    Defect_Type,\n    SUM(Rework_Count) AS Total_Rework\nFROM quality_control\nGROUP BY Product, Defect_Type\nORDER BY Product, Total_Rework DESC;\n```",
    "table_selection_response": "{\n  \"tables\": [\"quality_control\"],\n  \"columns\": [\"Product\", \"Defect_Type\", \"Rework_Count\"]\n}",
    "methodology_score": 75.0,
    "sql_score": 75.9665991902834,
    "table_column_score": 100.0,
    "response_quality_score": 97.71825408935547,
    "overall_score": 83.86013512553473,
    "methodology_details": {
      "similarity_score": 75.0,
      "matching_concepts": [
        "quality_control table",
        "Group by Defect_Type and Product",
        "Calculate the average Rework_Count for each group"
      ],
      "missing_concepts": [],
      "extra_concepts": [
        "Filter out any rows where Rework_Count is NULL or negative",
        "Calculate Total_Rework = SUM(Rework_Count)",
        "Calculate Total_Incidents = COUNT(*)",
        "Calculate the average rework per incident: Avg_Rework = Total_Rework ÷ Total_Incidents",
        "Sort the resulting summary table first by Product, then by Total_Rework",
        "Present the final table with columns: Product, Defect_Type, Total_Rework, Total_Incidents, Avg_Rework"
      ],
      "reasoning": "The LLM correctly identifies the table and the core grouping and aggregation operations. However, it adds several extra steps, like filtering for null values, calculating total incidents and average rework per incident, sorting, and presenting the table in a specific format, which weren't explicitly asked for in the original question, although some might be helpful."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 100.0,
      "aggregate_match": 25.0,
      "clause_match": 75.0,
      "fuzzy_score": 64.77732793522267,
      "overall_score": 75.9665991902834,
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "quality_control"
      ],
      "generated_tables": [
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "defect_type",
        "rework_count"
      ],
      "generated_columns": [
        "product",
        "defect_type",
        "rework_count"
      ]
    },
    "total_latency_ms": 2281.7459106445312,
    "total_tokens": 1383,
    "errors": [],
    "timestamp": "2025-11-30T13:39:55.800819"
  },
  {
    "question_id": "Medium_24",
    "question_text": "Which material has the highest wastage (in Kg) from July 2026 to Dec 2026?",
    "category": "Medium",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. Load the inventory_logs table and filter the Date column to keep only rows where Date is between 2026-07-01 and 2026-12-31 (inclusive).\n\n2. Group the filtered data by Material_Code and Material_Name.\n\n3. Within each group, sum the Wastage_Kg column to obtain total wastage per material for the 6-month period.\n\n4. Sort the aggregated results in descending order of total Wastage_Kg.\n\n5. Select the top row; its Material_Name (and Material_Code) is the material with the highest wastage in Kg from July 2026 to December 2026.\n```",
    "sql_response": "```sql\nSELECT Material_Code, SUM(Wastage_Kg) AS Total_Wastage_Kg\nFROM inventory_logs\nWHERE Date BETWEEN '2026-07-01' AND '2026-12-31'\nGROUP BY Material_Code\nORDER BY Total_Wastage_Kg DESC\nLIMIT 1;\n```",
    "table_selection_response": "```json\n{\n  \"tables\": [\"inventory_logs\"],\n  \"columns\": [\"Material_Name\", \"Wastage_Kg\"]\n}\n```",
    "methodology_score": 100,
    "sql_score": 66.75438596491229,
    "table_column_score": 88.0,
    "response_quality_score": 98.08158493041992,
    "overall_score": 85.1721935807613,
    "methodology_details": {
      "similarity_score": 100,
      "matching_concepts": [
        "Filter inventory_logs for dates between July 2026 and December 2026",
        "Group by Material_Name",
        "Sum the Wastage_Kg for each material",
        "Order by the sum of Wastage_Kg in descending order",
        "Select the top material"
      ],
      "missing_concepts": [],
      "extra_concepts": [],
      "reasoning": "The LLM's steps perfectly match the expected steps. All operations (filtering, grouping, aggregation, sorting, selection) are present and correctly ordered. The LLM also correctly identifies the table and relevant columns."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 20.0,
      "aggregate_match": 50.0,
      "clause_match": 100.0,
      "fuzzy_score": 78.36257309941521,
      "overall_score": 66.75438596491229,
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "sum",
        "material_name"
      ],
      "generated_columns": [
        "wastage_kg",
        "total_wastage_kg",
        "material_code"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 88.0,
      "table_score": 100.0,
      "column_score": 80.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 66.66666666666666,
      "tables_correct": [
        "inventory_logs"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "wastage_kg",
        "material_name"
      ],
      "columns_missing": [
        "date"
      ],
      "columns_extra": [],
      "expected_tables": [
        "inventory_logs"
      ],
      "generated_tables": [
        "inventory_logs"
      ],
      "expected_columns": [
        "wastage_kg",
        "material_name",
        "date"
      ],
      "generated_columns": [
        "wastage_kg",
        "material_name"
      ]
    },
    "total_latency_ms": 1918.4150695800781,
    "total_tokens": 1370,
    "errors": [],
    "timestamp": "2025-11-30T13:40:02.562737"
  },
  {
    "question_id": "Complex_31",
    "question_text": "What is the correlation between downtime (in minutes) and failed quantity for each product?",
    "category": "Complex",
    "model_id": "moonshotai/kimi-k2-instruct",
    "methodology_response": "```steps\n1. From production_logs, aggregate downtime per product per day: SELECT Product, Date, SUM(Downtime_Minutes) AS Total_Downtime FROM production_logs GROUP BY Product, Date.\n\n2. From quality_control, aggregate failed quantity per product per inspection date: SELECT Product, Inspection_Date AS Date, SUM(Failed_Qty) AS Total_Failed FROM quality_control GROUP BY Product, Inspection_Date.\n\n3. Join the two daily aggregates on Product and Date to create a single daily dataset with downtime and failed qty for every product-day combination.\n\n4. Filter the joined dataset to keep only rows where both Total_Downtime and Total_Failed are non-null (inner join logic).\n\n5. For each product, calculate the Pearson correlation coefficient between the daily Total_Downtime and Total_Failed series using the CORR() function (or equivalent) in your SQL/analytics tool.\n\n6. Return a list of products with their corresponding correlation coefficient; optionally flag products with |r| > 0.5 as having a strong linear relationship.\n```",
    "sql_response": "```sql\nSELECT\n    p.Product,\n    CORR(p.Downtime_Minutes, q.Failed_Qty) AS Downtime_Failure_Correlation\nFROM production_logs AS p\nJOIN (\n    SELECT\n        Inspection_Date,\n        Product,\n        SUM(Failed_Qty) AS Failed_Qty\n    FROM quality_control\n    GROUP BY Inspection_Date, Product\n) AS q\n  ON p.Date = q.Inspection_Date\n AND p.Product = q.Product\nGROUP BY p.Product;\n```",
    "table_selection_response": "{\n  \"tables\": [\"production_logs\", \"quality_control\"],\n  \"columns\": [\"Product\", \"Downtime_Minutes\", \"Failed_Qty\"]\n}",
    "methodology_score": 75.0,
    "sql_score": 70.97985347985347,
    "table_column_score": 100.0,
    "response_quality_score": 97.72412276268005,
    "overall_score": 82.11536099421673,
    "methodology_details": {
      "similarity_score": 75.0,
      "matching_concepts": [
        "Aggregate downtime per product",
        "Aggregate failed quantity per product",
        "Join results based on product",
        "Calculate correlation coefficient"
      ],
      "missing_concepts": [
        "Calculating *total* downtime and *total* failed quantity across all time"
      ],
      "extra_concepts": [
        "Aggregate downtime and failed quantity per day",
        "Filtering out NULL values using inner join logic",
        "Flagging products with strong correlation (|r| > 0.5)"
      ],
      "reasoning": "The LLM's response gets most of the major steps correct, but it calculates correlation on a *daily* basis instead of across the entire dataset. It also adds steps about handling null values and flagging strong correlations which aren't strictly necessary to answer the core question, but are good practices. It misses the key aspect of calculating total downtime/failed quantity across *all* time, opting to perform calculations on a daily basis."
    },
    "sql_details": {
      "table_match": 100.0,
      "column_match": 33.33333333333333,
      "aggregate_match": 66.66666666666666,
      "clause_match": 100.0,
      "fuzzy_score": 62.08791208791209,
      "overall_score": 70.97985347985347,
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "pl",
        "product",
        "totalfailedqty",
        "downtime_minutes",
        "totaldowntime",
        "failed_qty",
        "qc"
      ],
      "generated_columns": [
        "p",
        "product",
        "inspection_date",
        "downtime_minutes",
        "failed_qty"
      ],
      "has_valid_sql": true
    },
    "table_column_details": {
      "overall_score": 100.0,
      "table_score": 100.0,
      "column_score": 100.0,
      "table_precision": 100.0,
      "table_recall": 100.0,
      "column_precision": 100.0,
      "column_recall": 100.0,
      "tables_correct": [
        "production_logs",
        "quality_control"
      ],
      "tables_missing": [],
      "tables_extra": [],
      "columns_correct": [
        "product",
        "downtime_minutes",
        "failed_qty"
      ],
      "columns_missing": [],
      "columns_extra": [],
      "expected_tables": [
        "production_logs",
        "quality_control"
      ],
      "generated_tables": [
        "production_logs",
        "quality_control"
      ],
      "expected_columns": [
        "product",
        "downtime_minutes",
        "failed_qty"
      ],
      "generated_columns": [
        "product",
        "downtime_minutes",
        "failed_qty"
      ]
    },
    "total_latency_ms": 2275.8772373199463,
    "total_tokens": 1471,
    "errors": [],
    "timestamp": "2025-11-30T13:40:10.180615"
  }
]